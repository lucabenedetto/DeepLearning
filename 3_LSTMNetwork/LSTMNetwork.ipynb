{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div style=\"    font-variant: small-caps;\n",
    "    font-weight: normal;\n",
    "    font-size: 30px;\n",
    "    text-align: center;\n",
    "    padding: 15px;\n",
    "margin: 10px;\">Long Short Term Memory (LSTM) in a Recurrent Neural Network (RNN) for Language Modeling</div>\n",
    "<div style=\"    font-variant: small-caps;\n",
    "    font-weight: normal;\n",
    "    font-size: 20px;\n",
    "    text-align: center;\n",
    "    padding: 15px;\">Deep Learning</div>\n",
    "<div style=\"  float:right;\n",
    "    font-size: 12px;\n",
    "    line-height: 12px;\n",
    "padding: 10px 15px 8px;\">Luca BENEDETTO | Alberto IBARRONDO</div>\n",
    "\n",
    "<div style=\" display: inline-block; font-family: 'Lato', sans-serif; font-size: 12px; font-weight: bold; line-height: 12px; letter-spacing: 1px; padding: 10px 15px 8px; \">07/06/2017</div>\n",
    "<h1 style=\"text-align:center\"></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Summary\n",
    "This notebook builds and trains a Recurrent Neural Network, based on Long Short-Term Memory (LSTM) units for next word prediction task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#  1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will train a LSTM to predict the next word using a sample short story. The LSTM will learn to predict the next item of a sentence from the 3 previous items (given as input). Ponctuation marks are considered as dictionnary items so they can be predicted too. Figure 1 shows the LSTM and the process of next word prediction. \n",
    "\n",
    "<img src=\"lstm.png\" height=\"370\" width=\"370\"> \n",
    "\n",
    "\n",
    "Each word (and ponctuation) from text sentences is encoded by a unique integer. The integer value corresponds to the index of the corresponding word (or punctuation mark) in the dictionary. The network output is a one-hot-vector indicating the index of the predicted word in the reversed dictionnary (Section 1.2). For example if the prediction is 86, the predicted word will be \"company\". \n",
    "\n",
    "\n",
    "\n",
    "We use a sample short story from Aesopâ€™s Fables (http://www.taleswithmorals.com/) to train the model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class=\"alert alert-warning\">\"There was once a young Shepherd Boy who tended his sheep at the foot of a mountain near a dark forest.\n",
    "It was rather lonely for him all day, so he thought upon a plan by which he could get a little company and some excitement.\n",
    "He rushed down towards the village calling out \"Wolf, Wolf,\" and the villagers came out to meet him, and some of them stopped with him for a considerable time.\n",
    "This pleased the boy so much that a few days afterwards he tried the same trick, and again the villagers came to his help.\n",
    "But shortly after this a Wolf actually did come out from the forest, and began to worry the sheep, and the boy of course cried out \"Wolf, Wolf,\" still louder than before.\n",
    "But this time the villagers, who had been fooled twice before, thought the boy was again deceiving them, and nobody stirred to come to his help.\n",
    "So the Wolf made a good meal off the boy's flock, and when the boy complained, the wise man of the village said:\n",
    "\"A liar will not be believed, even when he speaks the truth\".</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.1 Loading libraries\n",
    "We start by loading the necessary libraries and resetting the default computational graph. For more details about the rnn packages we import, we suggest https://www.tensorflow.org/api_guides/python/contrib.rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections # used to build the dictionary\n",
    "import random\n",
    "import time\n",
    "import pickle # may be used to save your model \n",
    "import matplotlib.pyplot as plt\n",
    "#Import Tensorflow and rnn\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn  \n",
    "\n",
    "# Target log path\n",
    "logs_path = 'lstm_words'\n",
    "writer = tf.summary.FileWriter(logs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.2. Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's load and split the text of our story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there' 'was' 'once' 'a' 'young' 'shepherd' 'boy' 'who' 'tended' 'his'\n",
      " 'sheep' 'at' 'the' 'foot' 'of' 'a' 'mountain' 'near' 'a' 'dark' 'forest'\n",
      " '.' 'it' 'was' 'rather' 'lonely' 'for' 'him' 'all' 'day' ',' 'so' 'he'\n",
      " 'thought' 'upon' 'a' 'plan' 'by' 'which' 'he' 'could' 'get' 'a' 'little'\n",
      " 'company' 'and' 'some' 'excitement' '.' 'he' 'rushed' 'down' 'towards'\n",
      " 'the' 'village' 'calling' 'out' 'wolf' ',' 'wolf' ',' 'and' 'the'\n",
      " 'villagers' 'came' 'out' 'to' 'meet' 'him' ',' 'and' 'some' 'of' 'them'\n",
      " 'stopped' 'with' 'him' 'for' 'a' 'considerable' 'time' '.' 'this'\n",
      " 'pleased' 'the' 'boy' 'so' 'much' 'that' 'a' 'few' 'days' 'afterwards'\n",
      " 'he' 'tried' 'the' 'same' 'trick' ',' 'and' 'again' 'the' 'villagers'\n",
      " 'came' 'to' 'his' 'help' '.' 'but' 'shortly' 'after' 'this' 'a' 'wolf'\n",
      " 'actually' 'did' 'come' 'out' 'from' 'the' 'forest' ',' 'and' 'began' 'to'\n",
      " 'worry' 'the' 'sheep,' 'and' 'the' 'boy' 'of' 'course' 'cried' 'out'\n",
      " 'wolf' ',' 'wolf' ',' 'still' 'louder' 'than' 'before' '.' 'but' 'this'\n",
      " 'time' 'the' 'villagers' ',' 'who' 'had' 'been' 'fooled' 'twice' 'before'\n",
      " ',' 'thought' 'the' 'boy' 'was' 'again' 'deceiving' 'them' ',' 'and'\n",
      " 'nobody' 'stirred' 'to' 'come' 'to' 'his' 'help' '.' 'so' 'the' 'wolf'\n",
      " 'made' 'a' 'good' 'meal' 'off' 'the' \"boy's\" 'flock' ',' 'and' 'when'\n",
      " 'the' 'boy' 'complained' ',' 'the' 'wise' 'man' 'of' 'the' 'village'\n",
      " 'said' ':' 'a' 'liar' 'will' 'not' 'be' 'believed' ',' 'even' 'when' 'he'\n",
      " 'speaks' 'the' 'truth' '.']\n",
      "Loaded training data...\n",
      "Length training data: 214\n",
      "Number of unique elements: 113\n"
     ]
    }
   ],
   "source": [
    "def load_data(filename):\n",
    "    with open(filename) as f:\n",
    "        data = f.readlines()\n",
    "    data = [x.strip().lower() for x in data]\n",
    "    data = [data[i].split() for i in range(len(data))]\n",
    "    data = np.array(data)\n",
    "    data = np.reshape(data, [-1, ])\n",
    "    print(data)\n",
    "    return data\n",
    "\n",
    "#Run the cell \n",
    "train_file ='data/story.txt'\n",
    "train_data = load_data(train_file)\n",
    "print(\"Loaded training data...\")\n",
    "print(\"Length training data: %d\" %len(train_data))\n",
    "print(\"Number of unique elements: %d\" %len(set(train_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.2 Symbols encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The LSTM input's can only be numbers. A way to convert words (symbols or any items) to numbers is to assign a unique integer to each word. This process is often based on frequency of occurrence for efficient coding purpose.\n",
    "\n",
    "Here, we define a function to build an indexed word dictionary (word->number). The \"build_vocabulary\" function builds both:\n",
    "\n",
    "- Dictionary : used for encoding words to numbers for the LSTM inputs \n",
    "- Reverted dictionnary : used for decoding the outputs of the LSTM into words (and punctuation).\n",
    "\n",
    "For example, in the story above, we have **113** individual words. The \"build_vocabulary\" function builds a dictionary with the following entries ['the': 0], [',': 1], ['company': 85],...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    dic= dict()\n",
    "    for word, _ in count:\n",
    "        dic[word] = len(dic)\n",
    "    reverse_dic= dict(zip(dic.values(), dic.keys()))\n",
    "    return dic, reverse_dic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's display the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary_size =  113\n",
      "\n",
      "\n",
      "Dictionary : \n",
      "\n",
      "{'all': 32, 'liar': 33, 'help': 17, 'cried': 34, 'course': 35, 'still': 36, 'pleased': 37, 'before': 18, 'excitement': 91, 'deceiving': 38, 'had': 39, 'young': 69, 'actually': 40, 'to': 6, 'villagers': 11, 'shepherd': 41, 'them': 19, 'lonely': 42, 'get': 44, 'dark': 45, 'not': 64, 'day': 47, 'did': 48, 'calling': 49, 'twice': 50, 'good': 51, 'stopped': 52, 'truth': 53, 'meal': 54, 'sheep,': 55, 'some': 20, 'tended': 56, 'louder': 57, 'flock': 58, 'out': 9, 'even': 59, 'trick': 60, 'said': 61, 'for': 21, 'be': 62, 'after': 63, 'come': 22, 'by': 65, 'boy': 7, 'of': 10, 'could': 66, 'days': 67, 'wolf': 5, 'afterwards': 68, ',': 1, 'down': 70, 'village': 23, 'sheep': 72, 'little': 73, 'from': 74, 'rushed': 75, 'there': 76, 'been': 77, '.': 4, 'few': 78, 'much': 79, \"boy's\": 80, ':': 81, 'was': 12, 'a': 2, 'him': 13, 'that': 83, 'company': 84, 'nobody': 85, 'but': 24, 'fooled': 86, 'with': 87, 'than': 43, 'he': 8, 'made': 89, 'wise': 90, 'this': 14, 'will': 71, 'near': 92, 'believed': 93, 'meet': 94, 'and': 3, 'it': 95, 'his': 15, 'at': 96, 'worry': 97, 'again': 25, 'considerable': 88, 'rather': 98, 'began': 99, 'when': 26, 'same': 101, 'forest': 27, 'which': 102, 'speaks': 103, 'towards': 104, 'tried': 105, 'mountain': 106, 'who': 28, 'upon': 107, 'plan': 108, 'man': 109, 'complained': 82, 'stirred': 110, 'off': 100, 'foot': 46, 'shortly': 111, 'thought': 29, 'so': 16, 'time': 30, 'the': 0, 'came': 31, 'once': 112}\n",
      "\n",
      "\n",
      "Reverted Dictionary : \n",
      "\n",
      "{0: 'the', 1: ',', 2: 'a', 3: 'and', 4: '.', 5: 'wolf', 6: 'to', 7: 'boy', 8: 'he', 9: 'out', 10: 'of', 11: 'villagers', 12: 'was', 13: 'him', 14: 'this', 15: 'his', 16: 'so', 17: 'help', 18: 'before', 19: 'them', 20: 'some', 21: 'for', 22: 'come', 23: 'village', 24: 'but', 25: 'again', 26: 'when', 27: 'forest', 28: 'who', 29: 'thought', 30: 'time', 31: 'came', 32: 'all', 33: 'liar', 34: 'cried', 35: 'course', 36: 'still', 37: 'pleased', 38: 'deceiving', 39: 'had', 40: 'actually', 41: 'shepherd', 42: 'lonely', 43: 'than', 44: 'get', 45: 'dark', 46: 'foot', 47: 'day', 48: 'did', 49: 'calling', 50: 'twice', 51: 'good', 52: 'stopped', 53: 'truth', 54: 'meal', 55: 'sheep,', 56: 'tended', 57: 'louder', 58: 'flock', 59: 'even', 60: 'trick', 61: 'said', 62: 'be', 63: 'after', 64: 'not', 65: 'by', 66: 'could', 67: 'days', 68: 'afterwards', 69: 'young', 70: 'down', 71: 'will', 72: 'sheep', 73: 'little', 74: 'from', 75: 'rushed', 76: 'there', 77: 'been', 78: 'few', 79: 'much', 80: \"boy's\", 81: ':', 82: 'complained', 83: 'that', 84: 'company', 85: 'nobody', 86: 'fooled', 87: 'with', 88: 'considerable', 89: 'made', 90: 'wise', 91: 'excitement', 92: 'near', 93: 'believed', 94: 'meet', 95: 'it', 96: 'at', 97: 'worry', 98: 'rather', 99: 'began', 100: 'off', 101: 'same', 102: 'which', 103: 'speaks', 104: 'towards', 105: 'tried', 106: 'mountain', 107: 'upon', 108: 'plan', 109: 'man', 110: 'stirred', 111: 'shortly', 112: 'once'}\n"
     ]
    }
   ],
   "source": [
    "dictionary, reverse_dictionary = build_vocabulary(train_data)\n",
    "vocabulary_size= len(dictionary) \n",
    "print(\"Vocabulary_size = \", vocabulary_size)\n",
    "print(\"\\n\")\n",
    "print(\"Dictionary : \\n\")\n",
    "print(dictionary)\n",
    "print(\"\\n\")\n",
    "print(\"Reverted Dictionary : \\n\" )\n",
    "print(reverse_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2. LSTM Model in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We are now to develop an LSTM model to predict the word of following a sequence of 3 words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.1. Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We are defining a 2-layers LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def LSTMModel(x, n_input, weights, biases):\n",
    "\n",
    "    # reshape to [1, n_input]\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "\n",
    "    # Generate a n_input-element sequence of inputs\n",
    "    # (eg. [had] [a] [general] -> [20] [6] [33])\n",
    "    x = tf.split(x,n_input,1)\n",
    "\n",
    "    # 1-layer LSTM with n_hidden units.\n",
    "    rnn_cell = rnn.BasicLSTMCell(n_hidden)\n",
    "    \n",
    "    # 2-layer LSTM with n_hidden units.\n",
    "    rnn_cell = rnn.MultiRNNCell([rnn_cell]*2)\n",
    "    \n",
    "    # generate prediction\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # there are n_input outputs but\n",
    "    # we only want the last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2.2 Training Parameters and Graph creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "epochs = 50000\n",
    "display_step = 1000\n",
    "n_input = 3\n",
    "\n",
    "#For each LSTM cell that you initialise, supply a value for the hidden dimension, number of units in LSTM cell\n",
    "n_hidden = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2.3 Graph creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# LSTM  weights and biases\n",
    "weights = { 'out': tf.Variable(tf.random_normal([n_hidden, vocabulary_size]))}\n",
    "biases = {'out': tf.Variable(tf.random_normal([vocabulary_size])) }\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "y = tf.placeholder(\"float\", [None, vocabulary_size])\n",
    "\n",
    "#build the model\n",
    "pred = RNN(x, n_input, weights, biases)\n",
    "\n",
    "# Loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Model evaluation\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2.4 Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def LSTMtest(sentence, session, verbose=False):\n",
    "    sentence = sentence.strip()\n",
    "    words = sentence.split(' ')\n",
    "    if len(words) != n_input:\n",
    "        print(\"sentence length should be equal to\", n_input, \"!\")\n",
    "    try:\n",
    "        symbols_inputs = [dictionary[str(words[i - n_input])] for i in range(n_input)]\n",
    "        keys = np.reshape(np.array(symbols_inputs), [-1, n_input, 1])\n",
    "        onehot_pred = session.run(pred, feed_dict={x: keys})\n",
    "        onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
    "        words.append(reverse_dictionary[onehot_pred_index])\n",
    "        sentence = \" \".join(words)\n",
    "        if verbose:\n",
    "            print(sentence)\n",
    "        return reverse_dictionary[onehot_pred_index]\n",
    "    except:\n",
    "        print([\"Word\", words[i - n_input], \"not in dictionary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3. Training the LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the Training process, at each epoch, 3 words are taken from the training data, encoded to integer to form the input vector. The training labels are one-hot vector encoding the word that comes after the 3 inputs words. We display the loss and the training accuracy every 1000 iteration and save the model at the end of training in the **lstm_model** folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def LSTMTrain(name):\n",
    "    # Initializing the variables\n",
    "    start_time = time.time()\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    print(\"Start Training\")\n",
    "    ##############################################\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        session.run(init)\n",
    "        step = 0\n",
    "        offset = random.randint(0,n_input+1)\n",
    "        end_offset = n_input + 1\n",
    "        acc_total = 0\n",
    "        loss_total = 0\n",
    "\n",
    "        writer.add_graph(session.graph)\n",
    "\n",
    "        while step < epochs:\n",
    "            # Generate a minibatch. Add some randomness on selection process.\n",
    "            if offset > (len(train_data) - end_offset):\n",
    "                offset = random.randint(0, n_input+1)\n",
    "\n",
    "            symbols_in_keys = [ [dictionary[ str(train_data[i])]] for i in range(offset, offset+n_input) ]\n",
    "            symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "\n",
    "            symbols_out_onehot = np.zeros([vocabulary_size], dtype=float)\n",
    "            symbols_out_onehot[dictionary[str(train_data[offset+n_input])]] = 1.0\n",
    "            symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
    "\n",
    "            _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \\\n",
    "                                                    feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
    "            loss_total += loss\n",
    "            acc_total += acc\n",
    "\n",
    "            if (step+1) % display_step == 0:\n",
    "\n",
    "                symbols_in = [train_data[i] for i in range(offset, offset + n_input)]\n",
    "                symbols_out = train_data[offset + n_input]\n",
    "                symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
    "\n",
    "                print(\"Iter= \" + str(step+1) + \", Loss= \" + \\\n",
    "                      \"{:.6f}\".format(loss_total/display_step) + \", TrAcc= \" + \\\n",
    "                      \"{:.2f}%\".format(100*acc_total/display_step)),\n",
    "                print(\"%s - [%s] vs [%s]\" % (symbols_in,symbols_out,symbols_out_pred))\n",
    "\n",
    "                acc_total = 0\n",
    "                loss_total = 0\n",
    "\n",
    "            step += 1\n",
    "            offset += (n_input+1)\n",
    "\n",
    "    ##############################################\n",
    "        print(\"Optimization Finished!\")\n",
    "        print(\"Elapsed time:%.1f s\" %(time.time() - start_time))\n",
    "        print(\"Run on command line: tensorboard --logdir=%s\" % (logs_path))\n",
    "        print(\"Point your web browser to the returned link\")\n",
    "    ##############################################\n",
    "        model_saver.save(sess=session, save_path='lstm_model/'+name)\n",
    "    ##############################################\n",
    "        print(\"Model saved\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Iter= 1000, Loss= 4.619733, TrAcc= 6.50% ['speaks', 'the', 'truth'] - [.] vs [he]\n",
      "Iter= 2000, Loss= 3.736615, TrAcc= 13.80% ['he', 'speaks', 'the'] - [truth] vs [forest]\n",
      "Iter= 3000, Loss= 3.441579, TrAcc= 18.10% ['a', 'liar', 'will'] - [not] vs [:]\n",
      "Iter= 4000, Loss= 2.886903, TrAcc= 32.50% ['believed', ',', 'even'] - [when] vs [shepherd]\n",
      "Iter= 5000, Loss= 2.466733, TrAcc= 39.40% ['a', 'liar', 'will'] - [not] vs [:]\n",
      "Iter= 6000, Loss= 2.355309, TrAcc= 41.70% ['be', 'believed', ','] - [even] vs [boy's]\n",
      "Iter= 7000, Loss= 1.837935, TrAcc= 54.80% ['the', 'wise', 'man'] - [of] vs [of]\n",
      "Iter= 8000, Loss= 1.723356, TrAcc= 55.60% [',', 'and', 'when'] - [the] vs [of]\n",
      "Iter= 9000, Loss= 1.680242, TrAcc= 55.10% [',', 'the', 'wise'] - [man] vs [man]\n",
      "Iter= 10000, Loss= 1.190535, TrAcc= 68.40% ['a', 'liar', 'will'] - [not] vs [not]\n",
      "Iter= 11000, Loss= 1.245663, TrAcc= 65.70% ['young', 'shepherd', 'boy'] - [who] vs [who]\n",
      "Iter= 12000, Loss= 0.923475, TrAcc= 76.30% ['once', 'a', 'young'] - [shepherd] vs [days]\n",
      "Iter= 13000, Loss= 1.011464, TrAcc= 69.50% ['the', 'village', 'said'] - [:] vs [:]\n",
      "Iter= 14000, Loss= 0.869120, TrAcc= 76.90% ['will', 'not', 'be'] - [believed] vs [he]\n",
      "Iter= 15000, Loss= 0.931696, TrAcc= 73.40% ['man', 'of', 'the'] - [village] vs [village]\n",
      "Iter= 16000, Loss= 0.627505, TrAcc= 83.00% ['man', 'of', 'the'] - [village] vs [village]\n",
      "Iter= 17000, Loss= 0.772526, TrAcc= 78.30% ['said', ':', 'a'] - [liar] vs [liar]\n",
      "Iter= 18000, Loss= 0.460625, TrAcc= 87.60% ['not', 'be', 'believed'] - [,] vs [the]\n",
      "Iter= 19000, Loss= 0.551270, TrAcc= 85.00% ['the', 'boy', 'complained'] - [,] vs [,]\n",
      "Iter= 20000, Loss= 0.542769, TrAcc= 86.90% ['said', ':', 'a'] - [liar] vs [liar]\n",
      "Iter= 21000, Loss= 0.465838, TrAcc= 88.20% ['a', 'liar', 'will'] - [not] vs [not]\n",
      "Iter= 22000, Loss= 0.584257, TrAcc= 84.10% ['believed', ',', 'even'] - [when] vs [when]\n",
      "Iter= 23000, Loss= 0.374317, TrAcc= 89.60% [':', 'a', 'liar'] - [will] vs [will]\n",
      "Iter= 24000, Loss= 0.528858, TrAcc= 86.20% ['liar', 'will', 'not'] - [be] vs [be]\n",
      "Iter= 25000, Loss= 0.433693, TrAcc= 86.40% ['a', 'liar', 'will'] - [not] vs [not]\n",
      "Iter= 26000, Loss= 0.397060, TrAcc= 87.90% ['of', 'the', 'village'] - [said] vs [said]\n",
      "Iter= 27000, Loss= 0.397318, TrAcc= 89.00% ['village', 'said', ':'] - [a] vs [a]\n",
      "Iter= 28000, Loss= 0.273457, TrAcc= 92.70% ['young', 'shepherd', 'boy'] - [who] vs [who]\n",
      "Iter= 29000, Loss= 0.453159, TrAcc= 88.60% ['of', 'a', 'mountain'] - [near] vs [near]\n",
      "Iter= 30000, Loss= 0.367259, TrAcc= 90.70% ['him', 'all', 'day'] - [,] vs [,]\n",
      "Iter= 31000, Loss= 0.397182, TrAcc= 89.40% ['rather', 'lonely', 'for'] - [him] vs [him]\n",
      "Iter= 32000, Loss= 0.398924, TrAcc= 90.00% ['foot', 'of', 'a'] - [mountain] vs [mountain]\n",
      "Iter= 33000, Loss= 0.325172, TrAcc= 89.00% ['all', 'day', ','] - [so] vs [so]\n",
      "Iter= 34000, Loss= 0.317205, TrAcc= 90.00% ['dark', 'forest', '.'] - [it] vs [it]\n",
      "Iter= 35000, Loss= 0.284799, TrAcc= 91.20% ['all', 'day', ','] - [so] vs [so]\n",
      "Iter= 36000, Loss= 0.249984, TrAcc= 92.60% ['all', 'day', ','] - [so] vs [so]\n",
      "Iter= 37000, Loss= 0.281406, TrAcc= 90.30% ['lonely', 'for', 'him'] - [all] vs [all]\n",
      "Iter= 38000, Loss= 0.233675, TrAcc= 93.20% ['rather', 'lonely', 'for'] - [him] vs [him]\n",
      "Iter= 39000, Loss= 0.328268, TrAcc= 91.10% ['.', 'it', 'was'] - [rather] vs [rather]\n",
      "Iter= 40000, Loss= 0.306602, TrAcc= 89.40% ['sheep', 'at', 'the'] - [foot] vs [foot]\n",
      "Iter= 41000, Loss= 0.261380, TrAcc= 92.60% ['forest', '.', 'it'] - [was] vs [was]\n",
      "Iter= 42000, Loss= 0.180892, TrAcc= 95.30% ['he', 'thought', 'upon'] - [a] vs [a]\n",
      "Iter= 43000, Loss= 0.277204, TrAcc= 94.90% [',', 'so', 'he'] - [thought] vs [thought]\n",
      "Iter= 44000, Loss= 0.344732, TrAcc= 91.20% ['day', ',', 'so'] - [he] vs [he]\n",
      "Iter= 45000, Loss= 0.268094, TrAcc= 91.50% ['thought', 'upon', 'a'] - [plan] vs [plan]\n",
      "Iter= 46000, Loss= 0.227615, TrAcc= 92.40% ['could', 'get', 'a'] - [little] vs [little]\n",
      "Iter= 47000, Loss= 0.193860, TrAcc= 94.50% ['towards', 'the', 'village'] - [calling] vs [calling]\n",
      "Iter= 48000, Loss= 0.308691, TrAcc= 90.50% ['down', 'towards', 'the'] - [village] vs [boy's]\n",
      "Iter= 49000, Loss= 0.211860, TrAcc= 94.60% ['to', 'meet', 'him'] - [,] vs [,]\n",
      "Iter= 50000, Loss= 0.196414, TrAcc= 93.70% ['him', ',', 'and'] - [some] vs [some]\n",
      "Optimization Finished!\n",
      "Elapsed time:67.5 s\n",
      "Run on command line: tensorboard --logdir=lstm_words\n",
      "Point your web browser to the returned link\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "LSTMTrainModel('LSTMmodel_3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 4 Testing the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4.1 Next word prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We load the model (using the model_saved variable given in the training session) and test the sentences :\n",
    "- 'get a little' \n",
    "- 'nobody tried to'\n",
    "- Trying with other sentences using words from the story's vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n",
      "\n",
      "Testing the model\n",
      "get a little company\n",
      "nobody tried to come\n",
      "he rushed down towards\n",
      "a wolf actually did\n",
      "a liar will not\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    # Restore variables from disk.\n",
    "    model_saver.restore(session, \"lstm_model/LSTMmodel_3\")\n",
    "    print(\"Model restored.\")\n",
    "    print(\"\\nTesting the model\")\n",
    "    test_sentences = ['get a little', 'nobody tried to', 'he rushed down', 'a wolf actually', 'a liar will']\n",
    "    for sentence in test_sentences:\n",
    "        test(sentence, session, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4.2 More fun with the Story Writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's use the RNN/LSTM model learned in the previous question to create a\n",
    "new story/fable.\n",
    "For this we choose 3 words from the dictionary which will start the\n",
    "story and initialize the network. Using those 3 words the RNN will generate\n",
    "the next word or the story. Using the last 3 words (the newly predicted one\n",
    "and the last 2 from the input) we will use the network to predict the 5\n",
    "word of the story... and so on until your story is 5 sentences long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def RNNcreate_story(sentence, session, numOfSentences=5, n_input=3, verbose=False):\n",
    "    cnt_sent = 0\n",
    "    end_sent = ['.',',',':',';','!','?']\n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "    words = sentence.split(' ')\n",
    "    while cnt_sent < numOfSentences or (cnt_sent == numOfSentences and sentence[-1] not in ['.','!','?']):\n",
    "        symbols_inputs = [dictionary[str(words[i - n_input])] for i in range(n_input)]\n",
    "        keys = np.reshape(np.array(symbols_inputs), [-1, n_input, 1])\n",
    "        onehot_pred = session.run(pred, feed_dict={x: keys})\n",
    "        onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
    "        new_word = reverse_dictionary[onehot_pred_index]\n",
    "        words.append(new_word)\n",
    "        sentence += \" \" + words[-1]\n",
    "        words = words[-n_input:]\n",
    "        if verbose:\n",
    "            print(sentence)\n",
    "        if new_word in end_sent:\n",
    "            cnt_sent += 1\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n",
      "\n",
      "a wolf actually did come out from the forest , and began stirred to come to his help . so the wolf made a good will again a this pleased the boy so much that a few days afterwards he tried the truth of a mountain near a dark will not twice before . but the villagers came out to meet him , and some of them stopped with him for a considerable time .\n"
     ]
    }
   ],
   "source": [
    "#Your implementation goes here \n",
    "with tf.Session() as session:\n",
    "    # Restore variables from disk.\n",
    "    model_saver.restore(session, \"lstm_model/LSTMmodel_3\")\n",
    "    print(\"Model restored.\\n\")\n",
    "    RNNcreate_story('a wolf actually', session, numOfSentences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4.3 Playing with number of inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "The number of input in our example is 3, now we are gonna see what happens when we use other number (1, 2 and 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "epochs = 50000\n",
    "display_step = 1000\n",
    "n_input = 1\n",
    "n_hidden = 64\n",
    "\n",
    "weights = { 'out': tf.Variable(tf.random_normal([n_hidden, vocabulary_size]))}\n",
    "biases = {'out': tf.Variable(tf.random_normal([vocabulary_size])) }\n",
    "\n",
    "x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "y = tf.placeholder(\"float\", [None, vocabulary_size])\n",
    "\n",
    "pred = RNNModel(x, n_input, weights, biases)\n",
    "\n",
    "# Loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Model evaluation\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "model_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Iter= 1000, Loss= 4.519836, TrAcc= 6.10% ['time'] - [.] vs [.]\n",
      "Iter= 2000, Loss= 4.327969, TrAcc= 8.20% ['stirred'] - [to] vs [the]\n",
      "Iter= 3000, Loss= 4.211221, TrAcc= 8.50% ['company'] - [and] vs [the]\n",
      "Iter= 4000, Loss= 4.007069, TrAcc= 10.60% ['of'] - [course] vs [came]\n",
      "Iter= 5000, Loss= 3.964375, TrAcc= 10.60% ['a'] - [young] vs [and]\n",
      "Iter= 6000, Loss= 4.051782, TrAcc= 10.10% ['a'] - [few] vs [and]\n",
      "Iter= 7000, Loss= 3.840868, TrAcc= 13.30% ['made'] - [a] vs [the]\n",
      "Iter= 8000, Loss= 3.962126, TrAcc= 11.90% ['rushed'] - [down] vs [a]\n",
      "Iter= 9000, Loss= 3.617404, TrAcc= 13.80% [','] - [wolf] vs [and]\n",
      "Iter= 10000, Loss= 3.657220, TrAcc= 14.50% ['his'] - [sheep] vs [.]\n",
      "Iter= 11000, Loss= 3.966666, TrAcc= 12.30% ['he'] - [tried] vs [,]\n",
      "Iter= 12000, Loss= 3.806246, TrAcc= 12.50% ['off'] - [the] vs [to]\n",
      "Iter= 13000, Loss= 3.816383, TrAcc= 12.50% ['calling'] - [out] vs [a]\n",
      "Iter= 14000, Loss= 3.806111, TrAcc= 12.10% ['but'] - [this] vs [.]\n",
      "Iter= 15000, Loss= 3.783292, TrAcc= 13.10% ['mountain'] - [near] vs [the]\n",
      "Iter= 16000, Loss= 3.534387, TrAcc= 16.00% ['again'] - [the] vs [.]\n",
      "Iter= 17000, Loss= 3.868160, TrAcc= 14.60% ['and'] - [when] vs [some]\n",
      "Iter= 18000, Loss= 3.816075, TrAcc= 14.30% ['out'] - [to] vs [,]\n",
      "Iter= 19000, Loss= 3.562902, TrAcc= 15.80% ['had'] - [been] vs [the]\n",
      "Iter= 20000, Loss= 3.807804, TrAcc= 13.60% ['it'] - [was] vs [the]\n",
      "Iter= 21000, Loss= 3.927647, TrAcc= 12.90% ['.'] - [but] vs [,]\n",
      "Iter= 22000, Loss= 3.670316, TrAcc= 13.50% [':'] - [a] vs [the]\n",
      "Iter= 23000, Loss= 3.391761, TrAcc= 14.00% ['him'] - [,] vs [,]\n",
      "Iter= 24000, Loss= 3.925676, TrAcc= 12.20% ['fooled'] - [twice] vs [the]\n",
      "Iter= 25000, Loss= 3.749783, TrAcc= 12.30% ['all'] - [day] vs [.]\n",
      "Iter= 26000, Loss= 3.649916, TrAcc= 12.80% ['out'] - [from] vs [help]\n",
      "Iter= 27000, Loss= 3.645772, TrAcc= 14.50% ['liar'] - [will] vs [the]\n",
      "Iter= 28000, Loss= 3.707476, TrAcc= 12.60% ['a'] - [considerable] vs [wolf]\n",
      "Iter= 29000, Loss= 3.680318, TrAcc= 14.80% ['nobody'] - [stirred] vs [the]\n",
      "Iter= 30000, Loss= 3.565695, TrAcc= 16.40% ['could'] - [get] vs [.]\n",
      "Iter= 31000, Loss= 3.455500, TrAcc= 17.30% ['boy'] - [of] vs [his]\n",
      "Iter= 32000, Loss= 3.856004, TrAcc= 15.10% ['boy'] - [who] vs [,]\n",
      "Iter= 33000, Loss= 3.656752, TrAcc= 15.40% ['tried'] - [the] vs [a]\n",
      "Iter= 34000, Loss= 3.236506, TrAcc= 18.70% ['so'] - [the] vs [.]\n",
      "Iter= 35000, Loss= 3.509756, TrAcc= 17.10% ['rushed'] - [down] vs [by]\n",
      "Iter= 36000, Loss= 3.633586, TrAcc= 16.00% ['wolf'] - [,] vs [,]\n",
      "Iter= 37000, Loss= 3.337052, TrAcc= 17.70% ['young'] - [shepherd] vs [the]\n",
      "Iter= 38000, Loss= 3.354645, TrAcc= 19.20% ['much'] - [that] vs [a]\n",
      "Iter= 39000, Loss= 3.623474, TrAcc= 15.50% ['the'] - [wolf] vs [boy]\n",
      "Iter= 40000, Loss= 3.493507, TrAcc= 16.00% ['down'] - [towards] vs [a]\n",
      "Iter= 41000, Loss= 3.558879, TrAcc= 14.90% [','] - [still] vs [and]\n",
      "Iter= 42000, Loss= 3.549413, TrAcc= 14.60% ['the'] - [foot] vs [villagers]\n",
      "Iter= 43000, Loss= 3.531369, TrAcc= 14.10% ['villagers'] - [came] vs [came]\n",
      "Iter= 44000, Loss= 3.172051, TrAcc= 17.20% ['the'] - [boy] vs [boy]\n",
      "Iter= 45000, Loss= 3.502731, TrAcc= 15.80% [','] - [wolf] vs [and]\n",
      "Iter= 46000, Loss= 2.824796, TrAcc= 19.60% [','] - [still] vs [and]\n",
      "Iter= 47000, Loss= 3.751848, TrAcc= 15.30% ['the'] - [foot] vs [boy]\n",
      "Iter= 48000, Loss= 3.290758, TrAcc= 17.00% ['he'] - [tried] vs [,]\n",
      "Iter= 49000, Loss= 3.708877, TrAcc= 14.60% ['and'] - [when] vs [some]\n",
      "Iter= 50000, Loss= 3.294118, TrAcc= 16.40% ['village'] - [calling] vs [.]\n",
      "Optimization Finished!\n",
      "Elapsed time:64.5 s\n",
      "Run on command line: tensorboard --logdir=lstm_words\n",
      "Point your web browser to the returned link\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "RNNTrain('LSTMmodel_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n",
      "\n",
      "foot he wolf , and some . he wolf , and some . he wolf , and some .\n"
     ]
    }
   ],
   "source": [
    "#Your implementation goes here \n",
    "with tf.Session() as session:\n",
    "    # Restore variables from disk.\n",
    "    model_saver.restore(session, \"lstm_model/LSTMmodel_1\")\n",
    "    print(\"Model restored.\\n\")\n",
    "    RNNcreate_story('foot', session, numOfSentences=5, n_input=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class='alert alert-info'>\n",
    "We can see that with only one input the model enters almost immediately into a loop and it will not generate anything different. The commas and the dots are very frequent in the output; this us due to the high relative frequency of these symbols in the original text compared to any other word.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "epochs = 50000\n",
    "display_step = 1000\n",
    "n_input = 2\n",
    "n_hidden = 64\n",
    "\n",
    "weights = { 'out': tf.Variable(tf.random_normal([n_hidden, vocabulary_size]))}\n",
    "biases = {'out': tf.Variable(tf.random_normal([vocabulary_size])) }\n",
    "\n",
    "x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "y = tf.placeholder(\"float\", [None, vocabulary_size])\n",
    "\n",
    "pred = RNNModel(x, n_input, weights, biases)\n",
    "\n",
    "# Loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Model evaluation\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "model_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Iter= 1000, Loss= 4.336366, TrAcc= 9.80% ['by', 'which'] - [he] vs [a]\n",
      "Iter= 2000, Loss= 3.687833, TrAcc= 18.00% ['a', 'considerable'] - [time] vs [the]\n",
      "Iter= 3000, Loss= 3.523067, TrAcc= 19.00% ['come', 'out'] - [from] vs [who]\n",
      "Iter= 4000, Loss= 2.776961, TrAcc= 25.70% [',', 'thought'] - [the] vs [the]\n",
      "Iter= 5000, Loss= 3.306874, TrAcc= 19.50% ['complained', ','] - [the] vs [and]\n",
      "Iter= 6000, Loss= 3.188090, TrAcc= 20.70% ['.', 'it'] - [was] vs [the]\n",
      "Iter= 7000, Loss= 2.942333, TrAcc= 22.80% ['the', 'villagers'] - [came] vs [,]\n",
      "Iter= 8000, Loss= 2.827688, TrAcc= 25.50% ['.', 'but'] - [shortly] vs [this]\n",
      "Iter= 9000, Loss= 2.517571, TrAcc= 29.40% ['than', 'before'] - [.] vs [,]\n",
      "Iter= 10000, Loss= 2.511359, TrAcc= 31.00% [',', 'and'] - [when] vs [again]\n",
      "Iter= 11000, Loss= 2.387003, TrAcc= 34.50% ['the', 'foot'] - [of] vs [calling]\n",
      "Iter= 12000, Loss= 2.246355, TrAcc= 38.80% [',', 'wolf'] - [,] vs [when]\n",
      "Iter= 13000, Loss= 2.363784, TrAcc= 36.90% ['the', 'villagers'] - [came] vs [came]\n",
      "Iter= 14000, Loss= 2.075026, TrAcc= 44.50% ['wolf', ','] - [wolf] vs [wolf]\n",
      "Iter= 15000, Loss= 1.944080, TrAcc= 47.80% ['the', 'wolf'] - [made] vs [so]\n",
      "Iter= 16000, Loss= 2.231664, TrAcc= 40.30% ['there', 'was'] - [once] vs [for]\n",
      "Iter= 17000, Loss= 1.826472, TrAcc= 51.30% ['thought', 'upon'] - [a] vs [a]\n",
      "Iter= 18000, Loss= 1.855541, TrAcc= 50.70% ['for', 'a'] - [considerable] vs [and]\n",
      "Iter= 19000, Loss= 1.920005, TrAcc= 50.30% ['began', 'to'] - [worry] vs [worry]\n",
      "Iter= 20000, Loss= 1.913922, TrAcc= 48.50% ['was', 'again'] - [deceiving] vs [a]\n",
      "Iter= 21000, Loss= 1.867298, TrAcc= 50.10% ['the', 'wise'] - [man] vs [man]\n",
      "Iter= 22000, Loss= 1.998264, TrAcc= 47.20% ['plan', 'by'] - [which] vs [,]\n",
      "Iter= 23000, Loss= 1.567458, TrAcc= 56.00% ['few', 'days'] - [afterwards] vs [a]\n",
      "Iter= 24000, Loss= 1.864468, TrAcc= 50.30% ['the', 'sheep,'] - [and] vs [and]\n",
      "Iter= 25000, Loss= 1.774265, TrAcc= 52.10% ['to', 'his'] - [help] vs [help]\n",
      "Iter= 26000, Loss= 1.637218, TrAcc= 55.70% ['said', ':'] - [a] vs [a]\n",
      "Iter= 27000, Loss= 1.811519, TrAcc= 51.50% ['rather', 'lonely'] - [for] vs [before]\n",
      "Iter= 28000, Loss= 1.593970, TrAcc= 54.70% ['calling', 'out'] - [wolf] vs [a]\n",
      "Iter= 29000, Loss= 1.717107, TrAcc= 54.80% ['same', 'trick'] - [,] vs [,]\n",
      "Iter= 30000, Loss= 0.846970, TrAcc= 72.90% ['cried', 'out'] - [wolf] vs [to]\n",
      "Iter= 31000, Loss= 1.373639, TrAcc= 58.30% ['was', 'again'] - [deceiving] vs [a]\n",
      "Iter= 32000, Loss= 2.125378, TrAcc= 54.10% ['the', 'wise'] - [man] vs [man]\n",
      "Iter= 33000, Loss= 2.035631, TrAcc= 51.70% ['.', 'it'] - [was] vs [was]\n",
      "Iter= 34000, Loss= 1.840392, TrAcc= 52.70% ['him', ','] - [and] vs [wolf]\n",
      "Iter= 35000, Loss= 1.382493, TrAcc= 61.40% ['same', 'trick'] - [,] vs [afterwards]\n",
      "Iter= 36000, Loss= 1.159160, TrAcc= 69.80% [',', 'still'] - [louder] vs [louder]\n",
      "Iter= 37000, Loss= 2.205759, TrAcc= 49.70% ['help', '.'] - [so] vs [so]\n",
      "Iter= 38000, Loss= 1.900805, TrAcc= 54.30% ['his', 'sheep'] - [at] vs [at]\n",
      "Iter= 39000, Loss= 1.686734, TrAcc= 59.00% ['and', 'some'] - [excitement] vs [excitement]\n",
      "Iter= 40000, Loss= 1.760026, TrAcc= 53.00% ['this', 'pleased'] - [the] vs [the]\n",
      "Iter= 41000, Loss= 1.883512, TrAcc= 52.20% ['the', 'boy'] - [of] vs [so]\n",
      "Iter= 42000, Loss= 1.649607, TrAcc= 53.80% ['wolf', 'made'] - [a] vs [a]\n",
      "Iter= 43000, Loss= 1.645473, TrAcc= 54.90% ['speaks', 'the'] - [truth] vs [truth]\n",
      "Iter= 44000, Loss= 1.393600, TrAcc= 64.50% ['he', 'thought'] - [upon] vs [tended]\n",
      "Iter= 45000, Loss= 1.513444, TrAcc= 61.40% [',', 'and'] - [some] vs [the]\n",
      "Iter= 46000, Loss= 1.370686, TrAcc= 65.70% ['but', 'shortly'] - [after] vs [after]\n",
      "Iter= 47000, Loss= 1.514678, TrAcc= 63.90% [',', 'still'] - [louder] vs [louder]\n",
      "Iter= 48000, Loss= 1.525732, TrAcc= 61.50% ['so', 'the'] - [wolf] vs [wolf]\n",
      "Iter= 49000, Loss= 1.738141, TrAcc= 58.20% ['shepherd', 'boy'] - [who] vs [wolf]\n",
      "Iter= 50000, Loss= 0.947556, TrAcc= 73.40% ['some', 'excitement'] - [.] vs [after]\n",
      "Optimization Finished!\n",
      "Elapsed time:58.8 s\n",
      "Run on command line: tensorboard --logdir=lstm_words\n",
      "Point your web browser to the returned link\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "RNNTrain('LSTMmodel_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n",
      "\n",
      "a wolf actually did come . considerable was for stopped out a wolf actually did come . considerable was for stopped out a wolf actually did come . considerable was for stopped out a wolf actually did come . considerable was for stopped out a wolf actually did come .\n"
     ]
    }
   ],
   "source": [
    "#Your implementation goes here \n",
    "with tf.Session() as session:\n",
    "    # Restore variables from disk.\n",
    "    model_saver.restore(session, \"lstm_model/LSTMmodel_2\")\n",
    "    print(\"Model restored.\\n\")\n",
    "    RNNcreate_story('a wolf', session, numOfSentences=5, n_input=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "Considerig the last two words the situation is similar to the previous one, the difference is that reasonably the loop is a bit longer than the previous one.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "epochs = 50000\n",
    "display_step = 1000\n",
    "n_input = 5\n",
    "n_hidden = 64\n",
    "\n",
    "weights = { 'out': tf.Variable(tf.random_normal([n_hidden, vocabulary_size]))}\n",
    "biases = {'out': tf.Variable(tf.random_normal([vocabulary_size])) }\n",
    "\n",
    "x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "y = tf.placeholder(\"float\", [None, vocabulary_size])\n",
    "\n",
    "pred = RNNModel(x, n_input, weights, biases)\n",
    "\n",
    "# Loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Model evaluation\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "model_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Iter= 1000, Loss= 4.470892, TrAcc= 8.50% ['come', 'to', 'his', 'help', '.'] - [so] vs [,]\n",
      "Iter= 2000, Loss= 3.527248, TrAcc= 20.40% ['from', 'the', 'forest', ',', 'and'] - [began] vs [of]\n",
      "Iter= 3000, Loss= 2.446506, TrAcc= 44.30% ['again', 'the', 'villagers', 'came', 'to'] - [his] vs [deceiving]\n",
      "Iter= 4000, Loss= 1.936984, TrAcc= 53.30% ['towards', 'the', 'village', 'calling', 'out'] - [wolf] vs [a]\n",
      "Iter= 5000, Loss= 1.706365, TrAcc= 56.40% ['village', 'said', ':', 'a', 'liar'] - [will] vs [much]\n",
      "Iter= 6000, Loss= 0.857457, TrAcc= 77.30% [',', 'still', 'louder', 'than', 'before'] - [.] vs [.]\n",
      "Iter= 7000, Loss= 0.514233, TrAcc= 87.40% ['the', 'villagers', 'came', 'to', 'his'] - [help] vs [help]\n",
      "Iter= 8000, Loss= 0.455893, TrAcc= 87.90% ['villagers', 'came', 'out', 'to', 'meet'] - [him] vs [him]\n",
      "Iter= 9000, Loss= 0.274175, TrAcc= 93.10% ['a', 'little', 'company', 'and', 'some'] - [excitement] vs [excitement]\n",
      "Iter= 10000, Loss= 0.193359, TrAcc= 94.90% ['rather', 'lonely', 'for', 'him', 'all'] - [day] vs [day]\n",
      "Iter= 11000, Loss= 0.199799, TrAcc= 95.40% ['a', 'liar', 'will', 'not', 'be'] - [believed] vs [believed]\n",
      "Iter= 12000, Loss= 0.134664, TrAcc= 96.50% ['deceiving', 'them', ',', 'and', 'nobody'] - [stirred] vs [stirred]\n",
      "Iter= 13000, Loss= 0.124053, TrAcc= 97.00% ['of', 'course', 'cried', 'out', 'wolf'] - [,] vs [,]\n",
      "Iter= 14000, Loss= 0.083010, TrAcc= 98.00% ['the', 'forest', ',', 'and', 'began'] - [to] vs [to]\n",
      "Iter= 15000, Loss= 0.099725, TrAcc= 97.30% ['village', 'calling', 'out', 'wolf', ','] - [wolf] vs [wolf]\n",
      "Iter= 16000, Loss= 0.048435, TrAcc= 99.10% ['of', 'a', 'mountain', 'near', 'a'] - [dark] vs [dark]\n",
      "Iter= 17000, Loss= 0.075171, TrAcc= 97.90% ['wolf', 'made', 'a', 'good', 'meal'] - [off] vs [off]\n",
      "Iter= 18000, Loss= 0.076344, TrAcc= 98.20% [',', 'wolf', ',', 'still', 'louder'] - [than] vs [than]\n",
      "Iter= 19000, Loss= 0.081158, TrAcc= 97.80% ['boy', 'so', 'much', 'that', 'a'] - [few] vs [few]\n",
      "Iter= 20000, Loss= 0.069921, TrAcc= 98.40% ['came', 'out', 'to', 'meet', 'him'] - [,] vs [,]\n",
      "Iter= 21000, Loss= 0.057007, TrAcc= 98.70% ['boy', 'who', 'tended', 'his', 'sheep'] - [at] vs [at]\n",
      "Iter= 22000, Loss= 0.065841, TrAcc= 98.30% ['stirred', 'to', 'come', 'to', 'his'] - [help] vs [help]\n",
      "Iter= 23000, Loss= 0.042928, TrAcc= 98.90% ['boy', 'of', 'course', 'cried', 'out'] - [wolf] vs [wolf]\n",
      "Iter= 24000, Loss= 0.067561, TrAcc= 98.00% ['considerable', 'time', '.', 'this', 'pleased'] - [the] vs [the]\n",
      "Iter= 25000, Loss= 0.048364, TrAcc= 99.20% ['of', 'them', 'stopped', 'with', 'him'] - [for] vs [for]\n",
      "Iter= 26000, Loss= 0.066601, TrAcc= 98.30% ['sheep', 'at', 'the', 'foot', 'of'] - [a] vs [a]\n",
      "Iter= 27000, Loss= 0.060698, TrAcc= 98.60% ['come', 'to', 'his', 'help', '.'] - [so] vs [so]\n",
      "Iter= 28000, Loss= 0.054933, TrAcc= 98.80% ['than', 'before', '.', 'but', 'this'] - [time] vs [time]\n",
      "Iter= 29000, Loss= 0.037918, TrAcc= 99.20% ['his', 'help', '.', 'but', 'shortly'] - [after] vs [after]\n",
      "Iter= 30000, Loss= 0.055451, TrAcc= 98.80% ['them', 'stopped', 'with', 'him', 'for'] - [a] vs [a]\n",
      "Iter= 31000, Loss= 0.058863, TrAcc= 98.60% ['plan', 'by', 'which', 'he', 'could'] - [get] vs [get]\n",
      "Iter= 32000, Loss= 0.046259, TrAcc= 98.80% ['a', 'liar', 'will', 'not', 'be'] - [believed] vs [believed]\n",
      "Iter= 33000, Loss= 0.058114, TrAcc= 98.70% ['before', ',', 'thought', 'the', 'boy'] - [was] vs [was]\n",
      "Iter= 34000, Loss= 0.034799, TrAcc= 99.20% ['but', 'this', 'time', 'the', 'villagers'] - [,] vs [,]\n",
      "Iter= 35000, Loss= 0.051708, TrAcc= 98.90% ['out', 'from', 'the', 'forest', ','] - [and] vs [and]\n",
      "Iter= 36000, Loss= 0.031435, TrAcc= 99.30% ['him', 'for', 'a', 'considerable', 'time'] - [.] vs [.]\n",
      "Iter= 37000, Loss= 0.060743, TrAcc= 98.70% ['.', 'it', 'was', 'rather', 'lonely'] - [for] vs [for]\n",
      "Iter= 38000, Loss= 0.045377, TrAcc= 98.70% ['so', 'the', 'wolf', 'made', 'a'] - [good] vs [good]\n",
      "Iter= 39000, Loss= 0.038292, TrAcc= 99.10% ['after', 'this', 'a', 'wolf', 'actually'] - [did] vs [did]\n",
      "Iter= 40000, Loss= 0.036152, TrAcc= 99.20% ['which', 'he', 'could', 'get', 'a'] - [little] vs [little]\n",
      "Iter= 41000, Loss= 0.057424, TrAcc= 98.10% ['when', 'he', 'speaks', 'the', 'truth'] - [.] vs [.]\n",
      "Iter= 42000, Loss= 0.043506, TrAcc= 98.50% ['been', 'fooled', 'twice', 'before', ','] - [thought] vs [thought]\n",
      "Iter= 43000, Loss= 0.044132, TrAcc= 98.30% ['of', 'course', 'cried', 'out', 'wolf'] - [,] vs [,]\n",
      "Iter= 44000, Loss= 0.052136, TrAcc= 97.90% ['tried', 'the', 'same', 'trick', ','] - [and] vs [and]\n",
      "Iter= 45000, Loss= 0.063674, TrAcc= 98.20% ['came', 'out', 'to', 'meet', 'him'] - [,] vs [,]\n",
      "Iter= 46000, Loss= 0.041295, TrAcc= 98.90% ['rather', 'lonely', 'for', 'him', 'all'] - [day] vs [day]\n",
      "Iter= 47000, Loss= 0.042034, TrAcc= 98.70% ['complained', ',', 'the', 'wise', 'man'] - [of] vs [of]\n",
      "Iter= 48000, Loss= 0.056823, TrAcc= 98.50% ['still', 'louder', 'than', 'before', '.'] - [but] vs [but]\n",
      "Iter= 49000, Loss= 0.013639, TrAcc= 99.50% ['after', 'this', 'a', 'wolf', 'actually'] - [did] vs [did]\n",
      "Iter= 50000, Loss= 0.045306, TrAcc= 98.50% ['meet', 'him', ',', 'and', 'some'] - [of] vs [of]\n",
      "Optimization Finished!\n",
      "Elapsed time:81.2 s\n",
      "Run on command line: tensorboard --logdir=lstm_words\n",
      "Point your web browser to the returned link\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "RNNTrain('LSTMmodel_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n",
      "\n",
      "a boy a wolf and villagers came out to so him time with with him for a considerable time . this pleased the boy so much that a few days afterwards he tried the same trick , and again the boy complained , the wise man of the village said : a liar will not be believed , even when he speaks the truth .\n"
     ]
    }
   ],
   "source": [
    "#Your implementation goes here \n",
    "with tf.Session() as session:\n",
    "    # Restore variables from disk.\n",
    "    model_saver.restore(session, \"lstm_model/LSTMmodel_5\")\n",
    "    print(\"Model restored.\\n\")\n",
    "    RNNcreate_story('a boy a wolf and', session, numOfSentences=5, n_input=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class='alert alert-info'>\n",
    "In this case the number of words that we are considering is too big compared to the length of the text we have: indeed, the model tends to recreate the same exact sentences that where present in the original text, even though we started from a sentence ('a boy a wolf and') that was not there.\n",
    "We could consider this as overfitting our model.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
