{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div style=\"    font-variant: small-caps;\n",
    "    font-weight: normal;\n",
    "    font-size: 30px;\n",
    "    text-align: center;\n",
    "    padding: 15px;\n",
    "margin: 10px;\">Convolutional Neural Network (CNN) for Handwritten Digits Recognition</div>\n",
    "<div style=\"    font-variant: small-caps;\n",
    "    font-weight: normal;\n",
    "    font-size: 20px;\n",
    "    text-align: center;\n",
    "    padding: 15px;\">Deep Learning</div>\n",
    "<div style=\"  float:right;\n",
    "    font-size: 12px;\n",
    "    line-height: 12px;\n",
    "padding: 10px 15px 8px;\">Luca BENEDETTO | Alberto IBARRONDO</div>\n",
    "\n",
    "<div style=\" display: inline-block; font-family: 'Lato', sans-serif; font-size: 12px; font-weight: bold; line-height: 12px; letter-spacing: 1px; padding: 10px 15px 8px; \">29/05/2017</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the last notebook, we built a Multilayer Perceptron for recognizing hand-written digits from the MNIST data-set. The best achieved accuracy on testing data was about 97%, but modern implementations of Convolutional Neural Networks whould surpass that mark.\n",
    "\n",
    "In this notebook, we will build, train and optimize in TensorFlow one of the early Convolutional Neural Networks, **LeNet-5**, and push it beyond 99% accuracy. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1. A first NeuralNetwork model in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.1 Import Modules & Load MNIST Data in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.contrib.layers import flatten\n",
    "from __future__ import print_function\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Image Shape: 784\n",
      "Training Set:   55000 samples\n",
      "Validation Set: 5000 samples\n",
      "Test Set:       10000 samples\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "X_train, y_train           = mnist.train.images, mnist.train.labels\n",
    "X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test             = mnist.test.images, mnist.test.labels\n",
    "print(\"Image Shape: {}\".format(X_train[0].shape[0]))\n",
    "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
    "print(\"Validation Set: {} samples\".format(len(X_validation)))\n",
    "print(\"Test Set:       {} samples\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Before starting with CNN, let's train and test in TensorFlow a simple example :\n",
    "**y=softmax(Wx+b)** \n",
    "\n",
    "This model should reach an accuracy of about 92 %."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.2 Coding the Graph and Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  01   =====> Loss= 1.286880517\n",
      "Epoch:  02   =====> Loss= 0.732314337\n",
      "Epoch:  03   =====> Loss= 0.600182705\n",
      "Epoch:  04   =====> Loss= 0.536474698\n",
      "Epoch:  05   =====> Loss= 0.497743477\n",
      "Epoch:  06   =====> Loss= 0.471044735\n",
      "Epoch:  07   =====> Loss= 0.451285015\n",
      "Epoch:  08   =====> Loss= 0.435486047\n",
      "Epoch:  09   =====> Loss= 0.423485379\n",
      "Epoch:  10   =====> Loss= 0.413127333\n",
      "Epoch:  11   =====> Loss= 0.404424905\n",
      "Epoch:  12   =====> Loss= 0.396875077\n",
      "Epoch:  13   =====> Loss= 0.390122826\n",
      "Epoch:  14   =====> Loss= 0.384438272\n",
      "Epoch:  15   =====> Loss= 0.379063723\n",
      "Epoch:  16   =====> Loss= 0.374401041\n",
      "Epoch:  17   =====> Loss= 0.370427270\n",
      "Epoch:  18   =====> Loss= 0.366561572\n",
      "Epoch:  19   =====> Loss= 0.362732460\n",
      "Epoch:  20   =====> Loss= 0.359670188\n",
      "Epoch:  21   =====> Loss= 0.356530713\n",
      "Epoch:  22   =====> Loss= 0.353755475\n",
      "Epoch:  23   =====> Loss= 0.351379176\n",
      "Epoch:  24   =====> Loss= 0.348841268\n",
      "Epoch:  25   =====> Loss= 0.346291858\n",
      "Epoch:  26   =====> Loss= 0.344263495\n",
      "Epoch:  27   =====> Loss= 0.342145607\n",
      "Epoch:  28   =====> Loss= 0.340279845\n",
      "Epoch:  29   =====> Loss= 0.338280311\n",
      "Epoch:  30   =====> Loss= 0.336593690\n",
      "Epoch:  31   =====> Loss= 0.334856931\n",
      "Epoch:  32   =====> Loss= 0.333555910\n",
      "Epoch:  33   =====> Loss= 0.331999466\n",
      "Epoch:  34   =====> Loss= 0.330438348\n",
      "Epoch:  35   =====> Loss= 0.329235198\n",
      "Epoch:  36   =====> Loss= 0.327769872\n",
      "Epoch:  37   =====> Loss= 0.326818192\n",
      "Epoch:  38   =====> Loss= 0.325353872\n",
      "Epoch:  39   =====> Loss= 0.324172505\n",
      "Epoch:  40   =====> Loss= 0.323176802\n",
      "Epoch:  41   =====> Loss= 0.321943087\n",
      "Epoch:  42   =====> Loss= 0.320741635\n",
      "Epoch:  43   =====> Loss= 0.319880484\n",
      "Epoch:  44   =====> Loss= 0.318746548\n",
      "Epoch:  45   =====> Loss= 0.318128876\n",
      "Epoch:  46   =====> Loss= 0.317128404\n",
      "Epoch:  47   =====> Loss= 0.316180597\n",
      "Epoch:  48   =====> Loss= 0.315200540\n",
      "Epoch:  49   =====> Loss= 0.314626362\n",
      "Epoch:  50   =====> Loss= 0.313644664\n",
      "Epoch:  51   =====> Loss= 0.313065712\n",
      "Epoch:  52   =====> Loss= 0.312324699\n",
      "Epoch:  53   =====> Loss= 0.311312480\n",
      "Epoch:  54   =====> Loss= 0.310184292\n",
      "Epoch:  55   =====> Loss= 0.309985134\n",
      "Epoch:  56   =====> Loss= 0.309305577\n",
      "Epoch:  57   =====> Loss= 0.308584504\n",
      "Epoch:  58   =====> Loss= 0.307975754\n",
      "Epoch:  59   =====> Loss= 0.307369640\n",
      "Epoch:  60   =====> Loss= 0.306651877\n",
      "Epoch:  61   =====> Loss= 0.306076956\n",
      "Epoch:  62   =====> Loss= 0.305529022\n",
      "Epoch:  63   =====> Loss= 0.304919904\n",
      "Epoch:  64   =====> Loss= 0.304308251\n",
      "Epoch:  65   =====> Loss= 0.303945196\n",
      "Epoch:  66   =====> Loss= 0.303266914\n",
      "Epoch:  67   =====> Loss= 0.302906753\n",
      "Epoch:  68   =====> Loss= 0.302573205\n",
      "Epoch:  69   =====> Loss= 0.301829170\n",
      "Epoch:  70   =====> Loss= 0.301280140\n",
      "Epoch:  71   =====> Loss= 0.300879078\n",
      "Epoch:  72   =====> Loss= 0.300337700\n",
      "Epoch:  73   =====> Loss= 0.299630875\n",
      "Epoch:  74   =====> Loss= 0.299276637\n",
      "Epoch:  75   =====> Loss= 0.299070879\n",
      "Epoch:  76   =====> Loss= 0.298232197\n",
      "Epoch:  77   =====> Loss= 0.298021811\n",
      "Epoch:  78   =====> Loss= 0.297795259\n",
      "Epoch:  79   =====> Loss= 0.297131211\n",
      "Epoch:  80   =====> Loss= 0.296873052\n",
      "Epoch:  81   =====> Loss= 0.296567148\n",
      "Epoch:  82   =====> Loss= 0.295779722\n",
      "Epoch:  83   =====> Loss= 0.295447144\n",
      "Epoch:  84   =====> Loss= 0.295448288\n",
      "Epoch:  85   =====> Loss= 0.295106368\n",
      "Epoch:  86   =====> Loss= 0.294550212\n",
      "Epoch:  87   =====> Loss= 0.294151047\n",
      "Epoch:  88   =====> Loss= 0.294116553\n",
      "Epoch:  89   =====> Loss= 0.293291785\n",
      "Epoch:  90   =====> Loss= 0.293069592\n",
      "Epoch:  91   =====> Loss= 0.292543515\n",
      "Epoch:  92   =====> Loss= 0.292532154\n",
      "Epoch:  93   =====> Loss= 0.292028785\n",
      "Epoch:  94   =====> Loss= 0.291817178\n",
      "Epoch:  95   =====> Loss= 0.291677481\n",
      "Epoch:  96   =====> Loss= 0.291299572\n",
      "Epoch:  97   =====> Loss= 0.290715999\n",
      "Epoch:  98   =====> Loss= 0.290480864\n",
      "Epoch:  99   =====> Loss= 0.290369174\n",
      "Epoch:  100   =====> Loss= 0.289843813\n",
      "Optimization Finished!\n",
      "Accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "#GRAPH DEFINITION\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 100\n",
    "batch_size = 128\n",
    "display_step = 1\n",
    "logs_path = 'log_files/'  # useful for tensorboard\n",
    "\n",
    "# tf Graph Input:  mnist data image of shape 28*28=784\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='InputData')\n",
    "# 0-9 digits recognition,  10 classes\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]), name='Weights')\n",
    "b = tf.Variable(tf.zeros([10]), name='Bias')\n",
    "\n",
    "# Construct model and encapsulating all ops into scopes, \n",
    "#  making Tensorboard's Graph visualization more convenient\n",
    "with tf.name_scope('Model'):\n",
    "    # Model\n",
    "    pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "with tf.name_scope('Loss'):\n",
    "    # Minimize error using cross entropy\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "with tf.name_scope('SGD'):\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"TrainingLoss\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"TrainingAccuracy\", acc)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "#TRAINING\n",
    "\n",
    "\n",
    "# Launch the graph for training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop), cost op (to get loss value)\n",
    "            # and summary nodes\n",
    "            _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                     feed_dict={x: batch_xs, y: batch_ys})\n",
    "            # Write logs at every iteration\n",
    "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch: \", '%02d' % (epoch+1), \"  =====> Loss=\", \n",
    "                  \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    # Calculate accuracy\n",
    "    print(\"Accuracy:\", acc.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.3 Visualization with Tensorboard\n",
    "\n",
    "Using [Tensorboard](https://www.tensorflow.org/get_started/summaries_and_tensorboard), we can now visualize the created graph, giving us an overview of the architecture and how all of the major components are connected. You can also visalize and analyse the learning curves. \n",
    "\n",
    "In order to launch tensorBoard we follow these steps: \n",
    "- Open a Terminal and run the command line **\"tensorboard --logdir=log_files/\"**, it will generate an http link ,ex http://666.6.6.6:6006,\n",
    "- Copy this  link into a web browser \n",
    "- Display the images!\n",
    "\n",
    "<img src=\"MNIST_99_Challenge_Figures/Screenshot from 2017-05-26 17:53:17.png\",width=\"800\" height=\"600\" align=\"center\">\n",
    "<center><span>Figure 1: Tensorboard visualization </span></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2. The 99% MNIST Challenge using CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2.1 LeNet5 Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that we are familiar with familar with **tensorFlow** and **tensorBoard**, we are going to build, train and test the baseline [LeNet-5](http://yann.lecun.com/exdb/lenet/)  model for the MNIST digits recognition problem.  \n",
    "\n",
    "Further ahead we will make some optimizations to surpass 99% of accuracy. The best model so far achieved over 99.7% accuracy ([List of Results](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "<img src=\"lenet.png\",width=\"800\" height=\"600\" align=\"center\">\n",
    "<center><span>Figure 2: Lenet 5 </span></center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The LeNet architecture accepts a 32x32xC image as input, where C is the number of color channels. Since MNIST images are grayscale, C is 1 in this case.\n",
    "\n",
    "--------------------------\n",
    "1. **Layer 1: Convolutional.** The output shape should be 28x28x6 **Activation.** sigmoid **Pooling.** The output shape should be 14x14x6.\n",
    "- \n",
    "2. **Layer 2: Convolutional.** The output shape should be 10x10x16. **Activation.** sigmoid **Pooling.** The output shape should be 5x5x16.\n",
    "\n",
    "3. **Flatten.** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D.  You may need to use **flatten*  from tensorflow.contrib.layers import flatten\n",
    "\n",
    "4. **Layer 3: Fully Connected.** This should have 120 outputs. **Activation.** sigmoid\n",
    "\n",
    "5. **Layer 4: Fully Connected.** This should have 84 outputs. **Activation.** sigmoid\n",
    "\n",
    "6. **Layer 5: Fully Connected.** This should have 10 outputs. **Activation.** softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.1.1 LeNet5 model Implementation [Question 2.1.1]\n",
    "The implementation draws classes and functions from the [Tensorflow API](https://www.tensorflow.org/api_docs/python/tf/nn). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# LeNet5 variables init \n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>COMMENT:</b><br/>\n",
    "These functions are used for weigths and bias initilization. The standard deviation in the weights can be tuned in case we find any strange behaviour in the CNN.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# LeNet5 convolutional and max pool layers\n",
    "def conv2d(x, W,pad='SAME'):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=pad)\n",
    "\n",
    "def max_pool_2x2(x,pad='VALID'):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding=pad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>COMMENT:</b><br/>\n",
    "The stride of value 1x1 in the convolutional will yield a same size feature map when used with padding 'SAME', which adds padding to ensure that the proportion stays equal after the convolution. <br/><br/>\n",
    "Using a stride of 2x2 in the max pool ensures reducing the features in half, which is why padding 'VALID' is used (equivalent to no padding).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def LeNet5_Model(data, keep_pr=1, activFunc=tf.nn.sigmoid):\n",
    "    input_data = tf.reshape(data,[-1,28,28,1])\n",
    "    \n",
    "    # -------------------------------------------------\n",
    "    # --------------------VARIABLES--------------------\n",
    "    # -------------------------------------------------\n",
    "    # Convolutional layer 1 variables\n",
    "    W_conv1 = weight_variable([5,5,1,6])\n",
    "    b_conv1 = bias_variable([6])\n",
    "    \n",
    "    # Convolutional layer 2 variables\n",
    "    W_conv2 = weight_variable([5,5,6,16])\n",
    "    b_conv2 = bias_variable([16])\n",
    "    \n",
    "    # Fully connected layer 1 param\n",
    "    W_fc1 = weight_variable([400, 120])\n",
    "    b_fc1 = bias_variable([120])\n",
    "    \n",
    "    # Fully connected layer 2 param\n",
    "    W_fc2 = weight_variable([120, 84])\n",
    "    b_fc2 = bias_variable([84])\n",
    "    \n",
    "    # Fully connected layer 3 param\n",
    "    W_fc3 = weight_variable([84, 10])\n",
    "    b_fc3 = bias_variable([10])\n",
    "    \n",
    "    \n",
    "    # ----------------------------------------------------\n",
    "    # --------------------COMPUTATIONS--------------------\n",
    "    # ----------------------------------------------------\n",
    "    # Convolutional layer 1 & max pooling\n",
    "    h_conv1 = activFunc(conv2d(input_data,W_conv1)+ b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "    # Convolutional layer 2 & max pooling\n",
    "    h_conv2 = activFunc(conv2d(h_pool1,W_conv2, pad='VALID')+ b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)   \n",
    "\n",
    "    # Flattening\n",
    "    h_pool2_flat = tf.contrib.layers.flatten(h_pool2)\n",
    "\n",
    "    # Fully connected layer 1, sigmoid activation\n",
    "    h_fc1 = activFunc(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "    drop_fc1 = tf.nn.dropout(h_fc1, keep_pr)\n",
    "    \n",
    "    # Fully connected layer 2, sigmoid activation\n",
    "    h_fc2 = activFunc(tf.matmul(drop_fc1, W_fc2) + b_fc2)\n",
    "    drop_fc2 = tf.nn.dropout(h_fc2, keep_pr)\n",
    "    \n",
    "    # Fully connected layer 3, softmax activation\n",
    "    predicted = tf.nn.softmax(tf.matmul(drop_fc2, W_fc3) + b_fc3)\n",
    "    \n",
    "    return predicted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>COMMENT:</b><br/>\n",
    "First we define all the variables, then we compute all the activations using the functions defined above. We included a dropout in the fully connected layers, but is set off (with keep_p=1) by default.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.1.2 Number of parameters in LeNet5 [Question 2.1.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mumber of parameters in LeNet5: 64234\n"
     ]
    }
   ],
   "source": [
    "NParameters_LeNet5 = \\\n",
    "    2*(5*5*1*6) + \\\n",
    "    2*(5*5*6*16) + \\\n",
    "    400*120 + \\\n",
    "    120 + \\\n",
    "    120*84 + \\\n",
    "    84 + \\\n",
    "    84*10 + \\\n",
    "    10\n",
    "    \n",
    "    \n",
    "print('Mumber of parameters in LeNet5: %d'%NParameters_LeNet5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>COMMENT:</b><br/>\n",
    "<ul>\n",
    "<li> Weights and biases for convolutional layer 1 = 2 x (5 x 5 x 1 x 6) </li>\n",
    "<li> Weights and biases for convolutional layer 2 = 2 x (5 x 5 x 6 x 16)</li>\n",
    "<li> Weights for fully connected layer 1 = 400 x 120</li>\n",
    "<li> Biases for fully connected layer 1 = 120</li>\n",
    "<li> Weights for fully connected layer 2 = 120 x 84</li>\n",
    "<li> Biases for fully connected layer 2 = 84</li>\n",
    "<li> Weights for fully connected layer 3 = 84 x 10</li>\n",
    "<li> Biases for fully connected layer 3 = 10</li>\n",
    "</ul>\n",
    "TOTAL: 64234\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.1.3 CNNet: Tensorflow graph creation [Question 2.1.3]\n",
    "The initial training will be using the parameters cited below:\n",
    "\n",
    "     Learning rate =0.1\n",
    "     Loss Function : Cross entropy\n",
    "     Optimisateur: SGD\n",
    "     Number of training iterations= 10000\n",
    "     The batch size =128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def CNNet ( modelName, \n",
    "            learning_rate = 0.1, \n",
    "            training_epochs = 100,\n",
    "            batch_size = 128, \n",
    "            display_step = 1,\n",
    "            keep_p = 1,\n",
    "            activationFunc = tf.nn.sigmoid,\n",
    "            optimFunc = tf.train.GradientDescentOptimizer,\n",
    "            X_train=mnist.train.images, \n",
    "            y_train=mnist.train.labels,\n",
    "            X_val=mnist.validation.images,\n",
    "            y_val=mnist.validation.labels,\n",
    "            X_test= mnist.test.images,\n",
    "            y_test=mnist.test.labels,\n",
    "            loadModel=None\n",
    "          ):\n",
    "    \n",
    "    # ---------- DESCRIPTION OF DATASET ----------\n",
    "    InputSize = X_train[0].shape[0]\n",
    "    OutputSize = y_train[0].shape[0]\n",
    "    TrainingSetSize = len(X_train)\n",
    "    ValidationSetSize = len(X_validation)\n",
    "    TestSetSize = len(X_test)    \n",
    "    \n",
    "    # ---------- OUTPUT FOLDERS ----------\n",
    "    logsFolder = 'log_files/' # useful for tensorboard\n",
    "    saveFolder = 'Models/'    # useful to restore the model\n",
    "    \n",
    "    # ---------- RESET GRAPH ----------\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # ---------- DEFINE VARIABLES ----------\n",
    "    # tf Graph Input:  mnist data image of shape 28*28*1\n",
    "    x = tf.placeholder(tf.float32, [None,InputSize], name='InputData')\n",
    "    # 0-9 digits recognition,  10 classes\n",
    "    y = tf.placeholder(tf.float32, [None,OutputSize], name='LabelData')\n",
    "    # Dropout\n",
    "    keep_prob = tf.placeholder(tf.float32, name='DropoutKeepProbability')\n",
    "\n",
    "    # ---------- DEFINE GRAPH NODES ----------\n",
    "    with tf.name_scope('Model'):\n",
    "        # Model\n",
    "        model = LeNet5_Model(x, keep_prob, activFunc=activationFunc)\n",
    "    \n",
    "    with tf.name_scope('Loss'):\n",
    "        # Minimize error using cross entropy\n",
    "        cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(model+1e-9), reduction_indices=1))\n",
    "        #cost = tf.nn.softmax_cross_entropy_with_logits(model, tf.one_hot(y, 10))\n",
    "    with tf.name_scope('Optimizer'):\n",
    "        #Optimization, using cost reduction\n",
    "        optimizer = optimFunc(learning_rate).minimize(cost)\n",
    "        \n",
    "    with tf.name_scope('Accuracy'):\n",
    "        # Accuracies\n",
    "        acc = CNNetAccuracy(model, y)\n",
    "  \n",
    "        \n",
    "    # ---------- INITIALIZE VARIABLES ----------    \n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "    # ---------- TRACK BATCH LOSS AND ACCURACY ----------\n",
    "    # Create a summary to monitor cost tensor\n",
    "    tf.summary.scalar(\"BatchLoss\", cost)\n",
    "    # Create a summary to monitor batch accuracy tensor\n",
    "    tf.summary.scalar(\"BatchAccuracy\", acc)\n",
    "    # Merge all summaries into a single op\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    # ---------- TRAIN MODEL ----------\n",
    "    CNNtrain(model, cost, optimizer, acc,\n",
    "             x, y, keep_prob, TrainingSetSize,\n",
    "             X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "             init, merged_summary_op,\n",
    "             modelName, saveFolder,logsFolder,\n",
    "             learning_rate, training_epochs, batch_size, display_step , keep_p,\n",
    "             loadModel\n",
    "            )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>COMMENT:</b><br/>\n",
    "The CNNet function creates the whole graph from scratch (Nodes, Variables, Summaries) and calls the function CNNTrain, which will be in charge of creating a session and running the graph.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.1.4 CNNet: Accuracy [Question 2.1.4]  \n",
    "Here we implement the evaluation function for accuracy computation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def CNNetAccuracy(model, y):\n",
    "    accuracy = tf.reduce_mean(\n",
    "               tf.cast( tf.equal( tf.argmax(model, 1),\n",
    "                                  tf.argmax(y, 1)), tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>COMMENT:</b><br/>\n",
    "We've opted for defining the computation the way it should be set inside a Tensorflow graph. However, in order to execute it and obtain a value, we need to create the rest of the graph and use a session to perform the computation (it will be called in the training function)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.1.5 CNNet Training [Question 2.1.5]\n",
    "Here we implement training pipeline and run the training data through it to train the model. Other steps to consider are:\n",
    "\n",
    "- Before each epoch, shuffling the training set. \n",
    "- Printing the loss per mini batch and the training/validation accuracy per epoch. (Display results every 100 epochs)\n",
    "- Saving the model after training\n",
    "- Printing after training the final testing accuracy \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_my_file_format(filename_queue):\n",
    "    reader = tf.SomeReader()\n",
    "    key, record_string = reader.read(filename_queue)\n",
    "    example, label = tf.some_decoder(record_string)\n",
    "    processed_example = some_processing(example)\n",
    "    return processed_example, label\n",
    "\n",
    "def input_pipeline(filenames, batch_size, num_epochs=None):\n",
    "    filename_queue = tf.train.string_input_producer(\n",
    "      filenames, num_epochs=num_epochs, shuffle=True)\n",
    "    example, label = read_my_file_format(filename_queue)\n",
    "    # min_after_dequeue defines how big a buffer we will randomly sample\n",
    "    #   from -- bigger means better shuffling but slower start up and more\n",
    "    #   memory used.\n",
    "    # capacity must be larger than min_after_dequeue and the amount larger\n",
    "    #   determines the maximum we will prefetch.  Recommendation:\n",
    "    #   min_after_dequeue + (num_threads + a small safety margin) * batch_size\n",
    "    min_after_dequeue = 10000\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    example_batch, label_batch = tf.train.shuffle_batch(\n",
    "      [example, label], batch_size=batch_size, capacity=capacity,\n",
    "      min_after_dequeue=min_after_dequeue)\n",
    "    return example_batch, label_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>COMMENT:</b><br/>\n",
    "We import the pipeline functions that come from tensorflow: https://www.tensorflow.org/programmers_guide/reading_data\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def CNNtrain(model, cost, optimizer, acc,\n",
    "             x, y, keep_prob, TrainingSetSize,\n",
    "             X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "             init, merged_summary_op,\n",
    "             modelName, saveFolder, logsFolder,\n",
    "             learning_rate, training_epochs, batch_size, display_step, keep_p,\n",
    "             loadModel, dataFileName=None\n",
    "            ):\n",
    "    \n",
    "    # Initial model print\n",
    "    print(\"*Model [\", modelName,\"] {l_r: %.4f; n_iter: %d; batch: %d}\"%\\\n",
    "          (learning_rate, training_epochs, batch_size))\n",
    "    \n",
    "    # Start a tensorflow session\n",
    "    with tf.Session() as sess:\n",
    "        print (\"   Start Training!\")\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Load model if the parameter loadModel is not empty\n",
    "        saver = tf.train.Saver()\n",
    "        if(loadModel):\n",
    "            saver.restore(sess=sess,save_path='Models/'+loadModel)\n",
    "        \n",
    "        # op to write logs to Tensorboard\n",
    "        summary_writer = tf.summary.FileWriter(logsFolder,\n",
    "                                               graph=tf.get_default_graph())\n",
    "        \n",
    "        # Training cycle\n",
    "        t0 = time.time()\n",
    "        for epoch in range(training_epochs):\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(mnist.train.num_examples/batch_size)\n",
    "            # Loop over all batches\n",
    "            for i in range(total_batch):\n",
    "                \n",
    "                # batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "                batch_xs, batch_ys = input_pipeline(filenames=dataFileName, \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    num_epochs=total_batch):\n",
    "                # Run optimization op (backprop), cost op (to get loss value)\n",
    "                # and summary nodes\n",
    "                _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                          feed_dict={x: batch_xs,\n",
    "                                                     y: batch_ys,\n",
    "                                                     keep_prob: keep_p})\n",
    "                # Write logs at every iteration\n",
    "                summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "                # Compute average loss\n",
    "                avg_cost += c / total_batch\n",
    "\n",
    "            # Display logs per epoch step\n",
    "            if (epoch+1) % display_step == 0:\n",
    "                tr_acc = acc.eval({x: X_train, y: y_train, keep_prob: 1})\n",
    "                vl_acc = acc.eval({x: X_val, y: y_val, keep_prob: 1})\n",
    "                print(\"   Epoch: %02d | Loss=%.9f | TrainAcc=%.3f %% | ValAcc=%.3f %%\"% \n",
    "                      (epoch+1, avg_cost, tr_acc*100, vl_acc*100));\n",
    "        \n",
    "        \n",
    "        print (\"   Training Finished in %.1f seconds.\"%(time.time()-t0))\n",
    "        \n",
    "        # Evaluating model with the accuracies\n",
    "        print (\"   Final accuracies:\")\n",
    "        print (\"   ~ TrainAcc: %.3f %%\"%(100*acc.eval({x: X_train, y: y_train, keep_prob: 1})))\n",
    "        print (\"   ~ ValAcc: %.3f %%\"%(100*acc.eval({x: X_val, y: y_val, keep_prob: 1})))\n",
    "        print (\"   ~ TestAcc: %.3f %%\"%(100*acc.eval({x: X_test, y: y_test, keep_prob: 1})))\n",
    "        \n",
    "        # Saving Model\n",
    "        saver.save(sess=sess,save_path=saveFolder+modelName)\n",
    "        print (\"   Saving model in file: %s\"%(saveFolder+modelName))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>COMMENT:</b><br/>\n",
    "This function implements the session, and using and input pipeline, it fetches batches of data to the graph and runs it in order to obtain results.<br/><br/>\n",
    "The model is evaluated in the end, and we have added a possibility to load previous models as well as saving it. The training and validation accuracies are calculated on every epoch, while test accuracy is only calculated in the end.<br/><br/>\n",
    "In order to keep the original version available, we have commented the original batch function from the mnist dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Model [ lenet5-model ] {l_r: 0.1000; n_iter: 100; batch: 128}\n",
      "   Start Training!\n",
      "   Epoch: 01 | Loss=2.306631047 | TrainAcc=10.391 %| ValAcc=11.000 %\n",
      "   Epoch: 02 | Loss=2.305539821 | TrainAcc=10.251 %| ValAcc=9.860 %\n",
      "   Epoch: 03 | Loss=2.304715242 | TrainAcc=11.235 %| ValAcc=11.260 %\n",
      "   Epoch: 04 | Loss=2.304092920 | TrainAcc=9.945 %| ValAcc=9.760 %\n",
      "   Epoch: 05 | Loss=2.303851811 | TrainAcc=10.391 %| ValAcc=11.000 %\n",
      "   Epoch: 06 | Loss=2.302999963 | TrainAcc=10.391 %| ValAcc=11.000 %\n",
      "   Epoch: 07 | Loss=2.301510114 | TrainAcc=9.945 %| ValAcc=9.760 %\n",
      "   Epoch: 08 | Loss=2.298957096 | TrainAcc=9.945 %| ValAcc=9.760 %\n",
      "   Epoch: 09 | Loss=2.290840972 | TrainAcc=23.744 %| ValAcc=23.080 %\n",
      "   Epoch: 10 | Loss=2.217125939 | TrainAcc=45.135 %| ValAcc=45.980 %\n",
      "   Epoch: 11 | Loss=1.557864240 | TrainAcc=63.400 %| ValAcc=63.560 %\n",
      "   Epoch: 12 | Loss=0.869937158 | TrainAcc=79.955 %| ValAcc=80.900 %\n",
      "   Epoch: 13 | Loss=0.587728509 | TrainAcc=85.653 %| ValAcc=86.280 %\n",
      "   Epoch: 14 | Loss=0.442426659 | TrainAcc=89.376 %| ValAcc=89.680 %\n",
      "   Epoch: 15 | Loss=0.339899377 | TrainAcc=91.262 %| ValAcc=91.840 %\n",
      "   Epoch: 16 | Loss=0.284057041 | TrainAcc=92.544 %| ValAcc=93.500 %\n",
      "   Epoch: 17 | Loss=0.246779823 | TrainAcc=93.407 %| ValAcc=94.320 %\n",
      "   Epoch: 18 | Loss=0.213511055 | TrainAcc=94.040 %| ValAcc=94.680 %\n",
      "   Epoch: 19 | Loss=0.193927778 | TrainAcc=94.685 %| ValAcc=95.140 %\n",
      "   Epoch: 20 | Loss=0.175255372 | TrainAcc=95.049 %| ValAcc=95.460 %\n",
      "   Epoch: 21 | Loss=0.162582722 | TrainAcc=95.522 %| ValAcc=95.720 %\n",
      "   Epoch: 22 | Loss=0.147062424 | TrainAcc=95.782 %| ValAcc=95.960 %\n",
      "   Epoch: 23 | Loss=0.143401140 | TrainAcc=96.045 %| ValAcc=96.360 %\n",
      "   Epoch: 24 | Loss=0.127880746 | TrainAcc=96.325 %| ValAcc=96.600 %\n",
      "   Epoch: 25 | Loss=0.122923835 | TrainAcc=96.569 %| ValAcc=96.640 %\n",
      "   Epoch: 26 | Loss=0.116992221 | TrainAcc=96.664 %| ValAcc=96.680 %\n",
      "   Epoch: 27 | Loss=0.110676361 | TrainAcc=96.887 %| ValAcc=96.880 %\n",
      "   Epoch: 28 | Loss=0.105750041 | TrainAcc=97.038 %| ValAcc=97.160 %\n",
      "   Epoch: 29 | Loss=0.100782192 | TrainAcc=97.111 %| ValAcc=97.120 %\n",
      "   Epoch: 30 | Loss=0.093881377 | TrainAcc=97.255 %| ValAcc=97.500 %\n",
      "   Epoch: 31 | Loss=0.094529159 | TrainAcc=97.355 %| ValAcc=97.440 %\n",
      "   Epoch: 32 | Loss=0.086472801 | TrainAcc=97.460 %| ValAcc=97.480 %\n",
      "   Epoch: 33 | Loss=0.086841481 | TrainAcc=97.507 %| ValAcc=97.500 %\n",
      "   Epoch: 34 | Loss=0.082052249 | TrainAcc=97.651 %| ValAcc=97.620 %\n",
      "   Epoch: 35 | Loss=0.081445980 | TrainAcc=97.756 %| ValAcc=97.800 %\n",
      "   Epoch: 36 | Loss=0.076425658 | TrainAcc=97.789 %| ValAcc=97.880 %\n",
      "   Epoch: 37 | Loss=0.075406765 | TrainAcc=97.831 %| ValAcc=97.620 %\n",
      "   Epoch: 38 | Loss=0.071945635 | TrainAcc=97.787 %| ValAcc=97.760 %\n",
      "   Epoch: 39 | Loss=0.072156838 | TrainAcc=97.987 %| ValAcc=97.880 %\n",
      "   Epoch: 40 | Loss=0.067399871 | TrainAcc=97.975 %| ValAcc=97.860 %\n",
      "   Epoch: 41 | Loss=0.066895949 | TrainAcc=98.065 %| ValAcc=98.080 %\n",
      "   Epoch: 42 | Loss=0.065822107 | TrainAcc=98.071 %| ValAcc=98.020 %\n",
      "   Epoch: 43 | Loss=0.064373688 | TrainAcc=98.178 %| ValAcc=97.900 %\n",
      "   Epoch: 44 | Loss=0.061473853 | TrainAcc=98.116 %| ValAcc=97.740 %\n",
      "   Epoch: 45 | Loss=0.060343402 | TrainAcc=98.195 %| ValAcc=97.940 %\n",
      "   Epoch: 46 | Loss=0.060677404 | TrainAcc=98.275 %| ValAcc=98.060 %\n",
      "   Epoch: 47 | Loss=0.057433997 | TrainAcc=98.316 %| ValAcc=98.180 %\n",
      "   Epoch: 48 | Loss=0.058747132 | TrainAcc=98.244 %| ValAcc=97.800 %\n",
      "   Epoch: 49 | Loss=0.055160534 | TrainAcc=98.387 %| ValAcc=98.120 %\n",
      "   Epoch: 50 | Loss=0.053796961 | TrainAcc=98.433 %| ValAcc=98.200 %\n",
      "   Epoch: 51 | Loss=0.054843310 | TrainAcc=98.427 %| ValAcc=98.340 %\n",
      "   Epoch: 52 | Loss=0.052881928 | TrainAcc=98.502 %| ValAcc=98.280 %\n",
      "   Epoch: 53 | Loss=0.052524625 | TrainAcc=98.473 %| ValAcc=98.400 %\n",
      "   Epoch: 54 | Loss=0.049642457 | TrainAcc=98.575 %| ValAcc=98.280 %\n",
      "   Epoch: 55 | Loss=0.051047719 | TrainAcc=98.582 %| ValAcc=98.420 %\n",
      "   Epoch: 56 | Loss=0.049000237 | TrainAcc=98.565 %| ValAcc=98.320 %\n",
      "   Epoch: 57 | Loss=0.047842143 | TrainAcc=98.600 %| ValAcc=98.420 %\n",
      "   Epoch: 58 | Loss=0.048208271 | TrainAcc=98.585 %| ValAcc=98.240 %\n",
      "   Epoch: 59 | Loss=0.048406639 | TrainAcc=98.675 %| ValAcc=98.480 %\n",
      "   Epoch: 60 | Loss=0.045644873 | TrainAcc=98.705 %| ValAcc=98.520 %\n",
      "   Epoch: 61 | Loss=0.045256516 | TrainAcc=98.651 %| ValAcc=98.340 %\n",
      "   Epoch: 62 | Loss=0.043868340 | TrainAcc=98.722 %| ValAcc=98.440 %\n",
      "   Epoch: 63 | Loss=0.043714555 | TrainAcc=98.736 %| ValAcc=98.620 %\n",
      "   Epoch: 64 | Loss=0.044404055 | TrainAcc=98.707 %| ValAcc=98.420 %\n",
      "   Epoch: 65 | Loss=0.042454557 | TrainAcc=98.800 %| ValAcc=98.560 %\n",
      "   Epoch: 66 | Loss=0.041678178 | TrainAcc=98.791 %| ValAcc=98.480 %\n",
      "   Epoch: 67 | Loss=0.040897159 | TrainAcc=98.847 %| ValAcc=98.620 %\n",
      "   Epoch: 68 | Loss=0.041794721 | TrainAcc=98.893 %| ValAcc=98.660 %\n",
      "   Epoch: 69 | Loss=0.040252054 | TrainAcc=98.867 %| ValAcc=98.600 %\n",
      "   Epoch: 70 | Loss=0.039880601 | TrainAcc=98.742 %| ValAcc=98.500 %\n",
      "   Epoch: 71 | Loss=0.039827537 | TrainAcc=98.927 %| ValAcc=98.580 %\n",
      "   Epoch: 72 | Loss=0.038584120 | TrainAcc=98.904 %| ValAcc=98.660 %\n",
      "   Epoch: 73 | Loss=0.038074038 | TrainAcc=98.947 %| ValAcc=98.640 %\n",
      "   Epoch: 74 | Loss=0.037141427 | TrainAcc=98.858 %| ValAcc=98.620 %\n",
      "   Epoch: 75 | Loss=0.037215871 | TrainAcc=98.940 %| ValAcc=98.660 %\n",
      "   Epoch: 76 | Loss=0.037616341 | TrainAcc=98.987 %| ValAcc=98.660 %\n",
      "   Epoch: 77 | Loss=0.034986995 | TrainAcc=98.984 %| ValAcc=98.580 %\n",
      "   Epoch: 78 | Loss=0.036369161 | TrainAcc=98.938 %| ValAcc=98.580 %\n",
      "   Epoch: 79 | Loss=0.035246240 | TrainAcc=98.980 %| ValAcc=98.580 %\n",
      "   Epoch: 80 | Loss=0.034594428 | TrainAcc=99.025 %| ValAcc=98.640 %\n",
      "   Epoch: 81 | Loss=0.034820716 | TrainAcc=99.036 %| ValAcc=98.640 %\n",
      "   Epoch: 82 | Loss=0.033643226 | TrainAcc=99.095 %| ValAcc=98.640 %\n",
      "   Epoch: 83 | Loss=0.034329354 | TrainAcc=99.065 %| ValAcc=98.740 %\n",
      "   Epoch: 84 | Loss=0.033407850 | TrainAcc=99.085 %| ValAcc=98.740 %\n",
      "   Epoch: 85 | Loss=0.032714229 | TrainAcc=99.109 %| ValAcc=98.660 %\n",
      "   Epoch: 86 | Loss=0.033154654 | TrainAcc=99.122 %| ValAcc=98.700 %\n",
      "   Epoch: 87 | Loss=0.030976301 | TrainAcc=99.104 %| ValAcc=98.720 %\n",
      "   Epoch: 88 | Loss=0.031782167 | TrainAcc=99.144 %| ValAcc=98.660 %\n",
      "   Epoch: 89 | Loss=0.031738917 | TrainAcc=99.125 %| ValAcc=98.660 %\n",
      "   Epoch: 90 | Loss=0.030860026 | TrainAcc=99.165 %| ValAcc=98.700 %\n",
      "   Epoch: 91 | Loss=0.031554804 | TrainAcc=99.138 %| ValAcc=98.740 %\n",
      "   Epoch: 92 | Loss=0.028973189 | TrainAcc=99.189 %| ValAcc=98.760 %\n",
      "   Epoch: 93 | Loss=0.030849772 | TrainAcc=99.131 %| ValAcc=98.720 %\n",
      "   Epoch: 94 | Loss=0.029520935 | TrainAcc=99.236 %| ValAcc=98.740 %\n",
      "   Epoch: 95 | Loss=0.030010340 | TrainAcc=99.144 %| ValAcc=98.740 %\n",
      "   Epoch: 96 | Loss=0.029015477 | TrainAcc=99.231 %| ValAcc=98.640 %\n",
      "   Epoch: 97 | Loss=0.027510516 | TrainAcc=99.211 %| ValAcc=98.700 %\n",
      "   Epoch: 98 | Loss=0.028949183 | TrainAcc=99.269 %| ValAcc=98.760 %\n",
      "   Epoch: 99 | Loss=0.026873244 | TrainAcc=99.182 %| ValAcc=98.740 %\n",
      "   Epoch: 100 | Loss=0.028509810 | TrainAcc=99.273 %| ValAcc=98.760 %\n",
      "   Training Finished in 2687.3 seconds.\n",
      "   Final accuracies:\n",
      "   ~ TrainAcc: 99.273 %\n",
      "   ~ ValAcc: 98.760 %\n",
      "   ~ TestAcc: 98.760 %\n",
      "   Saving model in file: Models/lenet5-model\n"
     ]
    }
   ],
   "source": [
    "# Training our first model!\n",
    "CNNet ('lenet5-model', \n",
    "           learning_rate = 0.1, \n",
    "           training_epochs = 100,\n",
    "           batch_size = 128,\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>COMMENT:</b><br/>\n",
    "First accuracy obtained is 98.76%! very close to the objective of 99%.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.1.6 Visualization of results with Tensorboard [Question 2.1.6]\n",
    "We use tensorBoard to visualise and save the LeNet5 Graph and all learning curves. \n",
    "The data is then converted into CSV using the GUI drom tensorboard and is then plotted using Excel. The resulting figures are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"MNIST_99_Challenge_Figures/LeNet5_graph.png\",width=\"800\" height=\"600\" align=\"center\">\n",
    "<center><span>Figure 3: LeNet5 Graph </span></center>\n",
    "<img src=\"MNIST_99_Challenge_Figures/TrainingLeNet5.png\",width=\"800\" height=\"600\" align=\"center\">\n",
    "<center><span>Figure 4: LeNet5 Training </span></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>COMMENT:</b><br/>\n",
    "There is an initial step of search for a steep surface in the loss function, and in the epoch 8-9 it starts converging fast, completely stabilizing after epoch 50.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2.2 LeNet5 Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "## 2.2.1 Parameter Tuning [Question 2.2.1]  \n",
    "\n",
    "We change the sigmoid function to a Relu and perform the next steps:\n",
    "\n",
    "- Retrain the network with SGD and AdamOptimizer. Compare them with the best parameters:\n",
    "\n",
    "\n",
    "| Optimizer            |  Gradient Descent         |AdamOptimizer |\n",
    "| -------------        |: -------------: | ---------:   \n",
    "| Validation Accuracy  |    98.760 %   |  99.080 %  |      \n",
    "| Testing Accuracy     |      98.670 %     |  99.110 %  |       \n",
    "| Training Time        |     8048 s      |     2670 s   |  \n",
    "\n",
    "\n",
    "- Try with different learning rates for each Optimizer (0.0001 and 0.001 ) and different Batch sizes (50 and 128) for 10000 Epochs. \n",
    "\n",
    "- For each optimizer, plot (on the same curve) the **testing accuracies** function to **(learning rate, batch size)** \n",
    "\n",
    "\n",
    "- Did you reach the 99% accuracy ? What are the optimal parametres that gave you the best results? \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"MNIST_99_Challenge_Figures/ParameterTuning.png\",width=\"800\" height=\"600\" align=\"center\">\n",
    "<center><span>Figure 5: Parameter Tuning </span></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>COMMENT:</b><br/>\n",
    "Clearly the Adam Optimizer is better than the SGD, and in much less time. <br/><br/>\n",
    "We tried with several parameters and thebest combination we found is <b>AdamOptimizer, lr=0.001 bs=128</b>.<br/><br/>\n",
    "Also, we achieved the desired accuracy of 99%!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Model [ lenet5-model_relu_lr0001_bs50 ] {l_r: 0.0010; n_iter: 250; batch: 50}\n",
      "   Start Training!\n",
      "   Epoch: 01 | Loss=2.285461369 | TrainAcc=32.773 %| ValAcc=32.020 %\n",
      "   Epoch: 02 | Loss=2.132812783 | TrainAcc=59.864 %| ValAcc=59.840 %\n",
      "   Epoch: 03 | Loss=1.361398856 | TrainAcc=81.147 %| ValAcc=82.020 %\n",
      "   Epoch: 04 | Loss=0.553005308 | TrainAcc=87.944 %| ValAcc=88.600 %\n",
      "   Epoch: 05 | Loss=0.384707743 | TrainAcc=89.689 %| ValAcc=90.440 %\n",
      "   Epoch: 06 | Loss=0.323377239 | TrainAcc=90.982 %| ValAcc=92.000 %\n",
      "   Epoch: 07 | Loss=0.287973725 | TrainAcc=91.936 %| ValAcc=92.680 %\n",
      "   Epoch: 08 | Loss=0.263154375 | TrainAcc=92.489 %| ValAcc=93.280 %\n",
      "   Epoch: 09 | Loss=0.239348642 | TrainAcc=93.165 %| ValAcc=93.920 %\n",
      "   Epoch: 10 | Loss=0.222846294 | TrainAcc=93.533 %| ValAcc=94.260 %\n",
      "   Epoch: 11 | Loss=0.211532476 | TrainAcc=93.960 %| ValAcc=94.620 %\n",
      "   Epoch: 12 | Loss=0.196222519 | TrainAcc=94.267 %| ValAcc=94.860 %\n",
      "   Epoch: 13 | Loss=0.184968149 | TrainAcc=94.775 %| ValAcc=95.500 %\n",
      "   Epoch: 14 | Loss=0.171088510 | TrainAcc=95.025 %| ValAcc=95.720 %\n",
      "   Epoch: 15 | Loss=0.165861866 | TrainAcc=95.309 %| ValAcc=95.800 %\n",
      "   Epoch: 16 | Loss=0.160169858 | TrainAcc=95.518 %| ValAcc=95.960 %\n",
      "   Epoch: 17 | Loss=0.149477239 | TrainAcc=95.689 %| ValAcc=96.140 %\n",
      "   Epoch: 18 | Loss=0.144087977 | TrainAcc=95.689 %| ValAcc=96.160 %\n",
      "   Epoch: 19 | Loss=0.137406570 | TrainAcc=96.029 %| ValAcc=96.480 %\n",
      "   Epoch: 20 | Loss=0.133413623 | TrainAcc=96.204 %| ValAcc=96.540 %\n",
      "   Epoch: 21 | Loss=0.128573287 | TrainAcc=96.293 %| ValAcc=96.600 %\n",
      "   Epoch: 22 | Loss=0.123372656 | TrainAcc=96.427 %| ValAcc=96.900 %\n",
      "   Epoch: 23 | Loss=0.119049564 | TrainAcc=96.502 %| ValAcc=96.880 %\n",
      "   Epoch: 24 | Loss=0.116334625 | TrainAcc=96.631 %| ValAcc=96.940 %\n",
      "   Epoch: 25 | Loss=0.109801858 | TrainAcc=96.707 %| ValAcc=97.060 %\n",
      "   Epoch: 26 | Loss=0.112535800 | TrainAcc=96.818 %| ValAcc=97.120 %\n",
      "   Epoch: 27 | Loss=0.105374946 | TrainAcc=96.898 %| ValAcc=97.300 %\n",
      "   Epoch: 28 | Loss=0.104557877 | TrainAcc=96.958 %| ValAcc=97.060 %\n",
      "   Epoch: 29 | Loss=0.102630379 | TrainAcc=97.067 %| ValAcc=97.260 %\n",
      "   Epoch: 30 | Loss=0.096915770 | TrainAcc=97.085 %| ValAcc=97.220 %\n",
      "   Epoch: 31 | Loss=0.098936162 | TrainAcc=97.173 %| ValAcc=97.340 %\n",
      "   Epoch: 32 | Loss=0.094192208 | TrainAcc=97.093 %| ValAcc=97.240 %\n",
      "   Epoch: 33 | Loss=0.091378372 | TrainAcc=97.285 %| ValAcc=97.400 %\n",
      "   Epoch: 34 | Loss=0.093414419 | TrainAcc=97.318 %| ValAcc=97.420 %\n",
      "   Epoch: 35 | Loss=0.087457429 | TrainAcc=97.405 %| ValAcc=97.540 %\n",
      "   Epoch: 36 | Loss=0.086215350 | TrainAcc=97.445 %| ValAcc=97.520 %\n",
      "   Epoch: 37 | Loss=0.087351593 | TrainAcc=97.484 %| ValAcc=97.520 %\n",
      "   Epoch: 38 | Loss=0.082163252 | TrainAcc=97.529 %| ValAcc=97.600 %\n",
      "   Epoch: 39 | Loss=0.083657361 | TrainAcc=97.480 %| ValAcc=97.560 %\n",
      "   Epoch: 40 | Loss=0.082037196 | TrainAcc=97.611 %| ValAcc=97.560 %\n",
      "   Epoch: 41 | Loss=0.078911429 | TrainAcc=97.616 %| ValAcc=97.580 %\n",
      "   Epoch: 42 | Loss=0.079888741 | TrainAcc=97.665 %| ValAcc=97.680 %\n",
      "   Epoch: 43 | Loss=0.078120012 | TrainAcc=97.729 %| ValAcc=97.640 %\n",
      "   Epoch: 44 | Loss=0.077282885 | TrainAcc=97.662 %| ValAcc=97.820 %\n",
      "   Epoch: 45 | Loss=0.075211456 | TrainAcc=97.747 %| ValAcc=97.620 %\n",
      "   Epoch: 46 | Loss=0.071960739 | TrainAcc=97.744 %| ValAcc=97.840 %\n",
      "   Epoch: 47 | Loss=0.072970361 | TrainAcc=97.776 %| ValAcc=97.780 %\n",
      "   Epoch: 48 | Loss=0.073176821 | TrainAcc=97.858 %| ValAcc=97.720 %\n",
      "   Epoch: 49 | Loss=0.070212999 | TrainAcc=97.876 %| ValAcc=97.860 %\n",
      "   Epoch: 50 | Loss=0.072299475 | TrainAcc=97.918 %| ValAcc=97.840 %\n",
      "   Epoch: 51 | Loss=0.069322755 | TrainAcc=97.820 %| ValAcc=97.720 %\n",
      "   Epoch: 52 | Loss=0.067632132 | TrainAcc=97.909 %| ValAcc=97.680 %\n",
      "   Epoch: 53 | Loss=0.068068573 | TrainAcc=98.020 %| ValAcc=97.820 %\n",
      "   Epoch: 54 | Loss=0.068350828 | TrainAcc=98.036 %| ValAcc=97.780 %\n",
      "   Epoch: 55 | Loss=0.064534768 | TrainAcc=98.053 %| ValAcc=97.860 %\n",
      "   Epoch: 56 | Loss=0.065155246 | TrainAcc=98.064 %| ValAcc=97.820 %\n",
      "   Epoch: 57 | Loss=0.063375496 | TrainAcc=98.115 %| ValAcc=97.780 %\n",
      "   Epoch: 58 | Loss=0.064139586 | TrainAcc=98.060 %| ValAcc=97.860 %\n",
      "   Epoch: 59 | Loss=0.063935661 | TrainAcc=98.135 %| ValAcc=97.920 %\n",
      "   Epoch: 60 | Loss=0.061815903 | TrainAcc=98.067 %| ValAcc=97.720 %\n",
      "   Epoch: 61 | Loss=0.060524897 | TrainAcc=98.156 %| ValAcc=97.840 %\n",
      "   Epoch: 62 | Loss=0.059319592 | TrainAcc=98.153 %| ValAcc=97.860 %\n",
      "   Epoch: 63 | Loss=0.062925885 | TrainAcc=98.082 %| ValAcc=97.820 %\n",
      "   Epoch: 64 | Loss=0.058194372 | TrainAcc=98.218 %| ValAcc=97.900 %\n",
      "   Epoch: 65 | Loss=0.058804354 | TrainAcc=98.224 %| ValAcc=97.860 %\n",
      "   Epoch: 66 | Loss=0.058604298 | TrainAcc=98.304 %| ValAcc=97.940 %\n",
      "   Epoch: 67 | Loss=0.058870856 | TrainAcc=98.309 %| ValAcc=98.120 %\n",
      "   Epoch: 68 | Loss=0.057657476 | TrainAcc=98.300 %| ValAcc=97.980 %\n",
      "   Epoch: 69 | Loss=0.056509917 | TrainAcc=98.324 %| ValAcc=98.160 %\n",
      "   Epoch: 70 | Loss=0.055091935 | TrainAcc=98.316 %| ValAcc=97.960 %\n",
      "   Epoch: 71 | Loss=0.056100632 | TrainAcc=98.365 %| ValAcc=98.000 %\n",
      "   Epoch: 72 | Loss=0.054292002 | TrainAcc=98.365 %| ValAcc=97.960 %\n",
      "   Epoch: 73 | Loss=0.053274232 | TrainAcc=98.420 %| ValAcc=98.020 %\n",
      "   Epoch: 74 | Loss=0.056076834 | TrainAcc=98.375 %| ValAcc=98.120 %\n",
      "   Epoch: 75 | Loss=0.053387047 | TrainAcc=98.358 %| ValAcc=98.200 %\n",
      "   Epoch: 76 | Loss=0.051661383 | TrainAcc=98.380 %| ValAcc=98.200 %\n",
      "   Epoch: 77 | Loss=0.053674746 | TrainAcc=98.436 %| ValAcc=98.160 %\n",
      "   Epoch: 78 | Loss=0.052451015 | TrainAcc=98.429 %| ValAcc=98.040 %\n",
      "   Epoch: 79 | Loss=0.051703369 | TrainAcc=98.464 %| ValAcc=98.000 %\n",
      "   Epoch: 80 | Loss=0.051153506 | TrainAcc=98.413 %| ValAcc=98.020 %\n",
      "   Epoch: 81 | Loss=0.049609337 | TrainAcc=98.518 %| ValAcc=98.260 %\n",
      "   Epoch: 82 | Loss=0.050663759 | TrainAcc=98.507 %| ValAcc=98.200 %\n",
      "   Epoch: 83 | Loss=0.049329799 | TrainAcc=98.458 %| ValAcc=98.020 %\n",
      "   Epoch: 84 | Loss=0.050106153 | TrainAcc=98.518 %| ValAcc=98.100 %\n",
      "   Epoch: 85 | Loss=0.048873586 | TrainAcc=98.484 %| ValAcc=98.120 %\n",
      "   Epoch: 86 | Loss=0.047096422 | TrainAcc=98.465 %| ValAcc=98.140 %\n",
      "   Epoch: 87 | Loss=0.049887148 | TrainAcc=98.575 %| ValAcc=98.180 %\n",
      "   Epoch: 88 | Loss=0.046943648 | TrainAcc=98.585 %| ValAcc=98.120 %\n",
      "   Epoch: 89 | Loss=0.045796685 | TrainAcc=98.591 %| ValAcc=98.200 %\n",
      "   Epoch: 90 | Loss=0.048680889 | TrainAcc=98.595 %| ValAcc=98.140 %\n",
      "   Epoch: 91 | Loss=0.045938718 | TrainAcc=98.689 %| ValAcc=98.260 %\n",
      "   Epoch: 92 | Loss=0.047030030 | TrainAcc=98.622 %| ValAcc=98.120 %\n",
      "   Epoch: 93 | Loss=0.044578014 | TrainAcc=98.716 %| ValAcc=98.240 %\n",
      "   Epoch: 94 | Loss=0.046290484 | TrainAcc=98.729 %| ValAcc=98.300 %\n",
      "   Epoch: 95 | Loss=0.045873421 | TrainAcc=98.649 %| ValAcc=98.380 %\n",
      "   Epoch: 96 | Loss=0.043261532 | TrainAcc=98.718 %| ValAcc=98.280 %\n",
      "   Epoch: 97 | Loss=0.045893035 | TrainAcc=98.740 %| ValAcc=98.320 %\n",
      "   Epoch: 98 | Loss=0.043294866 | TrainAcc=98.582 %| ValAcc=98.180 %\n",
      "   Epoch: 99 | Loss=0.045202967 | TrainAcc=98.749 %| ValAcc=98.240 %\n",
      "   Epoch: 100 | Loss=0.043704280 | TrainAcc=98.782 %| ValAcc=98.300 %\n",
      "   Epoch: 101 | Loss=0.041505274 | TrainAcc=98.711 %| ValAcc=98.320 %\n",
      "   Epoch: 102 | Loss=0.043946795 | TrainAcc=98.698 %| ValAcc=98.380 %\n",
      "   Epoch: 103 | Loss=0.040998889 | TrainAcc=98.736 %| ValAcc=98.340 %\n",
      "   Epoch: 104 | Loss=0.042414350 | TrainAcc=98.773 %| ValAcc=98.440 %\n",
      "   Epoch: 105 | Loss=0.040338580 | TrainAcc=98.689 %| ValAcc=98.320 %\n",
      "   Epoch: 106 | Loss=0.042805888 | TrainAcc=98.705 %| ValAcc=98.240 %\n",
      "   Epoch: 107 | Loss=0.039416220 | TrainAcc=98.745 %| ValAcc=98.400 %\n",
      "   Epoch: 108 | Loss=0.041300113 | TrainAcc=98.891 %| ValAcc=98.380 %\n",
      "   Epoch: 109 | Loss=0.040174271 | TrainAcc=98.707 %| ValAcc=98.220 %\n",
      "   Epoch: 110 | Loss=0.040036633 | TrainAcc=98.896 %| ValAcc=98.440 %\n",
      "   Epoch: 111 | Loss=0.040614730 | TrainAcc=98.816 %| ValAcc=98.420 %\n",
      "   Epoch: 112 | Loss=0.038078927 | TrainAcc=98.887 %| ValAcc=98.540 %\n",
      "   Epoch: 113 | Loss=0.040605902 | TrainAcc=98.776 %| ValAcc=98.340 %\n",
      "   Epoch: 114 | Loss=0.038576678 | TrainAcc=98.913 %| ValAcc=98.500 %\n",
      "   Epoch: 115 | Loss=0.039944093 | TrainAcc=98.780 %| ValAcc=98.380 %\n",
      "   Epoch: 116 | Loss=0.038215257 | TrainAcc=98.891 %| ValAcc=98.460 %\n",
      "   Epoch: 117 | Loss=0.038546574 | TrainAcc=98.893 %| ValAcc=98.220 %\n",
      "   Epoch: 118 | Loss=0.037203973 | TrainAcc=98.984 %| ValAcc=98.520 %\n",
      "   Epoch: 119 | Loss=0.038822611 | TrainAcc=98.964 %| ValAcc=98.420 %\n",
      "   Epoch: 120 | Loss=0.035974897 | TrainAcc=98.936 %| ValAcc=98.380 %\n",
      "   Epoch: 121 | Loss=0.038453726 | TrainAcc=98.958 %| ValAcc=98.600 %\n",
      "   Epoch: 122 | Loss=0.036388228 | TrainAcc=98.969 %| ValAcc=98.580 %\n",
      "   Epoch: 123 | Loss=0.037233435 | TrainAcc=98.985 %| ValAcc=98.600 %\n",
      "   Epoch: 124 | Loss=0.035949366 | TrainAcc=98.969 %| ValAcc=98.500 %\n",
      "   Epoch: 125 | Loss=0.035691601 | TrainAcc=98.891 %| ValAcc=98.460 %\n",
      "   Epoch: 126 | Loss=0.036916094 | TrainAcc=98.864 %| ValAcc=98.440 %\n",
      "   Epoch: 127 | Loss=0.036189874 | TrainAcc=98.976 %| ValAcc=98.540 %\n",
      "   Epoch: 128 | Loss=0.035201176 | TrainAcc=98.873 %| ValAcc=98.280 %\n",
      "   Epoch: 129 | Loss=0.035285351 | TrainAcc=98.960 %| ValAcc=98.420 %\n",
      "   Epoch: 130 | Loss=0.034165581 | TrainAcc=98.985 %| ValAcc=98.540 %\n",
      "   Epoch: 131 | Loss=0.035110146 | TrainAcc=99.062 %| ValAcc=98.500 %\n",
      "   Epoch: 132 | Loss=0.035466153 | TrainAcc=99.007 %| ValAcc=98.480 %\n",
      "   Epoch: 133 | Loss=0.034104754 | TrainAcc=98.984 %| ValAcc=98.480 %\n",
      "   Epoch: 134 | Loss=0.032952479 | TrainAcc=98.996 %| ValAcc=98.560 %\n",
      "   Epoch: 135 | Loss=0.034341251 | TrainAcc=99.053 %| ValAcc=98.380 %\n",
      "   Epoch: 136 | Loss=0.034164459 | TrainAcc=99.009 %| ValAcc=98.480 %\n",
      "   Epoch: 137 | Loss=0.031646022 | TrainAcc=99.100 %| ValAcc=98.700 %\n",
      "   Epoch: 138 | Loss=0.034897388 | TrainAcc=99.062 %| ValAcc=98.440 %\n",
      "   Epoch: 139 | Loss=0.031209846 | TrainAcc=99.067 %| ValAcc=98.540 %\n",
      "   Epoch: 140 | Loss=0.034334577 | TrainAcc=99.035 %| ValAcc=98.500 %\n",
      "   Epoch: 141 | Loss=0.030958260 | TrainAcc=99.080 %| ValAcc=98.580 %\n",
      "   Epoch: 142 | Loss=0.033168876 | TrainAcc=99.111 %| ValAcc=98.560 %\n",
      "   Epoch: 143 | Loss=0.031770934 | TrainAcc=99.138 %| ValAcc=98.640 %\n",
      "   Epoch: 144 | Loss=0.030609913 | TrainAcc=99.122 %| ValAcc=98.580 %\n",
      "   Epoch: 145 | Loss=0.031782280 | TrainAcc=98.956 %| ValAcc=98.380 %\n",
      "   Epoch: 146 | Loss=0.030870964 | TrainAcc=99.056 %| ValAcc=98.460 %\n",
      "   Epoch: 147 | Loss=0.032422362 | TrainAcc=99.131 %| ValAcc=98.520 %\n",
      "   Epoch: 148 | Loss=0.030325894 | TrainAcc=99.064 %| ValAcc=98.640 %\n",
      "   Epoch: 149 | Loss=0.029634164 | TrainAcc=98.924 %| ValAcc=98.400 %\n",
      "   Epoch: 150 | Loss=0.032134254 | TrainAcc=99.211 %| ValAcc=98.560 %\n",
      "   Epoch: 151 | Loss=0.029420324 | TrainAcc=99.178 %| ValAcc=98.500 %\n",
      "   Epoch: 152 | Loss=0.030842710 | TrainAcc=99.144 %| ValAcc=98.580 %\n",
      "   Epoch: 153 | Loss=0.029855028 | TrainAcc=99.200 %| ValAcc=98.720 %\n",
      "   Epoch: 154 | Loss=0.030333675 | TrainAcc=99.182 %| ValAcc=98.600 %\n",
      "   Epoch: 155 | Loss=0.028160731 | TrainAcc=99.216 %| ValAcc=98.580 %\n",
      "   Epoch: 156 | Loss=0.030038591 | TrainAcc=99.140 %| ValAcc=98.580 %\n",
      "   Epoch: 157 | Loss=0.028323901 | TrainAcc=99.222 %| ValAcc=98.600 %\n",
      "   Epoch: 158 | Loss=0.029973145 | TrainAcc=99.098 %| ValAcc=98.600 %\n",
      "   Epoch: 159 | Loss=0.028062839 | TrainAcc=99.109 %| ValAcc=98.420 %\n",
      "   Epoch: 160 | Loss=0.030644112 | TrainAcc=99.200 %| ValAcc=98.680 %\n",
      "   Epoch: 161 | Loss=0.027604050 | TrainAcc=99.235 %| ValAcc=98.540 %\n",
      "   Epoch: 162 | Loss=0.027658464 | TrainAcc=99.189 %| ValAcc=98.580 %\n",
      "   Epoch: 163 | Loss=0.028571386 | TrainAcc=99.207 %| ValAcc=98.500 %\n",
      "   Epoch: 164 | Loss=0.029378573 | TrainAcc=99.247 %| ValAcc=98.580 %\n",
      "   Epoch: 165 | Loss=0.026748579 | TrainAcc=99.267 %| ValAcc=98.640 %\n",
      "   Epoch: 166 | Loss=0.027414794 | TrainAcc=99.236 %| ValAcc=98.440 %\n",
      "   Epoch: 167 | Loss=0.027102780 | TrainAcc=99.311 %| ValAcc=98.620 %\n",
      "   Epoch: 168 | Loss=0.027056465 | TrainAcc=99.213 %| ValAcc=98.540 %\n",
      "   Epoch: 169 | Loss=0.027199734 | TrainAcc=99.065 %| ValAcc=98.380 %\n",
      "   Epoch: 170 | Loss=0.027188095 | TrainAcc=99.247 %| ValAcc=98.680 %\n",
      "   Epoch: 171 | Loss=0.026980946 | TrainAcc=99.224 %| ValAcc=98.440 %\n",
      "   Epoch: 172 | Loss=0.025870041 | TrainAcc=99.213 %| ValAcc=98.640 %\n",
      "   Epoch: 173 | Loss=0.026671702 | TrainAcc=99.245 %| ValAcc=98.600 %\n",
      "   Epoch: 174 | Loss=0.026486101 | TrainAcc=99.180 %| ValAcc=98.500 %\n",
      "   Epoch: 175 | Loss=0.026025766 | TrainAcc=99.349 %| ValAcc=98.680 %\n",
      "   Epoch: 176 | Loss=0.026300980 | TrainAcc=99.280 %| ValAcc=98.540 %\n",
      "   Epoch: 177 | Loss=0.025519698 | TrainAcc=99.267 %| ValAcc=98.540 %\n",
      "   Epoch: 178 | Loss=0.024657348 | TrainAcc=99.356 %| ValAcc=98.580 %\n",
      "   Epoch: 179 | Loss=0.025053132 | TrainAcc=99.358 %| ValAcc=98.540 %\n",
      "   Epoch: 180 | Loss=0.026010278 | TrainAcc=99.340 %| ValAcc=98.720 %\n",
      "   Epoch: 181 | Loss=0.025944371 | TrainAcc=99.282 %| ValAcc=98.560 %\n",
      "   Epoch: 182 | Loss=0.023953962 | TrainAcc=99.338 %| ValAcc=98.580 %\n",
      "   Epoch: 183 | Loss=0.025965131 | TrainAcc=99.342 %| ValAcc=98.680 %\n",
      "   Epoch: 184 | Loss=0.023646340 | TrainAcc=99.305 %| ValAcc=98.500 %\n",
      "   Epoch: 185 | Loss=0.025258801 | TrainAcc=99.362 %| ValAcc=98.700 %\n",
      "   Epoch: 186 | Loss=0.023754590 | TrainAcc=99.329 %| ValAcc=98.620 %\n",
      "   Epoch: 187 | Loss=0.025041964 | TrainAcc=99.315 %| ValAcc=98.520 %\n",
      "   Epoch: 188 | Loss=0.023527632 | TrainAcc=99.342 %| ValAcc=98.600 %\n",
      "   Epoch: 189 | Loss=0.023989654 | TrainAcc=99.407 %| ValAcc=98.700 %\n",
      "   Epoch: 190 | Loss=0.022928843 | TrainAcc=99.349 %| ValAcc=98.620 %\n",
      "   Epoch: 191 | Loss=0.024109010 | TrainAcc=99.304 %| ValAcc=98.680 %\n",
      "   Epoch: 192 | Loss=0.025049035 | TrainAcc=99.329 %| ValAcc=98.400 %\n",
      "   Epoch: 193 | Loss=0.023319230 | TrainAcc=99.429 %| ValAcc=98.640 %\n",
      "   Epoch: 194 | Loss=0.021342231 | TrainAcc=99.340 %| ValAcc=98.400 %\n",
      "   Epoch: 195 | Loss=0.024970745 | TrainAcc=99.391 %| ValAcc=98.460 %\n",
      "   Epoch: 196 | Loss=0.021880959 | TrainAcc=99.362 %| ValAcc=98.600 %\n",
      "   Epoch: 197 | Loss=0.022764517 | TrainAcc=99.400 %| ValAcc=98.660 %\n",
      "   Epoch: 198 | Loss=0.022771413 | TrainAcc=99.445 %| ValAcc=98.660 %\n",
      "   Epoch: 199 | Loss=0.022161808 | TrainAcc=99.382 %| ValAcc=98.600 %\n",
      "   Epoch: 200 | Loss=0.023144943 | TrainAcc=99.424 %| ValAcc=98.580 %\n",
      "   Epoch: 201 | Loss=0.021443361 | TrainAcc=99.442 %| ValAcc=98.600 %\n",
      "   Epoch: 202 | Loss=0.023033988 | TrainAcc=99.422 %| ValAcc=98.460 %\n",
      "   Epoch: 203 | Loss=0.021147883 | TrainAcc=99.405 %| ValAcc=98.560 %\n",
      "   Epoch: 204 | Loss=0.022433182 | TrainAcc=99.447 %| ValAcc=98.580 %\n",
      "   Epoch: 205 | Loss=0.022052104 | TrainAcc=99.384 %| ValAcc=98.540 %\n",
      "   Epoch: 206 | Loss=0.020684697 | TrainAcc=99.500 %| ValAcc=98.680 %\n",
      "   Epoch: 207 | Loss=0.022343690 | TrainAcc=99.464 %| ValAcc=98.680 %\n",
      "   Epoch: 208 | Loss=0.021391985 | TrainAcc=99.431 %| ValAcc=98.680 %\n",
      "   Epoch: 209 | Loss=0.020406419 | TrainAcc=99.367 %| ValAcc=98.460 %\n",
      "   Epoch: 210 | Loss=0.021115260 | TrainAcc=99.476 %| ValAcc=98.740 %\n",
      "   Epoch: 211 | Loss=0.021083723 | TrainAcc=99.382 %| ValAcc=98.640 %\n",
      "   Epoch: 212 | Loss=0.021272094 | TrainAcc=99.462 %| ValAcc=98.600 %\n",
      "   Epoch: 213 | Loss=0.019890089 | TrainAcc=99.495 %| ValAcc=98.720 %\n",
      "   Epoch: 214 | Loss=0.020367526 | TrainAcc=99.513 %| ValAcc=98.660 %\n",
      "   Epoch: 215 | Loss=0.021144189 | TrainAcc=99.485 %| ValAcc=98.580 %\n",
      "   Epoch: 216 | Loss=0.020042413 | TrainAcc=99.493 %| ValAcc=98.740 %\n",
      "   Epoch: 217 | Loss=0.020319316 | TrainAcc=99.480 %| ValAcc=98.540 %\n",
      "   Epoch: 218 | Loss=0.020316104 | TrainAcc=99.511 %| ValAcc=98.680 %\n",
      "   Epoch: 219 | Loss=0.018853393 | TrainAcc=99.380 %| ValAcc=98.600 %\n",
      "   Epoch: 220 | Loss=0.020540637 | TrainAcc=99.524 %| ValAcc=98.780 %\n",
      "   Epoch: 221 | Loss=0.020118519 | TrainAcc=99.478 %| ValAcc=98.680 %\n",
      "   Epoch: 222 | Loss=0.018931521 | TrainAcc=99.467 %| ValAcc=98.600 %\n",
      "   Epoch: 223 | Loss=0.019585990 | TrainAcc=99.476 %| ValAcc=98.640 %\n",
      "   Epoch: 224 | Loss=0.020097468 | TrainAcc=99.455 %| ValAcc=98.640 %\n",
      "   Epoch: 225 | Loss=0.017883253 | TrainAcc=99.482 %| ValAcc=98.660 %\n",
      "   Epoch: 226 | Loss=0.019998586 | TrainAcc=99.520 %| ValAcc=98.780 %\n",
      "   Epoch: 227 | Loss=0.019607758 | TrainAcc=99.347 %| ValAcc=98.460 %\n",
      "   Epoch: 228 | Loss=0.017962221 | TrainAcc=99.465 %| ValAcc=98.800 %\n",
      "   Epoch: 229 | Loss=0.019619857 | TrainAcc=99.540 %| ValAcc=98.640 %\n",
      "   Epoch: 230 | Loss=0.018274406 | TrainAcc=99.445 %| ValAcc=98.620 %\n",
      "   Epoch: 231 | Loss=0.019043442 | TrainAcc=99.520 %| ValAcc=98.720 %\n",
      "   Epoch: 232 | Loss=0.018268231 | TrainAcc=99.513 %| ValAcc=98.640 %\n",
      "   Epoch: 233 | Loss=0.018206333 | TrainAcc=99.553 %| ValAcc=98.700 %\n",
      "   Epoch: 234 | Loss=0.018864286 | TrainAcc=99.578 %| ValAcc=98.680 %\n",
      "   Epoch: 235 | Loss=0.018296132 | TrainAcc=99.549 %| ValAcc=98.780 %\n",
      "   Epoch: 236 | Loss=0.018326587 | TrainAcc=99.513 %| ValAcc=98.740 %\n",
      "   Epoch: 237 | Loss=0.017348832 | TrainAcc=99.527 %| ValAcc=98.700 %\n",
      "   Epoch: 238 | Loss=0.017273919 | TrainAcc=99.575 %| ValAcc=98.720 %\n",
      "   Epoch: 239 | Loss=0.018349680 | TrainAcc=99.484 %| ValAcc=98.560 %\n",
      "   Epoch: 240 | Loss=0.017956691 | TrainAcc=99.558 %| ValAcc=98.740 %\n",
      "   Epoch: 241 | Loss=0.016142235 | TrainAcc=99.567 %| ValAcc=98.680 %\n",
      "   Epoch: 242 | Loss=0.018590857 | TrainAcc=99.593 %| ValAcc=98.680 %\n",
      "   Epoch: 243 | Loss=0.016355724 | TrainAcc=99.591 %| ValAcc=98.660 %\n",
      "   Epoch: 244 | Loss=0.018008449 | TrainAcc=99.611 %| ValAcc=98.760 %\n",
      "   Epoch: 245 | Loss=0.016973843 | TrainAcc=99.533 %| ValAcc=98.740 %\n",
      "   Epoch: 246 | Loss=0.016064819 | TrainAcc=99.580 %| ValAcc=98.700 %\n",
      "   Epoch: 247 | Loss=0.016608465 | TrainAcc=99.560 %| ValAcc=98.740 %\n",
      "   Epoch: 248 | Loss=0.018074080 | TrainAcc=99.533 %| ValAcc=98.600 %\n",
      "   Epoch: 249 | Loss=0.016987511 | TrainAcc=99.555 %| ValAcc=98.600 %\n",
      "   Epoch: 250 | Loss=0.016613258 | TrainAcc=99.613 %| ValAcc=98.760 %\n",
      "   Training Finished in 8048.2 seconds.\n",
      "   Final accuracies:\n",
      "   ~ TrainAcc: 99.613 %\n",
      "   ~ ValAcc: 98.760 %\n",
      "   ~ TestAcc: 98.670 %\n",
      "   Saving model in file: Models/lenet5-model_relu_lr0001_bs50\n",
      "*Model [ lenet5-model_relu_lr0001_bs128 ] {l_r: 0.0010; n_iter: 100; batch: 128}\n",
      "   Start Training!\n",
      "   Epoch: 01 | Loss=2.293035556 | TrainAcc=22.165 %| ValAcc=20.940 %\n",
      "   Epoch: 02 | Loss=2.246666034 | TrainAcc=33.467 %| ValAcc=32.760 %\n",
      "   Epoch: 03 | Loss=2.186070200 | TrainAcc=39.667 %| ValAcc=39.200 %\n",
      "   Epoch: 04 | Loss=2.063561223 | TrainAcc=49.151 %| ValAcc=49.440 %\n",
      "   Epoch: 05 | Loss=1.768198797 | TrainAcc=66.075 %| ValAcc=66.360 %\n",
      "   Epoch: 06 | Loss=1.249351058 | TrainAcc=75.558 %| ValAcc=75.660 %\n",
      "   Epoch: 07 | Loss=0.835257401 | TrainAcc=80.755 %| ValAcc=81.040 %\n",
      "   Epoch: 08 | Loss=0.636793443 | TrainAcc=83.851 %| ValAcc=84.720 %\n",
      "   Epoch: 09 | Loss=0.538231127 | TrainAcc=85.818 %| ValAcc=86.540 %\n",
      "   Epoch: 10 | Loss=0.471492427 | TrainAcc=87.236 %| ValAcc=88.120 %\n",
      "   Epoch: 11 | Loss=0.426160102 | TrainAcc=88.271 %| ValAcc=89.260 %\n",
      "   Epoch: 12 | Loss=0.394364733 | TrainAcc=89.120 %| ValAcc=90.080 %\n",
      "   Epoch: 13 | Loss=0.359810770 | TrainAcc=89.545 %| ValAcc=90.800 %\n",
      "   Epoch: 14 | Loss=0.346231685 | TrainAcc=90.260 %| ValAcc=91.380 %\n",
      "   Epoch: 15 | Loss=0.326311756 | TrainAcc=90.700 %| ValAcc=91.700 %\n",
      "   Epoch: 16 | Loss=0.308703201 | TrainAcc=90.993 %| ValAcc=91.920 %\n",
      "   Epoch: 17 | Loss=0.294904229 | TrainAcc=91.429 %| ValAcc=92.340 %\n",
      "   Epoch: 18 | Loss=0.280363275 | TrainAcc=91.718 %| ValAcc=92.460 %\n",
      "   Epoch: 19 | Loss=0.274616572 | TrainAcc=92.102 %| ValAcc=92.640 %\n",
      "   Epoch: 20 | Loss=0.261235427 | TrainAcc=92.400 %| ValAcc=92.780 %\n",
      "   Epoch: 21 | Loss=0.250474909 | TrainAcc=92.624 %| ValAcc=92.940 %\n",
      "   Epoch: 22 | Loss=0.244036376 | TrainAcc=92.858 %| ValAcc=93.200 %\n",
      "   Epoch: 23 | Loss=0.236118098 | TrainAcc=93.002 %| ValAcc=93.500 %\n",
      "   Epoch: 24 | Loss=0.230162490 | TrainAcc=93.293 %| ValAcc=93.440 %\n",
      "   Epoch: 25 | Loss=0.220249829 | TrainAcc=93.547 %| ValAcc=94.100 %\n",
      "   Epoch: 26 | Loss=0.214611463 | TrainAcc=93.645 %| ValAcc=94.060 %\n",
      "   Epoch: 27 | Loss=0.209055034 | TrainAcc=93.971 %| ValAcc=94.100 %\n",
      "   Epoch: 28 | Loss=0.206053230 | TrainAcc=94.131 %| ValAcc=94.520 %\n",
      "   Epoch: 29 | Loss=0.196894795 | TrainAcc=94.269 %| ValAcc=94.820 %\n",
      "   Epoch: 30 | Loss=0.192198141 | TrainAcc=94.369 %| ValAcc=94.860 %\n",
      "   Epoch: 31 | Loss=0.189277246 | TrainAcc=94.558 %| ValAcc=94.960 %\n",
      "   Epoch: 32 | Loss=0.184327742 | TrainAcc=94.756 %| ValAcc=95.180 %\n",
      "   Epoch: 33 | Loss=0.180260290 | TrainAcc=94.795 %| ValAcc=95.200 %\n",
      "   Epoch: 34 | Loss=0.173474034 | TrainAcc=94.889 %| ValAcc=95.180 %\n",
      "   Epoch: 35 | Loss=0.174056324 | TrainAcc=95.004 %| ValAcc=95.320 %\n",
      "   Epoch: 36 | Loss=0.165283750 | TrainAcc=95.075 %| ValAcc=95.380 %\n",
      "   Epoch: 37 | Loss=0.166025882 | TrainAcc=95.175 %| ValAcc=95.720 %\n",
      "   Epoch: 38 | Loss=0.159386282 | TrainAcc=95.285 %| ValAcc=95.620 %\n",
      "   Epoch: 39 | Loss=0.158759687 | TrainAcc=95.453 %| ValAcc=95.820 %\n",
      "   Epoch: 40 | Loss=0.157157082 | TrainAcc=95.471 %| ValAcc=95.800 %\n",
      "   Epoch: 41 | Loss=0.148359736 | TrainAcc=95.591 %| ValAcc=95.940 %\n",
      "   Epoch: 42 | Loss=0.149188039 | TrainAcc=95.664 %| ValAcc=95.940 %\n",
      "   Epoch: 43 | Loss=0.146598287 | TrainAcc=95.765 %| ValAcc=95.960 %\n",
      "   Epoch: 44 | Loss=0.145838529 | TrainAcc=95.838 %| ValAcc=96.060 %\n",
      "   Epoch: 45 | Loss=0.139487645 | TrainAcc=95.871 %| ValAcc=96.140 %\n",
      "   Epoch: 46 | Loss=0.140147173 | TrainAcc=95.933 %| ValAcc=96.180 %\n",
      "   Epoch: 47 | Loss=0.134008562 | TrainAcc=96.053 %| ValAcc=96.240 %\n",
      "   Epoch: 48 | Loss=0.136843647 | TrainAcc=96.058 %| ValAcc=96.420 %\n",
      "   Epoch: 49 | Loss=0.129832151 | TrainAcc=96.145 %| ValAcc=96.320 %\n",
      "   Epoch: 50 | Loss=0.129506139 | TrainAcc=96.235 %| ValAcc=96.440 %\n",
      "   Epoch: 51 | Loss=0.129012275 | TrainAcc=96.322 %| ValAcc=96.460 %\n",
      "   Epoch: 52 | Loss=0.125390068 | TrainAcc=96.322 %| ValAcc=96.420 %\n",
      "   Epoch: 53 | Loss=0.124851513 | TrainAcc=96.373 %| ValAcc=96.580 %\n",
      "   Epoch: 54 | Loss=0.122864547 | TrainAcc=96.396 %| ValAcc=96.540 %\n",
      "   Epoch: 55 | Loss=0.120778402 | TrainAcc=96.480 %| ValAcc=96.580 %\n",
      "   Epoch: 56 | Loss=0.117437502 | TrainAcc=96.536 %| ValAcc=96.600 %\n",
      "   Epoch: 57 | Loss=0.118215633 | TrainAcc=96.611 %| ValAcc=96.700 %\n",
      "   Epoch: 58 | Loss=0.115738848 | TrainAcc=96.598 %| ValAcc=96.840 %\n",
      "   Epoch: 59 | Loss=0.118387058 | TrainAcc=96.675 %| ValAcc=96.720 %\n",
      "   Epoch: 60 | Loss=0.110518392 | TrainAcc=96.685 %| ValAcc=96.940 %\n",
      "   Epoch: 61 | Loss=0.113898398 | TrainAcc=96.758 %| ValAcc=96.820 %\n",
      "   Epoch: 62 | Loss=0.109593702 | TrainAcc=96.815 %| ValAcc=96.940 %\n",
      "   Epoch: 63 | Loss=0.109178530 | TrainAcc=96.889 %| ValAcc=96.980 %\n",
      "   Epoch: 64 | Loss=0.107544898 | TrainAcc=96.825 %| ValAcc=96.800 %\n",
      "   Epoch: 65 | Loss=0.109088464 | TrainAcc=96.878 %| ValAcc=96.980 %\n",
      "   Epoch: 66 | Loss=0.104567428 | TrainAcc=96.978 %| ValAcc=97.040 %\n",
      "   Epoch: 67 | Loss=0.104195407 | TrainAcc=96.945 %| ValAcc=97.040 %\n",
      "   Epoch: 68 | Loss=0.103226814 | TrainAcc=96.989 %| ValAcc=97.320 %\n",
      "   Epoch: 69 | Loss=0.103489420 | TrainAcc=97.002 %| ValAcc=97.060 %\n",
      "   Epoch: 70 | Loss=0.098616219 | TrainAcc=97.045 %| ValAcc=97.160 %\n",
      "   Epoch: 71 | Loss=0.101540734 | TrainAcc=97.080 %| ValAcc=97.300 %\n",
      "   Epoch: 72 | Loss=0.096985124 | TrainAcc=97.125 %| ValAcc=97.160 %\n",
      "   Epoch: 73 | Loss=0.101444842 | TrainAcc=97.135 %| ValAcc=97.140 %\n",
      "   Epoch: 74 | Loss=0.094289097 | TrainAcc=97.135 %| ValAcc=97.240 %\n",
      "   Epoch: 75 | Loss=0.097983516 | TrainAcc=97.196 %| ValAcc=97.240 %\n",
      "   Epoch: 76 | Loss=0.094324127 | TrainAcc=97.187 %| ValAcc=97.320 %\n",
      "   Epoch: 77 | Loss=0.095838315 | TrainAcc=97.256 %| ValAcc=97.340 %\n",
      "   Epoch: 78 | Loss=0.092501018 | TrainAcc=97.211 %| ValAcc=97.320 %\n",
      "   Epoch: 79 | Loss=0.094569027 | TrainAcc=97.275 %| ValAcc=97.280 %\n",
      "   Epoch: 80 | Loss=0.093161546 | TrainAcc=97.309 %| ValAcc=97.440 %\n",
      "   Epoch: 81 | Loss=0.090951208 | TrainAcc=97.325 %| ValAcc=97.360 %\n",
      "   Epoch: 82 | Loss=0.089560234 | TrainAcc=97.280 %| ValAcc=97.220 %\n",
      "   Epoch: 83 | Loss=0.088428938 | TrainAcc=97.349 %| ValAcc=97.440 %\n",
      "   Epoch: 84 | Loss=0.089073880 | TrainAcc=97.431 %| ValAcc=97.360 %\n",
      "   Epoch: 85 | Loss=0.090287475 | TrainAcc=97.376 %| ValAcc=97.520 %\n",
      "   Epoch: 86 | Loss=0.085439353 | TrainAcc=97.427 %| ValAcc=97.480 %\n",
      "   Epoch: 87 | Loss=0.087864276 | TrainAcc=97.418 %| ValAcc=97.500 %\n",
      "   Epoch: 88 | Loss=0.086936165 | TrainAcc=97.536 %| ValAcc=97.660 %\n",
      "   Epoch: 89 | Loss=0.084366638 | TrainAcc=97.476 %| ValAcc=97.580 %\n",
      "   Epoch: 90 | Loss=0.085125366 | TrainAcc=97.465 %| ValAcc=97.520 %\n",
      "   Epoch: 91 | Loss=0.084454491 | TrainAcc=97.562 %| ValAcc=97.600 %\n",
      "   Epoch: 92 | Loss=0.083359959 | TrainAcc=97.613 %| ValAcc=97.680 %\n",
      "   Epoch: 93 | Loss=0.084190196 | TrainAcc=97.569 %| ValAcc=97.720 %\n",
      "   Epoch: 94 | Loss=0.080908084 | TrainAcc=97.616 %| ValAcc=97.720 %\n",
      "   Epoch: 95 | Loss=0.080281464 | TrainAcc=97.613 %| ValAcc=97.720 %\n",
      "   Epoch: 96 | Loss=0.084289883 | TrainAcc=97.655 %| ValAcc=97.600 %\n",
      "   Epoch: 97 | Loss=0.080252835 | TrainAcc=97.591 %| ValAcc=97.620 %\n",
      "   Epoch: 98 | Loss=0.080035667 | TrainAcc=97.704 %| ValAcc=97.680 %\n",
      "   Epoch: 99 | Loss=0.078600037 | TrainAcc=97.704 %| ValAcc=97.640 %\n",
      "   Epoch: 100 | Loss=0.079353372 | TrainAcc=97.702 %| ValAcc=97.700 %\n",
      "   Training Finished in 2677.6 seconds.\n",
      "   Final accuracies:\n",
      "   ~ TrainAcc: 97.702 %\n",
      "   ~ ValAcc: 97.700 %\n",
      "   ~ TestAcc: 97.840 %\n",
      "   Saving model in file: Models/lenet5-model_relu_lr0001_bs128\n",
      "*Model [ lenet5-model_relu_lr00001_bs50 ] {l_r: 0.0001; n_iter: 250; batch: 50}\n",
      "   Start Training!\n",
      "   Epoch: 01 | Loss=2.314985525 | TrainAcc=13.107 %| ValAcc=12.480 %\n",
      "   Epoch: 02 | Loss=2.303570949 | TrainAcc=13.627 %| ValAcc=13.160 %\n",
      "   Epoch: 03 | Loss=2.292266892 | TrainAcc=14.485 %| ValAcc=13.900 %\n",
      "   Epoch: 04 | Loss=2.285166473 | TrainAcc=14.902 %| ValAcc=14.720 %\n",
      "   Epoch: 05 | Loss=2.277670279 | TrainAcc=16.085 %| ValAcc=15.640 %\n",
      "   Epoch: 06 | Loss=2.270431685 | TrainAcc=18.344 %| ValAcc=18.180 %\n",
      "   Epoch: 07 | Loss=2.264070818 | TrainAcc=22.576 %| ValAcc=23.360 %\n",
      "   Epoch: 08 | Loss=2.257355559 | TrainAcc=25.198 %| ValAcc=25.940 %\n",
      "   Epoch: 09 | Loss=2.248916448 | TrainAcc=26.504 %| ValAcc=27.640 %\n",
      "   Epoch: 10 | Loss=2.240554862 | TrainAcc=27.687 %| ValAcc=28.720 %\n",
      "   Epoch: 11 | Loss=2.231804672 | TrainAcc=30.158 %| ValAcc=31.280 %\n",
      "   Epoch: 12 | Loss=2.221249664 | TrainAcc=33.098 %| ValAcc=34.840 %\n",
      "   Epoch: 13 | Loss=2.208500127 | TrainAcc=35.562 %| ValAcc=37.460 %\n",
      "   Epoch: 14 | Loss=2.195690938 | TrainAcc=38.247 %| ValAcc=39.840 %\n",
      "   Epoch: 15 | Loss=2.180523416 | TrainAcc=40.696 %| ValAcc=41.840 %\n",
      "   Epoch: 16 | Loss=2.160457633 | TrainAcc=42.620 %| ValAcc=43.420 %\n",
      "   Epoch: 17 | Loss=2.140027166 | TrainAcc=44.605 %| ValAcc=45.340 %\n",
      "   Epoch: 18 | Loss=2.114254781 | TrainAcc=45.973 %| ValAcc=47.000 %\n",
      "   Epoch: 19 | Loss=2.084103340 | TrainAcc=47.140 %| ValAcc=48.080 %\n",
      "   Epoch: 20 | Loss=2.044793764 | TrainAcc=48.322 %| ValAcc=49.400 %\n",
      "   Epoch: 21 | Loss=2.000263449 | TrainAcc=49.625 %| ValAcc=50.620 %\n",
      "   Epoch: 22 | Loss=1.942387450 | TrainAcc=50.569 %| ValAcc=51.180 %\n",
      "   Epoch: 23 | Loss=1.878377276 | TrainAcc=52.702 %| ValAcc=53.340 %\n",
      "   Epoch: 24 | Loss=1.798259903 | TrainAcc=55.024 %| ValAcc=55.580 %\n",
      "   Epoch: 25 | Loss=1.703164416 | TrainAcc=57.811 %| ValAcc=58.360 %\n",
      "   Epoch: 26 | Loss=1.599884366 | TrainAcc=60.725 %| ValAcc=61.460 %\n",
      "   Epoch: 27 | Loss=1.484770735 | TrainAcc=64.509 %| ValAcc=65.320 %\n",
      "   Epoch: 28 | Loss=1.361039547 | TrainAcc=68.000 %| ValAcc=68.680 %\n",
      "   Epoch: 29 | Loss=1.238242439 | TrainAcc=70.680 %| ValAcc=71.940 %\n",
      "   Epoch: 30 | Loss=1.121098438 | TrainAcc=73.080 %| ValAcc=74.700 %\n",
      "   Epoch: 31 | Loss=1.012906806 | TrainAcc=75.709 %| ValAcc=77.280 %\n",
      "   Epoch: 32 | Loss=0.910597260 | TrainAcc=77.449 %| ValAcc=79.000 %\n",
      "   Epoch: 33 | Loss=0.829403715 | TrainAcc=78.985 %| ValAcc=80.560 %\n",
      "   Epoch: 34 | Loss=0.765381871 | TrainAcc=80.380 %| ValAcc=81.680 %\n",
      "   Epoch: 35 | Loss=0.701467588 | TrainAcc=81.458 %| ValAcc=82.740 %\n",
      "   Epoch: 36 | Loss=0.658902448 | TrainAcc=82.411 %| ValAcc=83.740 %\n",
      "   Epoch: 37 | Loss=0.614492829 | TrainAcc=83.358 %| ValAcc=84.520 %\n",
      "   Epoch: 38 | Loss=0.582584663 | TrainAcc=83.955 %| ValAcc=85.340 %\n",
      "   Epoch: 39 | Loss=0.554420969 | TrainAcc=84.642 %| ValAcc=86.080 %\n",
      "   Epoch: 40 | Loss=0.528286731 | TrainAcc=85.160 %| ValAcc=86.700 %\n",
      "   Epoch: 41 | Loss=0.508263771 | TrainAcc=85.685 %| ValAcc=87.320 %\n",
      "   Epoch: 42 | Loss=0.486846848 | TrainAcc=86.045 %| ValAcc=87.680 %\n",
      "   Epoch: 43 | Loss=0.476681690 | TrainAcc=86.593 %| ValAcc=88.260 %\n",
      "   Epoch: 44 | Loss=0.452750813 | TrainAcc=86.904 %| ValAcc=88.520 %\n",
      "   Epoch: 45 | Loss=0.443897470 | TrainAcc=87.298 %| ValAcc=88.620 %\n",
      "   Epoch: 46 | Loss=0.431029184 | TrainAcc=87.633 %| ValAcc=89.200 %\n",
      "   Epoch: 47 | Loss=0.418487607 | TrainAcc=87.933 %| ValAcc=89.380 %\n",
      "   Epoch: 48 | Loss=0.410564729 | TrainAcc=88.151 %| ValAcc=89.520 %\n",
      "   Epoch: 49 | Loss=0.400064718 | TrainAcc=88.304 %| ValAcc=89.940 %\n",
      "   Epoch: 50 | Loss=0.392947442 | TrainAcc=88.645 %| ValAcc=90.020 %\n",
      "   Epoch: 51 | Loss=0.384895758 | TrainAcc=88.780 %| ValAcc=90.120 %\n",
      "   Epoch: 52 | Loss=0.375271516 | TrainAcc=88.982 %| ValAcc=90.120 %\n",
      "   Epoch: 53 | Loss=0.369290199 | TrainAcc=89.196 %| ValAcc=90.360 %\n",
      "   Epoch: 54 | Loss=0.364940785 | TrainAcc=89.367 %| ValAcc=90.580 %\n",
      "   Epoch: 55 | Loss=0.360273867 | TrainAcc=89.542 %| ValAcc=90.660 %\n",
      "   Epoch: 56 | Loss=0.352652895 | TrainAcc=89.627 %| ValAcc=90.800 %\n",
      "   Epoch: 57 | Loss=0.342446766 | TrainAcc=89.736 %| ValAcc=90.880 %\n",
      "   Epoch: 58 | Loss=0.344311007 | TrainAcc=89.860 %| ValAcc=90.940 %\n",
      "   Epoch: 59 | Loss=0.336488790 | TrainAcc=90.051 %| ValAcc=91.060 %\n",
      "   Epoch: 60 | Loss=0.334706277 | TrainAcc=90.204 %| ValAcc=91.180 %\n",
      "   Epoch: 61 | Loss=0.327295696 | TrainAcc=90.285 %| ValAcc=91.280 %\n",
      "   Epoch: 62 | Loss=0.326029734 | TrainAcc=90.393 %| ValAcc=91.300 %\n",
      "   Epoch: 63 | Loss=0.317943900 | TrainAcc=90.516 %| ValAcc=91.400 %\n",
      "   Epoch: 64 | Loss=0.317915915 | TrainAcc=90.653 %| ValAcc=91.460 %\n",
      "   Epoch: 65 | Loss=0.312686075 | TrainAcc=90.765 %| ValAcc=91.440 %\n",
      "   Epoch: 66 | Loss=0.307838891 | TrainAcc=90.842 %| ValAcc=91.620 %\n",
      "   Epoch: 67 | Loss=0.306942262 | TrainAcc=90.949 %| ValAcc=91.620 %\n",
      "   Epoch: 68 | Loss=0.299314241 | TrainAcc=91.071 %| ValAcc=91.720 %\n",
      "   Epoch: 69 | Loss=0.302386993 | TrainAcc=91.127 %| ValAcc=91.740 %\n",
      "   Epoch: 70 | Loss=0.294664326 | TrainAcc=91.256 %| ValAcc=91.960 %\n",
      "   Epoch: 71 | Loss=0.292771967 | TrainAcc=91.335 %| ValAcc=92.060 %\n",
      "   Epoch: 72 | Loss=0.292249043 | TrainAcc=91.435 %| ValAcc=92.060 %\n",
      "   Epoch: 73 | Loss=0.283415456 | TrainAcc=91.513 %| ValAcc=92.160 %\n",
      "   Epoch: 74 | Loss=0.284900427 | TrainAcc=91.591 %| ValAcc=92.280 %\n",
      "   Epoch: 75 | Loss=0.283456304 | TrainAcc=91.682 %| ValAcc=92.260 %\n",
      "   Epoch: 76 | Loss=0.275268862 | TrainAcc=91.811 %| ValAcc=92.400 %\n",
      "   Epoch: 77 | Loss=0.279007617 | TrainAcc=91.831 %| ValAcc=92.500 %\n",
      "   Epoch: 78 | Loss=0.274047177 | TrainAcc=91.924 %| ValAcc=92.500 %\n",
      "   Epoch: 79 | Loss=0.269300972 | TrainAcc=91.987 %| ValAcc=92.460 %\n",
      "   Epoch: 80 | Loss=0.269824231 | TrainAcc=92.009 %| ValAcc=92.640 %\n",
      "   Epoch: 81 | Loss=0.265518033 | TrainAcc=92.118 %| ValAcc=92.640 %\n",
      "   Epoch: 82 | Loss=0.266615008 | TrainAcc=92.207 %| ValAcc=92.700 %\n",
      "   Epoch: 83 | Loss=0.260461711 | TrainAcc=92.280 %| ValAcc=92.740 %\n",
      "   Epoch: 84 | Loss=0.260167850 | TrainAcc=92.344 %| ValAcc=92.780 %\n",
      "   Epoch: 85 | Loss=0.255841624 | TrainAcc=92.400 %| ValAcc=92.880 %\n",
      "   Epoch: 86 | Loss=0.257598835 | TrainAcc=92.409 %| ValAcc=92.880 %\n",
      "   Epoch: 87 | Loss=0.250511190 | TrainAcc=92.556 %| ValAcc=92.880 %\n",
      "   Epoch: 88 | Loss=0.253187975 | TrainAcc=92.624 %| ValAcc=92.900 %\n",
      "   Epoch: 89 | Loss=0.247921524 | TrainAcc=92.635 %| ValAcc=93.100 %\n",
      "   Epoch: 90 | Loss=0.246428853 | TrainAcc=92.696 %| ValAcc=93.080 %\n",
      "   Epoch: 91 | Loss=0.245834942 | TrainAcc=92.778 %| ValAcc=93.060 %\n",
      "   Epoch: 92 | Loss=0.241439298 | TrainAcc=92.805 %| ValAcc=93.200 %\n",
      "   Epoch: 93 | Loss=0.242251254 | TrainAcc=92.895 %| ValAcc=93.340 %\n",
      "   Epoch: 94 | Loss=0.240581569 | TrainAcc=92.962 %| ValAcc=93.380 %\n",
      "   Epoch: 95 | Loss=0.241351764 | TrainAcc=92.964 %| ValAcc=93.360 %\n",
      "   Epoch: 96 | Loss=0.233245014 | TrainAcc=93.042 %| ValAcc=93.400 %\n",
      "   Epoch: 97 | Loss=0.232896858 | TrainAcc=93.087 %| ValAcc=93.540 %\n",
      "   Epoch: 98 | Loss=0.232641541 | TrainAcc=93.162 %| ValAcc=93.580 %\n",
      "   Epoch: 99 | Loss=0.232187252 | TrainAcc=93.218 %| ValAcc=93.700 %\n",
      "   Epoch: 100 | Loss=0.231109739 | TrainAcc=93.249 %| ValAcc=93.620 %\n",
      "   Epoch: 101 | Loss=0.224595334 | TrainAcc=93.311 %| ValAcc=93.660 %\n",
      "   Epoch: 102 | Loss=0.227438581 | TrainAcc=93.340 %| ValAcc=93.760 %\n",
      "   Epoch: 103 | Loss=0.225614956 | TrainAcc=93.369 %| ValAcc=93.740 %\n",
      "   Epoch: 104 | Loss=0.223620215 | TrainAcc=93.425 %| ValAcc=93.820 %\n",
      "   Epoch: 105 | Loss=0.223429890 | TrainAcc=93.498 %| ValAcc=93.900 %\n",
      "   Epoch: 106 | Loss=0.213946659 | TrainAcc=93.496 %| ValAcc=93.880 %\n",
      "   Epoch: 107 | Loss=0.220411039 | TrainAcc=93.573 %| ValAcc=93.940 %\n",
      "   Epoch: 108 | Loss=0.218172389 | TrainAcc=93.624 %| ValAcc=94.020 %\n",
      "   Epoch: 109 | Loss=0.217589823 | TrainAcc=93.676 %| ValAcc=94.120 %\n",
      "   Epoch: 110 | Loss=0.212281138 | TrainAcc=93.691 %| ValAcc=94.100 %\n",
      "   Epoch: 111 | Loss=0.213533433 | TrainAcc=93.745 %| ValAcc=94.200 %\n",
      "   Epoch: 112 | Loss=0.211532881 | TrainAcc=93.762 %| ValAcc=94.140 %\n",
      "   Epoch: 113 | Loss=0.207521766 | TrainAcc=93.811 %| ValAcc=94.100 %\n",
      "   Epoch: 114 | Loss=0.210219757 | TrainAcc=93.889 %| ValAcc=94.160 %\n",
      "   Epoch: 115 | Loss=0.207966589 | TrainAcc=93.853 %| ValAcc=94.360 %\n",
      "   Epoch: 116 | Loss=0.206663887 | TrainAcc=93.933 %| ValAcc=94.340 %\n",
      "   Epoch: 117 | Loss=0.203593924 | TrainAcc=93.927 %| ValAcc=94.260 %\n",
      "   Epoch: 118 | Loss=0.204824897 | TrainAcc=94.020 %| ValAcc=94.420 %\n",
      "   Epoch: 119 | Loss=0.201984227 | TrainAcc=94.035 %| ValAcc=94.320 %\n",
      "   Epoch: 120 | Loss=0.203079267 | TrainAcc=94.065 %| ValAcc=94.440 %\n",
      "   Epoch: 121 | Loss=0.198841196 | TrainAcc=94.115 %| ValAcc=94.440 %\n",
      "   Epoch: 122 | Loss=0.198597782 | TrainAcc=94.113 %| ValAcc=94.540 %\n",
      "   Epoch: 123 | Loss=0.193577027 | TrainAcc=94.187 %| ValAcc=94.540 %\n",
      "   Epoch: 124 | Loss=0.199845902 | TrainAcc=94.224 %| ValAcc=94.520 %\n",
      "   Epoch: 125 | Loss=0.194908778 | TrainAcc=94.253 %| ValAcc=94.660 %\n",
      "   Epoch: 126 | Loss=0.195411178 | TrainAcc=94.284 %| ValAcc=94.600 %\n",
      "   Epoch: 127 | Loss=0.191325981 | TrainAcc=94.309 %| ValAcc=94.740 %\n",
      "   Epoch: 128 | Loss=0.193564938 | TrainAcc=94.375 %| ValAcc=94.720 %\n",
      "   Epoch: 129 | Loss=0.190541065 | TrainAcc=94.398 %| ValAcc=94.680 %\n",
      "   Epoch: 130 | Loss=0.186756333 | TrainAcc=94.453 %| ValAcc=94.620 %\n",
      "   Epoch: 131 | Loss=0.191649404 | TrainAcc=94.451 %| ValAcc=94.760 %\n",
      "   Epoch: 132 | Loss=0.186383875 | TrainAcc=94.495 %| ValAcc=94.700 %\n",
      "   Epoch: 133 | Loss=0.184948635 | TrainAcc=94.520 %| ValAcc=94.680 %\n",
      "   Epoch: 134 | Loss=0.188199030 | TrainAcc=94.545 %| ValAcc=94.820 %\n",
      "   Epoch: 135 | Loss=0.187691669 | TrainAcc=94.595 %| ValAcc=94.880 %\n",
      "   Epoch: 136 | Loss=0.181240232 | TrainAcc=94.591 %| ValAcc=94.940 %\n",
      "   Epoch: 137 | Loss=0.185310067 | TrainAcc=94.625 %| ValAcc=95.020 %\n",
      "   Epoch: 138 | Loss=0.178128529 | TrainAcc=94.629 %| ValAcc=95.000 %\n",
      "   Epoch: 139 | Loss=0.181835335 | TrainAcc=94.662 %| ValAcc=95.000 %\n",
      "   Epoch: 140 | Loss=0.180046309 | TrainAcc=94.698 %| ValAcc=95.080 %\n",
      "   Epoch: 141 | Loss=0.178251870 | TrainAcc=94.742 %| ValAcc=95.060 %\n",
      "   Epoch: 142 | Loss=0.180386245 | TrainAcc=94.789 %| ValAcc=95.120 %\n",
      "   Epoch: 143 | Loss=0.173250497 | TrainAcc=94.775 %| ValAcc=95.160 %\n",
      "   Epoch: 144 | Loss=0.175348180 | TrainAcc=94.809 %| ValAcc=95.180 %\n",
      "   Epoch: 145 | Loss=0.178705334 | TrainAcc=94.875 %| ValAcc=95.160 %\n",
      "   Epoch: 146 | Loss=0.172555403 | TrainAcc=94.878 %| ValAcc=95.280 %\n",
      "   Epoch: 147 | Loss=0.171959419 | TrainAcc=94.909 %| ValAcc=95.260 %\n",
      "   Epoch: 148 | Loss=0.172236921 | TrainAcc=94.911 %| ValAcc=95.300 %\n",
      "   Epoch: 149 | Loss=0.172518645 | TrainAcc=94.915 %| ValAcc=95.300 %\n",
      "   Epoch: 150 | Loss=0.172821536 | TrainAcc=94.971 %| ValAcc=95.200 %\n",
      "   Epoch: 151 | Loss=0.168204195 | TrainAcc=95.011 %| ValAcc=95.320 %\n",
      "   Epoch: 152 | Loss=0.171667507 | TrainAcc=95.042 %| ValAcc=95.380 %\n",
      "   Epoch: 153 | Loss=0.166490953 | TrainAcc=95.078 %| ValAcc=95.380 %\n",
      "   Epoch: 154 | Loss=0.168979874 | TrainAcc=95.107 %| ValAcc=95.520 %\n",
      "   Epoch: 155 | Loss=0.164047803 | TrainAcc=95.098 %| ValAcc=95.380 %\n",
      "   Epoch: 156 | Loss=0.168692682 | TrainAcc=95.131 %| ValAcc=95.480 %\n",
      "   Epoch: 157 | Loss=0.163266546 | TrainAcc=95.160 %| ValAcc=95.380 %\n",
      "   Epoch: 158 | Loss=0.168262320 | TrainAcc=95.182 %| ValAcc=95.520 %\n",
      "   Epoch: 159 | Loss=0.160446556 | TrainAcc=95.216 %| ValAcc=95.480 %\n",
      "   Epoch: 160 | Loss=0.162422763 | TrainAcc=95.247 %| ValAcc=95.500 %\n",
      "   Epoch: 161 | Loss=0.163621631 | TrainAcc=95.253 %| ValAcc=95.440 %\n",
      "   Epoch: 162 | Loss=0.160800652 | TrainAcc=95.269 %| ValAcc=95.580 %\n",
      "   Epoch: 163 | Loss=0.161992428 | TrainAcc=95.307 %| ValAcc=95.520 %\n",
      "   Epoch: 164 | Loss=0.159300671 | TrainAcc=95.338 %| ValAcc=95.640 %\n",
      "   Epoch: 165 | Loss=0.160127863 | TrainAcc=95.302 %| ValAcc=95.720 %\n",
      "   Epoch: 166 | Loss=0.156826847 | TrainAcc=95.373 %| ValAcc=95.620 %\n",
      "   Epoch: 167 | Loss=0.158739888 | TrainAcc=95.409 %| ValAcc=95.680 %\n",
      "   Epoch: 168 | Loss=0.156653134 | TrainAcc=95.382 %| ValAcc=95.780 %\n",
      "   Epoch: 169 | Loss=0.155294492 | TrainAcc=95.447 %| ValAcc=95.700 %\n",
      "   Epoch: 170 | Loss=0.155948808 | TrainAcc=95.478 %| ValAcc=95.840 %\n",
      "   Epoch: 171 | Loss=0.156179781 | TrainAcc=95.464 %| ValAcc=95.740 %\n",
      "   Epoch: 172 | Loss=0.151935020 | TrainAcc=95.496 %| ValAcc=95.740 %\n",
      "   Epoch: 173 | Loss=0.160131592 | TrainAcc=95.515 %| ValAcc=95.840 %\n",
      "   Epoch: 174 | Loss=0.149119602 | TrainAcc=95.487 %| ValAcc=95.820 %\n",
      "   Epoch: 175 | Loss=0.150963416 | TrainAcc=95.564 %| ValAcc=95.740 %\n",
      "   Epoch: 176 | Loss=0.153489168 | TrainAcc=95.556 %| ValAcc=95.880 %\n",
      "   Epoch: 177 | Loss=0.150545897 | TrainAcc=95.591 %| ValAcc=95.800 %\n",
      "   Epoch: 178 | Loss=0.148704058 | TrainAcc=95.624 %| ValAcc=95.980 %\n",
      "   Epoch: 179 | Loss=0.151458196 | TrainAcc=95.582 %| ValAcc=95.880 %\n",
      "   Epoch: 180 | Loss=0.149281954 | TrainAcc=95.651 %| ValAcc=95.920 %\n",
      "   Epoch: 181 | Loss=0.148003377 | TrainAcc=95.676 %| ValAcc=95.920 %\n",
      "   Epoch: 182 | Loss=0.147801542 | TrainAcc=95.691 %| ValAcc=96.080 %\n",
      "   Epoch: 183 | Loss=0.148273561 | TrainAcc=95.756 %| ValAcc=95.880 %\n",
      "   Epoch: 184 | Loss=0.146991543 | TrainAcc=95.744 %| ValAcc=96.040 %\n",
      "   Epoch: 185 | Loss=0.145601059 | TrainAcc=95.764 %| ValAcc=96.060 %\n",
      "   Epoch: 186 | Loss=0.142590876 | TrainAcc=95.785 %| ValAcc=96.120 %\n",
      "   Epoch: 187 | Loss=0.148276989 | TrainAcc=95.762 %| ValAcc=96.140 %\n",
      "   Epoch: 188 | Loss=0.144793094 | TrainAcc=95.809 %| ValAcc=96.000 %\n",
      "   Epoch: 189 | Loss=0.142560451 | TrainAcc=95.825 %| ValAcc=96.060 %\n",
      "   Epoch: 190 | Loss=0.142002000 | TrainAcc=95.864 %| ValAcc=96.100 %\n",
      "   Epoch: 191 | Loss=0.143785883 | TrainAcc=95.798 %| ValAcc=96.220 %\n",
      "   Epoch: 192 | Loss=0.143809186 | TrainAcc=95.882 %| ValAcc=96.220 %\n",
      "   Epoch: 193 | Loss=0.141037597 | TrainAcc=95.920 %| ValAcc=96.240 %\n",
      "   Epoch: 194 | Loss=0.141473102 | TrainAcc=95.907 %| ValAcc=96.160 %\n",
      "   Epoch: 195 | Loss=0.139092010 | TrainAcc=95.902 %| ValAcc=96.200 %\n",
      "   Epoch: 196 | Loss=0.137800354 | TrainAcc=95.960 %| ValAcc=96.200 %\n",
      "   Epoch: 197 | Loss=0.141285376 | TrainAcc=95.962 %| ValAcc=96.160 %\n",
      "   Epoch: 198 | Loss=0.140247021 | TrainAcc=95.978 %| ValAcc=96.220 %\n",
      "   Epoch: 199 | Loss=0.136024137 | TrainAcc=95.989 %| ValAcc=96.280 %\n",
      "   Epoch: 200 | Loss=0.136971220 | TrainAcc=96.018 %| ValAcc=96.200 %\n",
      "   Epoch: 201 | Loss=0.138395581 | TrainAcc=96.053 %| ValAcc=96.220 %\n",
      "   Epoch: 202 | Loss=0.134938532 | TrainAcc=96.038 %| ValAcc=96.280 %\n",
      "   Epoch: 203 | Loss=0.137524319 | TrainAcc=96.053 %| ValAcc=96.400 %\n",
      "   Epoch: 204 | Loss=0.136034950 | TrainAcc=96.087 %| ValAcc=96.300 %\n",
      "   Epoch: 205 | Loss=0.135439428 | TrainAcc=96.082 %| ValAcc=96.300 %\n",
      "   Epoch: 206 | Loss=0.134429487 | TrainAcc=96.087 %| ValAcc=96.320 %\n",
      "   Epoch: 207 | Loss=0.133750065 | TrainAcc=96.105 %| ValAcc=96.360 %\n",
      "   Epoch: 208 | Loss=0.134040213 | TrainAcc=96.129 %| ValAcc=96.300 %\n",
      "   Epoch: 209 | Loss=0.134654282 | TrainAcc=96.124 %| ValAcc=96.460 %\n",
      "   Epoch: 210 | Loss=0.132733592 | TrainAcc=96.145 %| ValAcc=96.380 %\n",
      "   Epoch: 211 | Loss=0.131765544 | TrainAcc=96.162 %| ValAcc=96.360 %\n",
      "   Epoch: 212 | Loss=0.132059089 | TrainAcc=96.207 %| ValAcc=96.360 %\n",
      "   Epoch: 213 | Loss=0.132965419 | TrainAcc=96.191 %| ValAcc=96.440 %\n",
      "   Epoch: 214 | Loss=0.129213718 | TrainAcc=96.231 %| ValAcc=96.340 %\n",
      "   Epoch: 215 | Loss=0.129014924 | TrainAcc=96.225 %| ValAcc=96.500 %\n",
      "   Epoch: 216 | Loss=0.129576416 | TrainAcc=96.216 %| ValAcc=96.420 %\n",
      "   Epoch: 217 | Loss=0.130480536 | TrainAcc=96.224 %| ValAcc=96.640 %\n",
      "   Epoch: 218 | Loss=0.129588438 | TrainAcc=96.255 %| ValAcc=96.460 %\n",
      "   Epoch: 219 | Loss=0.127203240 | TrainAcc=96.296 %| ValAcc=96.520 %\n",
      "   Epoch: 220 | Loss=0.128085777 | TrainAcc=96.322 %| ValAcc=96.480 %\n",
      "   Epoch: 221 | Loss=0.128372911 | TrainAcc=96.291 %| ValAcc=96.460 %\n",
      "   Epoch: 222 | Loss=0.126331320 | TrainAcc=96.329 %| ValAcc=96.620 %\n",
      "   Epoch: 223 | Loss=0.127302805 | TrainAcc=96.342 %| ValAcc=96.600 %\n",
      "   Epoch: 224 | Loss=0.127195578 | TrainAcc=96.376 %| ValAcc=96.500 %\n",
      "   Epoch: 225 | Loss=0.126046078 | TrainAcc=96.345 %| ValAcc=96.580 %\n",
      "   Epoch: 226 | Loss=0.125183495 | TrainAcc=96.360 %| ValAcc=96.460 %\n",
      "   Epoch: 227 | Loss=0.124820395 | TrainAcc=96.407 %| ValAcc=96.560 %\n",
      "   Epoch: 228 | Loss=0.124684962 | TrainAcc=96.387 %| ValAcc=96.660 %\n",
      "   Epoch: 229 | Loss=0.123408470 | TrainAcc=96.380 %| ValAcc=96.560 %\n",
      "   Epoch: 230 | Loss=0.125455444 | TrainAcc=96.382 %| ValAcc=96.600 %\n",
      "   Epoch: 231 | Loss=0.123210281 | TrainAcc=96.418 %| ValAcc=96.620 %\n",
      "   Epoch: 232 | Loss=0.124717135 | TrainAcc=96.456 %| ValAcc=96.680 %\n",
      "   Epoch: 233 | Loss=0.119213459 | TrainAcc=96.458 %| ValAcc=96.680 %\n",
      "   Epoch: 234 | Loss=0.126141816 | TrainAcc=96.462 %| ValAcc=96.600 %\n",
      "   Epoch: 235 | Loss=0.119753075 | TrainAcc=96.480 %| ValAcc=96.680 %\n",
      "   Epoch: 236 | Loss=0.121573644 | TrainAcc=96.507 %| ValAcc=96.640 %\n",
      "   Epoch: 237 | Loss=0.121737504 | TrainAcc=96.485 %| ValAcc=96.680 %\n",
      "   Epoch: 238 | Loss=0.122521917 | TrainAcc=96.496 %| ValAcc=96.660 %\n",
      "   Epoch: 239 | Loss=0.119084093 | TrainAcc=96.518 %| ValAcc=96.640 %\n",
      "   Epoch: 240 | Loss=0.118705162 | TrainAcc=96.547 %| ValAcc=96.660 %\n",
      "   Epoch: 241 | Loss=0.120080533 | TrainAcc=96.542 %| ValAcc=96.620 %\n",
      "   Epoch: 242 | Loss=0.118810946 | TrainAcc=96.576 %| ValAcc=96.720 %\n",
      "   Epoch: 243 | Loss=0.118861000 | TrainAcc=96.545 %| ValAcc=96.720 %\n",
      "   Epoch: 244 | Loss=0.119461329 | TrainAcc=96.571 %| ValAcc=96.820 %\n",
      "   Epoch: 245 | Loss=0.117513268 | TrainAcc=96.573 %| ValAcc=96.660 %\n",
      "   Epoch: 246 | Loss=0.117326301 | TrainAcc=96.573 %| ValAcc=96.780 %\n",
      "   Epoch: 247 | Loss=0.119149440 | TrainAcc=96.589 %| ValAcc=96.680 %\n",
      "   Epoch: 248 | Loss=0.118075147 | TrainAcc=96.620 %| ValAcc=96.740 %\n",
      "   Epoch: 249 | Loss=0.113719353 | TrainAcc=96.649 %| ValAcc=96.800 %\n",
      "   Epoch: 250 | Loss=0.115177694 | TrainAcc=96.642 %| ValAcc=96.760 %\n",
      "   Training Finished in 8045.8 seconds.\n",
      "   Final accuracies:\n",
      "   ~ TrainAcc: 96.642 %\n",
      "   ~ ValAcc: 96.760 %\n",
      "   ~ TestAcc: 96.650 %\n",
      "   Saving model in file: Models/lenet5-model_relu_lr00001_bs50\n",
      "*Model [ lenet5-model_relu_lr00001_bs128 ] {l_r: 0.0001; n_iter: 100; batch: 128}\n",
      "   Start Training!\n",
      "   Epoch: 01 | Loss=2.322786413 | TrainAcc=10.007 %| ValAcc=9.700 %\n",
      "   Epoch: 02 | Loss=2.307046299 | TrainAcc=10.085 %| ValAcc=9.920 %\n",
      "   Epoch: 03 | Loss=2.295670853 | TrainAcc=10.518 %| ValAcc=10.260 %\n",
      "   Epoch: 04 | Loss=2.285771324 | TrainAcc=11.305 %| ValAcc=10.880 %\n",
      "   Epoch: 05 | Loss=2.277320490 | TrainAcc=12.600 %| ValAcc=12.000 %\n",
      "   Epoch: 06 | Loss=2.268347038 | TrainAcc=14.502 %| ValAcc=13.520 %\n",
      "   Epoch: 07 | Loss=2.259515516 | TrainAcc=16.635 %| ValAcc=15.620 %\n",
      "   Epoch: 08 | Loss=2.252328448 | TrainAcc=18.853 %| ValAcc=17.820 %\n",
      "   Epoch: 09 | Loss=2.244325393 | TrainAcc=20.816 %| ValAcc=19.740 %\n",
      "   Epoch: 10 | Loss=2.235092477 | TrainAcc=22.964 %| ValAcc=21.960 %\n",
      "   Epoch: 11 | Loss=2.228000410 | TrainAcc=25.098 %| ValAcc=23.860 %\n",
      "   Epoch: 12 | Loss=2.218495022 | TrainAcc=27.013 %| ValAcc=25.800 %\n",
      "   Epoch: 13 | Loss=2.209493330 | TrainAcc=28.791 %| ValAcc=28.220 %\n",
      "   Epoch: 14 | Loss=2.199719649 | TrainAcc=30.416 %| ValAcc=30.160 %\n",
      "   Epoch: 15 | Loss=2.189365889 | TrainAcc=32.005 %| ValAcc=31.880 %\n",
      "   Epoch: 16 | Loss=2.178001434 | TrainAcc=33.764 %| ValAcc=34.060 %\n",
      "   Epoch: 17 | Loss=2.166993818 | TrainAcc=35.491 %| ValAcc=35.980 %\n",
      "   Epoch: 18 | Loss=2.153940103 | TrainAcc=37.440 %| ValAcc=38.440 %\n",
      "   Epoch: 19 | Loss=2.139086447 | TrainAcc=39.635 %| ValAcc=40.640 %\n",
      "   Epoch: 20 | Loss=2.124312734 | TrainAcc=42.287 %| ValAcc=43.420 %\n",
      "   Epoch: 21 | Loss=2.106967799 | TrainAcc=45.538 %| ValAcc=46.900 %\n",
      "   Epoch: 22 | Loss=2.088326276 | TrainAcc=48.496 %| ValAcc=49.720 %\n",
      "   Epoch: 23 | Loss=2.069166414 | TrainAcc=51.676 %| ValAcc=53.020 %\n",
      "   Epoch: 24 | Loss=2.047771516 | TrainAcc=54.264 %| ValAcc=55.740 %\n",
      "   Epoch: 25 | Loss=2.022704663 | TrainAcc=56.380 %| ValAcc=58.020 %\n",
      "   Epoch: 26 | Loss=1.996585790 | TrainAcc=58.087 %| ValAcc=59.840 %\n",
      "   Epoch: 27 | Loss=1.966928986 | TrainAcc=59.280 %| ValAcc=60.900 %\n",
      "   Epoch: 28 | Loss=1.937688264 | TrainAcc=60.498 %| ValAcc=62.240 %\n",
      "   Epoch: 29 | Loss=1.901715566 | TrainAcc=61.460 %| ValAcc=63.220 %\n",
      "   Epoch: 30 | Loss=1.862793120 | TrainAcc=62.400 %| ValAcc=64.200 %\n",
      "   Epoch: 31 | Loss=1.822145738 | TrainAcc=63.062 %| ValAcc=65.120 %\n",
      "   Epoch: 32 | Loss=1.779418231 | TrainAcc=64.047 %| ValAcc=65.940 %\n",
      "   Epoch: 33 | Loss=1.728231279 | TrainAcc=64.787 %| ValAcc=66.840 %\n",
      "   Epoch: 34 | Loss=1.676680680 | TrainAcc=65.615 %| ValAcc=67.540 %\n",
      "   Epoch: 35 | Loss=1.623943850 | TrainAcc=66.638 %| ValAcc=68.800 %\n",
      "   Epoch: 36 | Loss=1.566972327 | TrainAcc=67.493 %| ValAcc=69.200 %\n",
      "   Epoch: 37 | Loss=1.502996355 | TrainAcc=68.376 %| ValAcc=70.120 %\n",
      "   Epoch: 38 | Loss=1.445534814 | TrainAcc=69.355 %| ValAcc=70.980 %\n",
      "   Epoch: 39 | Loss=1.383146668 | TrainAcc=70.180 %| ValAcc=71.600 %\n",
      "   Epoch: 40 | Loss=1.323725369 | TrainAcc=70.867 %| ValAcc=72.220 %\n",
      "   Epoch: 41 | Loss=1.259714164 | TrainAcc=71.835 %| ValAcc=73.040 %\n",
      "   Epoch: 42 | Loss=1.203402391 | TrainAcc=72.602 %| ValAcc=73.860 %\n",
      "   Epoch: 43 | Loss=1.149834964 | TrainAcc=73.296 %| ValAcc=74.620 %\n",
      "   Epoch: 44 | Loss=1.094195289 | TrainAcc=74.027 %| ValAcc=75.280 %\n",
      "   Epoch: 45 | Loss=1.047919147 | TrainAcc=74.707 %| ValAcc=75.860 %\n",
      "   Epoch: 46 | Loss=1.001486297 | TrainAcc=75.420 %| ValAcc=76.360 %\n",
      "   Epoch: 47 | Loss=0.958827297 | TrainAcc=76.022 %| ValAcc=76.860 %\n",
      "   Epoch: 48 | Loss=0.918125708 | TrainAcc=76.569 %| ValAcc=77.460 %\n",
      "   Epoch: 49 | Loss=0.880327590 | TrainAcc=77.115 %| ValAcc=77.880 %\n",
      "   Epoch: 50 | Loss=0.850251869 | TrainAcc=77.671 %| ValAcc=78.660 %\n",
      "   Epoch: 51 | Loss=0.821696190 | TrainAcc=78.311 %| ValAcc=79.140 %\n",
      "   Epoch: 52 | Loss=0.792248034 | TrainAcc=78.716 %| ValAcc=79.600 %\n",
      "   Epoch: 53 | Loss=0.765896645 | TrainAcc=79.227 %| ValAcc=80.140 %\n",
      "   Epoch: 54 | Loss=0.743200235 | TrainAcc=79.669 %| ValAcc=80.500 %\n",
      "   Epoch: 55 | Loss=0.721796864 | TrainAcc=80.182 %| ValAcc=80.940 %\n",
      "   Epoch: 56 | Loss=0.700939522 | TrainAcc=80.638 %| ValAcc=81.280 %\n",
      "   Epoch: 57 | Loss=0.684760879 | TrainAcc=81.060 %| ValAcc=81.700 %\n",
      "   Epoch: 58 | Loss=0.666939806 | TrainAcc=81.385 %| ValAcc=81.940 %\n",
      "   Epoch: 59 | Loss=0.647422488 | TrainAcc=81.751 %| ValAcc=82.460 %\n",
      "   Epoch: 60 | Loss=0.637413307 | TrainAcc=82.158 %| ValAcc=82.700 %\n",
      "   Epoch: 61 | Loss=0.622192703 | TrainAcc=82.476 %| ValAcc=83.240 %\n",
      "   Epoch: 62 | Loss=0.606586226 | TrainAcc=82.818 %| ValAcc=83.260 %\n",
      "   Epoch: 63 | Loss=0.596082183 | TrainAcc=83.189 %| ValAcc=83.680 %\n",
      "   Epoch: 64 | Loss=0.586861113 | TrainAcc=83.500 %| ValAcc=83.880 %\n",
      "   Epoch: 65 | Loss=0.572539531 | TrainAcc=83.842 %| ValAcc=84.140 %\n",
      "   Epoch: 66 | Loss=0.565598261 | TrainAcc=84.129 %| ValAcc=84.460 %\n",
      "   Epoch: 67 | Loss=0.550423336 | TrainAcc=84.420 %| ValAcc=84.660 %\n",
      "   Epoch: 68 | Loss=0.547154660 | TrainAcc=84.658 %| ValAcc=84.840 %\n",
      "   Epoch: 69 | Loss=0.530562904 | TrainAcc=84.955 %| ValAcc=85.140 %\n",
      "   Epoch: 70 | Loss=0.525955050 | TrainAcc=85.193 %| ValAcc=85.340 %\n",
      "   Epoch: 71 | Loss=0.521893564 | TrainAcc=85.405 %| ValAcc=85.480 %\n",
      "   Epoch: 72 | Loss=0.509557030 | TrainAcc=85.625 %| ValAcc=85.700 %\n",
      "   Epoch: 73 | Loss=0.502819771 | TrainAcc=85.793 %| ValAcc=85.960 %\n",
      "   Epoch: 74 | Loss=0.500924603 | TrainAcc=85.985 %| ValAcc=86.260 %\n",
      "   Epoch: 75 | Loss=0.489013893 | TrainAcc=86.180 %| ValAcc=86.260 %\n",
      "   Epoch: 76 | Loss=0.482810846 | TrainAcc=86.331 %| ValAcc=86.400 %\n",
      "   Epoch: 77 | Loss=0.478583624 | TrainAcc=86.495 %| ValAcc=86.620 %\n",
      "   Epoch: 78 | Loss=0.472155858 | TrainAcc=86.653 %| ValAcc=86.740 %\n",
      "   Epoch: 79 | Loss=0.468178588 | TrainAcc=86.795 %| ValAcc=86.860 %\n",
      "   Epoch: 80 | Loss=0.458374419 | TrainAcc=86.935 %| ValAcc=86.960 %\n",
      "   Epoch: 81 | Loss=0.459943284 | TrainAcc=87.142 %| ValAcc=87.060 %\n",
      "   Epoch: 82 | Loss=0.448902352 | TrainAcc=87.193 %| ValAcc=87.220 %\n",
      "   Epoch: 83 | Loss=0.443457911 | TrainAcc=87.360 %| ValAcc=87.360 %\n",
      "   Epoch: 84 | Loss=0.445978415 | TrainAcc=87.498 %| ValAcc=87.560 %\n",
      "   Epoch: 85 | Loss=0.435353727 | TrainAcc=87.613 %| ValAcc=87.560 %\n",
      "   Epoch: 86 | Loss=0.435593086 | TrainAcc=87.725 %| ValAcc=87.720 %\n",
      "   Epoch: 87 | Loss=0.427134861 | TrainAcc=87.838 %| ValAcc=87.800 %\n",
      "   Epoch: 88 | Loss=0.426932987 | TrainAcc=87.925 %| ValAcc=87.960 %\n",
      "   Epoch: 89 | Loss=0.420769198 | TrainAcc=88.029 %| ValAcc=88.080 %\n",
      "   Epoch: 90 | Loss=0.418061255 | TrainAcc=88.120 %| ValAcc=88.280 %\n",
      "   Epoch: 91 | Loss=0.413985109 | TrainAcc=88.213 %| ValAcc=88.260 %\n",
      "   Epoch: 92 | Loss=0.408096864 | TrainAcc=88.344 %| ValAcc=88.540 %\n",
      "   Epoch: 93 | Loss=0.407318063 | TrainAcc=88.416 %| ValAcc=88.620 %\n",
      "   Epoch: 94 | Loss=0.408314100 | TrainAcc=88.504 %| ValAcc=88.580 %\n",
      "   Epoch: 95 | Loss=0.397142939 | TrainAcc=88.589 %| ValAcc=88.760 %\n",
      "   Epoch: 96 | Loss=0.395994738 | TrainAcc=88.675 %| ValAcc=88.840 %\n",
      "   Epoch: 97 | Loss=0.397768723 | TrainAcc=88.800 %| ValAcc=89.060 %\n",
      "   Epoch: 98 | Loss=0.390778536 | TrainAcc=88.835 %| ValAcc=88.980 %\n",
      "   Epoch: 99 | Loss=0.388622540 | TrainAcc=88.945 %| ValAcc=89.180 %\n",
      "   Epoch: 100 | Loss=0.381312465 | TrainAcc=88.989 %| ValAcc=89.160 %\n",
      "   Training Finished in 2668.7 seconds.\n",
      "   Final accuracies:\n",
      "   ~ TrainAcc: 88.989 %\n",
      "   ~ ValAcc: 89.160 %\n",
      "   ~ TestAcc: 89.590 %\n",
      "   Saving model in file: Models/lenet5-model_relu_lr00001_bs128\n"
     ]
    }
   ],
   "source": [
    "# Trying parameters with SGD\n",
    "setOfParams = [ [0.001, 250, 50, 'lenet5-model_relu_lr0001_bs50'],\n",
    "                [0.001, 100, 128, 'lenet5-model_relu_lr0001_bs128'],\n",
    "                [0.0001, 250, 50, 'lenet5-model_relu_lr00001_bs50'],\n",
    "                [0.0001, 100, 128, 'lenet5-model_relu_lr00001_bs128'],\n",
    "    \n",
    "]\n",
    "for p in setOfParams:\n",
    "    CNNet (modelName=p[3], \n",
    "           learning_rate = p[0], \n",
    "           training_epochs = p[1],\n",
    "           batch_size = p[2],\n",
    "           activationFunc=tf.nn.relu,\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Model [ lenet5-model_adam_relu_lr0001_bs50 ] {l_r: 0.0010; n_iter: 250; batch: 50}\n",
      "   Start Training!\n",
      "   Epoch: 01 | Loss=0.244328993 | TrainAcc=97.695 %| ValAcc=97.900 %\n",
      "   Epoch: 02 | Loss=0.065261318 | TrainAcc=98.433 %| ValAcc=98.400 %\n",
      "   Epoch: 03 | Loss=0.045404583 | TrainAcc=98.811 %| ValAcc=98.600 %\n",
      "   Epoch: 04 | Loss=0.035783383 | TrainAcc=99.156 %| ValAcc=98.740 %\n",
      "   Epoch: 05 | Loss=0.029785236 | TrainAcc=99.391 %| ValAcc=98.880 %\n",
      "   Epoch: 06 | Loss=0.023469996 | TrainAcc=99.227 %| ValAcc=98.580 %\n",
      "   Epoch: 07 | Loss=0.022254807 | TrainAcc=99.578 %| ValAcc=98.780 %\n",
      "   Epoch: 08 | Loss=0.017690490 | TrainAcc=99.673 %| ValAcc=98.980 %\n",
      "   Epoch: 09 | Loss=0.013129403 | TrainAcc=99.747 %| ValAcc=99.180 %\n",
      "   Epoch: 10 | Loss=0.013963227 | TrainAcc=99.555 %| ValAcc=98.620 %\n",
      "   Epoch: 11 | Loss=0.012480628 | TrainAcc=99.584 %| ValAcc=98.860 %\n",
      "   Epoch: 12 | Loss=0.012241643 | TrainAcc=99.720 %| ValAcc=98.840 %\n",
      "   Epoch: 13 | Loss=0.009946326 | TrainAcc=99.849 %| ValAcc=99.180 %\n",
      "   Epoch: 14 | Loss=0.009944076 | TrainAcc=99.820 %| ValAcc=99.180 %\n",
      "   Epoch: 15 | Loss=0.009824660 | TrainAcc=99.811 %| ValAcc=99.120 %\n",
      "   Epoch: 16 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 17 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 18 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 19 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 20 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 21 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 22 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 23 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 24 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 25 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 26 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 27 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 28 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 29 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 30 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 31 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 32 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 33 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 34 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 35 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 36 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 37 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 38 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 39 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 40 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 41 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 42 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 43 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 44 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 45 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 46 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 47 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 48 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 49 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 50 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 51 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 52 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 53 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 54 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 55 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 56 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 57 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 58 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 59 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 60 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 61 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 62 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 63 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 64 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 65 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 66 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 67 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 68 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 69 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 70 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 71 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 72 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 73 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 74 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 75 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 76 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 77 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 78 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 79 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 80 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 81 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 82 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 83 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 84 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 85 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 86 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 87 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 88 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 89 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 90 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 91 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 92 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 93 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 94 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 95 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 96 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 97 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 98 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 99 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 100 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 101 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 102 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 103 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 104 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 105 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 106 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 107 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 108 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 109 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 110 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 111 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 112 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 113 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 114 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 115 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 116 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 117 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 118 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 119 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 120 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 121 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 122 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 123 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 124 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 125 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 126 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 127 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 128 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 129 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 130 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 131 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 132 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 133 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 134 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 135 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 136 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 137 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 138 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 139 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 140 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 141 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 142 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 143 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 144 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 145 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 146 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 147 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 148 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 149 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 150 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 151 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 152 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 153 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 154 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 155 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 156 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 157 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 158 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 159 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 160 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 161 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 162 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 163 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 164 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 165 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 166 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 167 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 168 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 169 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 170 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 171 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 172 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 173 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 174 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 175 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 176 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 177 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 178 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 179 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 180 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 181 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 182 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 183 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 184 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 185 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 186 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 187 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 188 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 189 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 190 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 191 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 192 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 193 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 194 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 195 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 196 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 197 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 198 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 199 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 200 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 201 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 202 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 203 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 204 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 205 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 206 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 207 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 208 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 209 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 210 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 211 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 212 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 213 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 214 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 215 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 216 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 217 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 218 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 219 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 220 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 221 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 222 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 223 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 224 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 225 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 226 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 227 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 228 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 229 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 230 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 231 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 232 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 233 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 234 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 235 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 236 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 237 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 238 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 239 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 240 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 241 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 242 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 243 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 244 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 245 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 246 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 247 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 248 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 249 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 250 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Training Finished in 8012.0 seconds.\n",
      "   Final accuracies:\n",
      "   ~ TrainAcc: 9.898 %\n",
      "   ~ ValAcc: 9.580 %\n",
      "   ~ TestAcc: 9.800 %\n",
      "   Saving model in file: Models/lenet5-model_adam_relu_lr0001_bs50\n",
      "*Model [ lenet5-model_adam_relu_lr0001_bs128 ] {l_r: 0.0010; n_iter: 100; batch: 128}\n",
      "   Start Training!\n",
      "   Epoch: 01 | Loss=0.336491016 | TrainAcc=96.625 %| ValAcc=97.140 %\n",
      "   Epoch: 02 | Loss=0.087823771 | TrainAcc=97.842 %| ValAcc=97.960 %\n",
      "   Epoch: 03 | Loss=0.062794081 | TrainAcc=98.615 %| ValAcc=98.340 %\n",
      "   Epoch: 04 | Loss=0.047299532 | TrainAcc=98.920 %| ValAcc=98.700 %\n",
      "   Epoch: 05 | Loss=0.038719179 | TrainAcc=98.945 %| ValAcc=98.700 %\n",
      "   Epoch: 06 | Loss=0.033209020 | TrainAcc=99.238 %| ValAcc=98.880 %\n",
      "   Epoch: 07 | Loss=0.029150596 | TrainAcc=99.196 %| ValAcc=98.840 %\n",
      "   Epoch: 08 | Loss=0.022505452 | TrainAcc=99.035 %| ValAcc=98.500 %\n",
      "   Epoch: 09 | Loss=0.022066285 | TrainAcc=99.582 %| ValAcc=98.880 %\n",
      "   Epoch: 10 | Loss=0.020705636 | TrainAcc=99.256 %| ValAcc=98.800 %\n",
      "   Epoch: 11 | Loss=0.014939778 | TrainAcc=99.600 %| ValAcc=98.620 %\n",
      "   Epoch: 12 | Loss=0.015436752 | TrainAcc=99.695 %| ValAcc=99.000 %\n",
      "   Epoch: 13 | Loss=0.013359710 | TrainAcc=99.740 %| ValAcc=99.000 %\n",
      "   Epoch: 14 | Loss=0.011378326 | TrainAcc=99.467 %| ValAcc=98.780 %\n",
      "   Epoch: 15 | Loss=0.012907592 | TrainAcc=99.745 %| ValAcc=98.900 %\n",
      "   Epoch: 16 | Loss=0.009455264 | TrainAcc=99.878 %| ValAcc=99.220 %\n",
      "   Epoch: 17 | Loss=0.009712120 | TrainAcc=99.707 %| ValAcc=98.740 %\n",
      "   Epoch: 18 | Loss=0.008370863 | TrainAcc=99.784 %| ValAcc=99.080 %\n",
      "   Epoch: 19 | Loss=0.008306214 | TrainAcc=99.835 %| ValAcc=99.060 %\n",
      "   Epoch: 20 | Loss=0.005540892 | TrainAcc=99.902 %| ValAcc=99.140 %\n",
      "   Epoch: 21 | Loss=0.009586892 | TrainAcc=99.784 %| ValAcc=98.980 %\n",
      "   Epoch: 22 | Loss=0.006334985 | TrainAcc=99.858 %| ValAcc=99.200 %\n",
      "   Epoch: 23 | Loss=0.005407022 | TrainAcc=99.816 %| ValAcc=98.960 %\n",
      "   Epoch: 24 | Loss=0.007346477 | TrainAcc=99.905 %| ValAcc=99.100 %\n",
      "   Epoch: 25 | Loss=0.005765667 | TrainAcc=99.935 %| ValAcc=99.200 %\n",
      "   Epoch: 26 | Loss=0.006273060 | TrainAcc=99.767 %| ValAcc=99.000 %\n",
      "   Epoch: 27 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 28 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 29 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 30 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 31 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 32 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 33 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 34 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 35 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 36 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 37 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 38 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 39 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 40 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 41 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 42 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 43 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 44 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 45 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 46 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 47 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 48 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 49 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 50 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 51 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 52 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 53 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 54 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 55 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 56 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 57 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 58 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 59 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 60 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 61 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 62 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 63 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 64 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 65 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 66 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 67 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 68 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 69 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 70 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 71 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 72 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 73 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 74 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 75 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 76 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 77 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 78 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 79 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 80 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 81 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 82 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 83 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 84 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 85 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 86 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 87 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 88 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 89 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 90 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 91 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 92 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 93 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 94 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 95 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 96 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 97 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 98 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 99 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 100 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Training Finished in 2676.3 seconds.\n",
      "   Final accuracies:\n",
      "   ~ TrainAcc: 9.898 %\n",
      "   ~ ValAcc: 9.580 %\n",
      "   ~ TestAcc: 9.800 %\n",
      "   Saving model in file: Models/lenet5-model_adam_relu_lr0001_bs128\n",
      "*Model [ lenet5-model_adam_relu_lr00001_bs50 ] {l_r: 0.0001; n_iter: 250; batch: 50}\n",
      "   Start Training!\n",
      "   Epoch: 01 | Loss=0.800074414 | TrainAcc=91.435 %| ValAcc=92.040 %\n",
      "   Epoch: 02 | Loss=0.232612559 | TrainAcc=94.473 %| ValAcc=95.240 %\n",
      "   Epoch: 03 | Loss=0.165665199 | TrainAcc=95.824 %| ValAcc=96.360 %\n",
      "   Epoch: 04 | Loss=0.129587101 | TrainAcc=96.598 %| ValAcc=96.860 %\n",
      "   Epoch: 05 | Loss=0.108686594 | TrainAcc=97.136 %| ValAcc=97.340 %\n",
      "   Epoch: 06 | Loss=0.092979553 | TrainAcc=97.418 %| ValAcc=97.560 %\n",
      "   Epoch: 07 | Loss=0.083623754 | TrainAcc=97.549 %| ValAcc=97.740 %\n",
      "   Epoch: 08 | Loss=0.076368846 | TrainAcc=97.896 %| ValAcc=98.060 %\n",
      "   Epoch: 09 | Loss=0.068475015 | TrainAcc=98.104 %| ValAcc=98.140 %\n",
      "   Epoch: 10 | Loss=0.064992078 | TrainAcc=98.182 %| ValAcc=98.260 %\n",
      "   Epoch: 11 | Loss=0.058705781 | TrainAcc=98.475 %| ValAcc=98.280 %\n",
      "   Epoch: 12 | Loss=0.054053025 | TrainAcc=98.398 %| ValAcc=98.380 %\n",
      "   Epoch: 13 | Loss=0.052144248 | TrainAcc=98.429 %| ValAcc=98.300 %\n",
      "   Epoch: 14 | Loss=0.046998210 | TrainAcc=98.689 %| ValAcc=98.460 %\n",
      "   Epoch: 15 | Loss=0.046525351 | TrainAcc=98.704 %| ValAcc=98.320 %\n",
      "   Epoch: 16 | Loss=0.041771256 | TrainAcc=98.713 %| ValAcc=98.400 %\n",
      "   Epoch: 17 | Loss=0.041033997 | TrainAcc=98.878 %| ValAcc=98.540 %\n",
      "   Epoch: 18 | Loss=0.038263413 | TrainAcc=99.018 %| ValAcc=98.660 %\n",
      "   Epoch: 19 | Loss=0.035132884 | TrainAcc=98.962 %| ValAcc=98.680 %\n",
      "   Epoch: 20 | Loss=0.034132983 | TrainAcc=99.018 %| ValAcc=98.620 %\n",
      "   Epoch: 21 | Loss=0.032466385 | TrainAcc=99.151 %| ValAcc=98.640 %\n",
      "   Epoch: 22 | Loss=0.029523266 | TrainAcc=99.053 %| ValAcc=98.740 %\n",
      "   Epoch: 23 | Loss=0.029812324 | TrainAcc=99.291 %| ValAcc=98.680 %\n",
      "   Epoch: 24 | Loss=0.027621515 | TrainAcc=99.324 %| ValAcc=98.560 %\n",
      "   Epoch: 25 | Loss=0.026531568 | TrainAcc=99.255 %| ValAcc=98.700 %\n",
      "   Epoch: 26 | Loss=0.025878912 | TrainAcc=99.238 %| ValAcc=98.700 %\n",
      "   Epoch: 27 | Loss=0.023946666 | TrainAcc=99.236 %| ValAcc=98.800 %\n",
      "   Epoch: 28 | Loss=0.021727157 | TrainAcc=99.473 %| ValAcc=98.880 %\n",
      "   Epoch: 29 | Loss=0.022842133 | TrainAcc=99.340 %| ValAcc=98.680 %\n",
      "   Epoch: 30 | Loss=0.021067322 | TrainAcc=99.493 %| ValAcc=98.960 %\n",
      "   Epoch: 31 | Loss=0.018080928 | TrainAcc=99.360 %| ValAcc=98.900 %\n",
      "   Epoch: 32 | Loss=0.019563682 | TrainAcc=99.427 %| ValAcc=98.780 %\n",
      "   Epoch: 33 | Loss=0.016981285 | TrainAcc=99.398 %| ValAcc=98.800 %\n",
      "   Epoch: 34 | Loss=0.017121460 | TrainAcc=99.515 %| ValAcc=98.880 %\n",
      "   Epoch: 35 | Loss=0.015632870 | TrainAcc=99.551 %| ValAcc=98.880 %\n",
      "   Epoch: 36 | Loss=0.015889073 | TrainAcc=99.684 %| ValAcc=98.820 %\n",
      "   Epoch: 37 | Loss=0.014673981 | TrainAcc=99.705 %| ValAcc=98.820 %\n",
      "   Epoch: 38 | Loss=0.013237425 | TrainAcc=99.704 %| ValAcc=98.860 %\n",
      "   Epoch: 39 | Loss=0.012777660 | TrainAcc=99.631 %| ValAcc=98.760 %\n",
      "   Epoch: 40 | Loss=0.013384059 | TrainAcc=99.595 %| ValAcc=98.820 %\n",
      "   Epoch: 41 | Loss=0.010264258 | TrainAcc=99.738 %| ValAcc=98.880 %\n",
      "   Epoch: 42 | Loss=0.011598217 | TrainAcc=99.749 %| ValAcc=98.740 %\n",
      "   Epoch: 43 | Loss=0.010276781 | TrainAcc=99.793 %| ValAcc=98.960 %\n",
      "   Epoch: 44 | Loss=0.009798509 | TrainAcc=99.782 %| ValAcc=98.920 %\n",
      "   Epoch: 45 | Loss=0.008598425 | TrainAcc=99.825 %| ValAcc=98.920 %\n",
      "   Epoch: 46 | Loss=0.009067529 | TrainAcc=99.851 %| ValAcc=98.960 %\n",
      "   Epoch: 47 | Loss=0.008154073 | TrainAcc=99.862 %| ValAcc=99.020 %\n",
      "   Epoch: 48 | Loss=0.007881212 | TrainAcc=99.838 %| ValAcc=98.820 %\n",
      "   Epoch: 49 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 50 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 51 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 52 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 53 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 54 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 55 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 56 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 57 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 58 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 59 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 60 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 61 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 62 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 63 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 64 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 65 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 66 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 67 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 68 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 69 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 70 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 71 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 72 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 73 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 74 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 75 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 76 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 77 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 78 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 79 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 80 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 81 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 82 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 83 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 84 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 85 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 86 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 87 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 88 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 89 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 90 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 91 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 92 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 93 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 94 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 95 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 96 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 97 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 98 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 99 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 100 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 101 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 102 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 103 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 104 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 105 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 106 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 107 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 108 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 109 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 110 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 111 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 112 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 113 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 114 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 115 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 116 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 117 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 118 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 119 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 120 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 121 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 122 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 123 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 124 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 125 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 126 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 127 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 128 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 129 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 130 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 131 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 132 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 133 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 134 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 135 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 136 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 137 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 138 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 139 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 140 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 141 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 142 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 143 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 144 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 145 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 146 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 147 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 148 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 149 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 150 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 151 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 152 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 153 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 154 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 155 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 156 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 157 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 158 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 159 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 160 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 161 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 162 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 163 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 164 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 165 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 166 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 167 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 168 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 169 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 170 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 171 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 172 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 173 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 174 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 175 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 176 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 177 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 178 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 179 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 180 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 181 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 182 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 183 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 184 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 185 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 186 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 187 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 188 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 189 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 190 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 191 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 192 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 193 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 194 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 195 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 196 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 197 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 198 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 199 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 200 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 201 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 202 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 203 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 204 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 205 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 206 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 207 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 208 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 209 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 210 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 211 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 212 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 213 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 214 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 215 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 216 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 217 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 218 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 219 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 220 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 221 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 222 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 223 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 224 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 225 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 226 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 227 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 228 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 229 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 230 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 231 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 232 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 233 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 234 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 235 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 236 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 237 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 238 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 239 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 240 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 241 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 242 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 243 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 244 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 245 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 246 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 247 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 248 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 249 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 250 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Training Finished in 8012.1 seconds.\n",
      "   Final accuracies:\n",
      "   ~ TrainAcc: 9.898 %\n",
      "   ~ ValAcc: 9.580 %\n",
      "   ~ TestAcc: 9.800 %\n",
      "   Saving model in file: Models/lenet5-model_adam_relu_lr00001_bs50\n",
      "*Model [ lenet5-model_adam_relu_lr00001_bs128 ] {l_r: 0.0001; n_iter: 100; batch: 128}\n",
      "   Start Training!\n",
      "   Epoch: 01 | Loss=1.243151578 | TrainAcc=87.002 %| ValAcc=87.460 %\n",
      "   Epoch: 02 | Loss=0.359933009 | TrainAcc=91.498 %| ValAcc=91.940 %\n",
      "   Epoch: 03 | Loss=0.259833849 | TrainAcc=93.315 %| ValAcc=93.820 %\n",
      "   Epoch: 04 | Loss=0.204902797 | TrainAcc=94.387 %| ValAcc=94.980 %\n",
      "   Epoch: 05 | Loss=0.174768799 | TrainAcc=95.224 %| ValAcc=95.640 %\n",
      "   Epoch: 06 | Loss=0.146592573 | TrainAcc=96.018 %| ValAcc=96.600 %\n",
      "   Epoch: 07 | Loss=0.130953929 | TrainAcc=96.384 %| ValAcc=96.860 %\n",
      "   Epoch: 08 | Loss=0.118611152 | TrainAcc=96.635 %| ValAcc=97.180 %\n",
      "   Epoch: 09 | Loss=0.106283579 | TrainAcc=97.056 %| ValAcc=97.460 %\n",
      "   Epoch: 10 | Loss=0.095082833 | TrainAcc=97.253 %| ValAcc=97.620 %\n",
      "   Epoch: 11 | Loss=0.090205770 | TrainAcc=97.435 %| ValAcc=97.860 %\n",
      "   Epoch: 12 | Loss=0.082553578 | TrainAcc=97.507 %| ValAcc=97.720 %\n",
      "   Epoch: 13 | Loss=0.077852895 | TrainAcc=97.616 %| ValAcc=97.960 %\n",
      "   Epoch: 14 | Loss=0.071480656 | TrainAcc=97.775 %| ValAcc=98.020 %\n",
      "   Epoch: 15 | Loss=0.070990781 | TrainAcc=98.031 %| ValAcc=98.360 %\n",
      "   Epoch: 16 | Loss=0.064028748 | TrainAcc=97.905 %| ValAcc=98.060 %\n",
      "   Epoch: 17 | Loss=0.063709609 | TrainAcc=98.296 %| ValAcc=98.240 %\n",
      "   Epoch: 18 | Loss=0.058705929 | TrainAcc=98.385 %| ValAcc=98.320 %\n",
      "   Epoch: 19 | Loss=0.056092837 | TrainAcc=98.400 %| ValAcc=98.440 %\n",
      "   Epoch: 20 | Loss=0.054884658 | TrainAcc=98.358 %| ValAcc=98.340 %\n",
      "   Epoch: 21 | Loss=0.051762977 | TrainAcc=98.569 %| ValAcc=98.460 %\n",
      "   Epoch: 22 | Loss=0.050787053 | TrainAcc=98.518 %| ValAcc=98.220 %\n",
      "   Epoch: 23 | Loss=0.045924386 | TrainAcc=98.607 %| ValAcc=98.560 %\n",
      "   Epoch: 24 | Loss=0.047333643 | TrainAcc=98.760 %| ValAcc=98.560 %\n",
      "   Epoch: 25 | Loss=0.043663675 | TrainAcc=98.807 %| ValAcc=98.580 %\n",
      "   Epoch: 26 | Loss=0.041773758 | TrainAcc=98.651 %| ValAcc=98.620 %\n",
      "   Epoch: 27 | Loss=0.042589592 | TrainAcc=98.667 %| ValAcc=98.520 %\n",
      "   Epoch: 28 | Loss=0.039210400 | TrainAcc=98.771 %| ValAcc=98.680 %\n",
      "   Epoch: 29 | Loss=0.037513105 | TrainAcc=98.947 %| ValAcc=98.520 %\n",
      "   Epoch: 30 | Loss=0.037314608 | TrainAcc=98.947 %| ValAcc=98.720 %\n",
      "   Epoch: 31 | Loss=0.035301512 | TrainAcc=99.024 %| ValAcc=98.580 %\n",
      "   Epoch: 32 | Loss=0.035654814 | TrainAcc=98.989 %| ValAcc=98.680 %\n",
      "   Epoch: 33 | Loss=0.032341450 | TrainAcc=98.987 %| ValAcc=98.620 %\n",
      "   Epoch: 34 | Loss=0.033059189 | TrainAcc=99.111 %| ValAcc=98.580 %\n",
      "   Epoch: 35 | Loss=0.030910072 | TrainAcc=99.040 %| ValAcc=98.620 %\n",
      "   Epoch: 36 | Loss=0.030249225 | TrainAcc=99.205 %| ValAcc=98.620 %\n",
      "   Epoch: 37 | Loss=0.028323251 | TrainAcc=99.165 %| ValAcc=98.620 %\n",
      "   Epoch: 38 | Loss=0.027994644 | TrainAcc=99.131 %| ValAcc=98.580 %\n",
      "   Epoch: 39 | Loss=0.028260773 | TrainAcc=99.304 %| ValAcc=98.740 %\n",
      "   Epoch: 40 | Loss=0.024990981 | TrainAcc=99.211 %| ValAcc=98.640 %\n",
      "   Epoch: 41 | Loss=0.025866443 | TrainAcc=99.284 %| ValAcc=98.720 %\n",
      "   Epoch: 42 | Loss=0.024167910 | TrainAcc=99.316 %| ValAcc=98.700 %\n",
      "   Epoch: 43 | Loss=0.025538293 | TrainAcc=99.315 %| ValAcc=98.660 %\n",
      "   Epoch: 44 | Loss=0.023930598 | TrainAcc=99.315 %| ValAcc=98.880 %\n",
      "   Epoch: 45 | Loss=0.021857773 | TrainAcc=99.387 %| ValAcc=98.740 %\n",
      "   Epoch: 46 | Loss=0.022430068 | TrainAcc=99.447 %| ValAcc=98.640 %\n",
      "   Epoch: 47 | Loss=0.018786778 | TrainAcc=99.444 %| ValAcc=98.760 %\n",
      "   Epoch: 48 | Loss=0.021054137 | TrainAcc=99.500 %| ValAcc=98.920 %\n",
      "   Epoch: 49 | Loss=0.019998885 | TrainAcc=99.462 %| ValAcc=98.680 %\n",
      "   Epoch: 50 | Loss=0.019766372 | TrainAcc=99.576 %| ValAcc=98.880 %\n",
      "   Epoch: 51 | Loss=0.018265747 | TrainAcc=99.464 %| ValAcc=98.880 %\n",
      "   Epoch: 52 | Loss=0.017254081 | TrainAcc=99.498 %| ValAcc=98.780 %\n",
      "   Epoch: 53 | Loss=0.018538862 | TrainAcc=99.529 %| ValAcc=98.600 %\n",
      "   Epoch: 54 | Loss=0.016953231 | TrainAcc=99.531 %| ValAcc=98.880 %\n",
      "   Epoch: 55 | Loss=0.015863687 | TrainAcc=99.609 %| ValAcc=98.680 %\n",
      "   Epoch: 56 | Loss=0.014810196 | TrainAcc=99.642 %| ValAcc=98.880 %\n",
      "   Epoch: 57 | Loss=0.015748078 | TrainAcc=99.524 %| ValAcc=98.760 %\n",
      "   Epoch: 58 | Loss=0.015422316 | TrainAcc=99.571 %| ValAcc=98.760 %\n",
      "   Epoch: 59 | Loss=0.013619001 | TrainAcc=99.716 %| ValAcc=98.880 %\n",
      "   Epoch: 60 | Loss=0.013755894 | TrainAcc=99.673 %| ValAcc=98.920 %\n",
      "   Epoch: 61 | Loss=0.013852081 | TrainAcc=99.736 %| ValAcc=98.820 %\n",
      "   Epoch: 62 | Loss=0.011031355 | TrainAcc=99.669 %| ValAcc=98.760 %\n",
      "   Epoch: 63 | Loss=0.011998245 | TrainAcc=99.784 %| ValAcc=98.860 %\n",
      "   Epoch: 64 | Loss=0.012225419 | TrainAcc=99.711 %| ValAcc=98.920 %\n",
      "   Epoch: 65 | Loss=0.011627046 | TrainAcc=99.780 %| ValAcc=98.900 %\n",
      "   Epoch: 66 | Loss=0.010133227 | TrainAcc=99.705 %| ValAcc=98.780 %\n",
      "   Epoch: 67 | Loss=0.011849558 | TrainAcc=99.796 %| ValAcc=98.980 %\n",
      "   Epoch: 68 | Loss=0.009863431 | TrainAcc=99.642 %| ValAcc=98.540 %\n",
      "   Epoch: 69 | Loss=0.009812404 | TrainAcc=99.669 %| ValAcc=98.740 %\n",
      "   Epoch: 70 | Loss=0.009197602 | TrainAcc=99.816 %| ValAcc=98.840 %\n",
      "   Epoch: 71 | Loss=0.009015236 | TrainAcc=99.682 %| ValAcc=98.660 %\n",
      "   Epoch: 72 | Loss=0.009391751 | TrainAcc=99.780 %| ValAcc=98.780 %\n",
      "   Epoch: 73 | Loss=0.009050498 | TrainAcc=99.835 %| ValAcc=98.760 %\n",
      "   Epoch: 74 | Loss=0.007686785 | TrainAcc=99.875 %| ValAcc=98.800 %\n",
      "   Epoch: 75 | Loss=0.008135438 | TrainAcc=99.871 %| ValAcc=98.820 %\n",
      "   Epoch: 76 | Loss=0.007799114 | TrainAcc=99.844 %| ValAcc=98.860 %\n",
      "   Epoch: 77 | Loss=0.006827752 | TrainAcc=99.909 %| ValAcc=98.920 %\n",
      "   Epoch: 78 | Loss=0.006551904 | TrainAcc=99.889 %| ValAcc=98.900 %\n",
      "   Epoch: 79 | Loss=0.007174227 | TrainAcc=99.813 %| ValAcc=98.740 %\n",
      "   Epoch: 80 | Loss=0.006238979 | TrainAcc=99.913 %| ValAcc=98.880 %\n",
      "   Epoch: 81 | Loss=0.006978288 | TrainAcc=99.896 %| ValAcc=98.880 %\n",
      "   Epoch: 82 | Loss=0.005869644 | TrainAcc=99.876 %| ValAcc=98.800 %\n",
      "   Epoch: 83 | Loss=0.005809541 | TrainAcc=99.911 %| ValAcc=98.960 %\n",
      "   Epoch: 84 | Loss=0.005085145 | TrainAcc=99.871 %| ValAcc=98.880 %\n",
      "   Epoch: 85 | Loss=0.006105772 | TrainAcc=99.925 %| ValAcc=98.940 %\n",
      "   Epoch: 86 | Loss=0.006132292 | TrainAcc=99.942 %| ValAcc=98.920 %\n",
      "   Epoch: 87 | Loss=0.004119795 | TrainAcc=99.951 %| ValAcc=98.940 %\n",
      "   Epoch: 88 | Loss=0.004874629 | TrainAcc=99.935 %| ValAcc=98.900 %\n",
      "   Epoch: 89 | Loss=0.005838063 | TrainAcc=99.758 %| ValAcc=98.800 %\n",
      "   Epoch: 90 | Loss=0.004928051 | TrainAcc=99.949 %| ValAcc=98.800 %\n",
      "   Epoch: 91 | Loss=0.004511933 | TrainAcc=99.865 %| ValAcc=98.880 %\n",
      "   Epoch: 92 | Loss=0.003457094 | TrainAcc=99.944 %| ValAcc=98.840 %\n",
      "   Epoch: 93 | Loss=0.003789006 | TrainAcc=99.909 %| ValAcc=98.920 %\n",
      "   Epoch: 94 | Loss=0.003867277 | TrainAcc=99.882 %| ValAcc=98.840 %\n",
      "   Epoch: 95 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 96 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 97 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 98 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 99 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Epoch: 100 | Loss=nan | TrainAcc=9.898 %| ValAcc=9.580 %\n",
      "   Training Finished in 2691.6 seconds.\n",
      "   Final accuracies:\n",
      "   ~ TrainAcc: 9.898 %\n",
      "   ~ ValAcc: 9.580 %\n",
      "   ~ TestAcc: 9.800 %\n",
      "   Saving model in file: Models/lenet5-model_adam_relu_lr00001_bs128\n"
     ]
    }
   ],
   "source": [
    "# Trying parameters with Adam Optimizer\n",
    "setOfParams = [ [0.001, 250, 50, 'lenet5-model_adam_relu_lr0001_bs50'],\n",
    "                [0.001, 100, 128, 'lenet5-model_adam_relu_lr0001_bs128'],\n",
    "                [0.0001, 250, 50, 'lenet5-model_adam_relu_lr00001_bs50'],\n",
    "                [0.0001, 100, 128, 'lenet5-model_adam_relu_lr00001_bs128'],\n",
    "    \n",
    "]\n",
    "for p in setOfParams:\n",
    "    CNNet (modelName=p[3], \n",
    "           learning_rate = p[0], \n",
    "           training_epochs = p[1],\n",
    "           batch_size = p[2],\n",
    "           activationFunc=tf.nn.relu,\n",
    "           optimFunc=tf.train.AdamOptimizer\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>COMMENT:</b><br/>\n",
    "As seen in the training, the AdamOptimizer gets NaN at a certain point. In order to make it numerically stable, we have included a small value in the logarighm of the loss node in the grapl (1e-9), and we have reduced the standard deviation in the weight initialization\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Model [ lenet5-model_adam_relu_lr0001_bs50 ] {l_r: 0.0010; n_iter: 100; batch: 50}\n",
      "   Start Training!\n",
      "   Epoch: 01 | Loss=0.502500334 | TrainAcc=95.858 % | ValAcc=96.600 %\n",
      "   Epoch: 02 | Loss=0.106422822 | TrainAcc=97.651 % | ValAcc=97.320 %\n",
      "   Epoch: 03 | Loss=0.073697202 | TrainAcc=98.216 % | ValAcc=98.040 %\n",
      "   Epoch: 04 | Loss=0.058289272 | TrainAcc=98.545 % | ValAcc=98.040 %\n",
      "   Epoch: 05 | Loss=0.045475034 | TrainAcc=98.578 % | ValAcc=98.200 %\n",
      "   Epoch: 06 | Loss=0.040004382 | TrainAcc=99.122 % | ValAcc=98.800 %\n",
      "   Epoch: 07 | Loss=0.032673832 | TrainAcc=99.055 % | ValAcc=98.660 %\n",
      "   Epoch: 08 | Loss=0.030387537 | TrainAcc=99.180 % | ValAcc=98.800 %\n",
      "   Epoch: 09 | Loss=0.024355082 | TrainAcc=99.173 % | ValAcc=98.700 %\n",
      "   Epoch: 10 | Loss=0.023165833 | TrainAcc=99.229 % | ValAcc=98.820 %\n",
      "   Epoch: 11 | Loss=0.018626527 | TrainAcc=99.598 % | ValAcc=99.080 %\n",
      "   Epoch: 12 | Loss=0.018691430 | TrainAcc=99.309 % | ValAcc=98.560 %\n",
      "   Epoch: 13 | Loss=0.016446427 | TrainAcc=99.685 % | ValAcc=99.020 %\n",
      "   Epoch: 14 | Loss=0.014194758 | TrainAcc=99.722 % | ValAcc=98.980 %\n",
      "   Epoch: 15 | Loss=0.013725779 | TrainAcc=99.385 % | ValAcc=98.700 %\n",
      "   Epoch: 16 | Loss=0.012012833 | TrainAcc=99.685 % | ValAcc=98.920 %\n",
      "   Epoch: 17 | Loss=0.009427501 | TrainAcc=99.267 % | ValAcc=98.480 %\n",
      "   Epoch: 18 | Loss=0.010868581 | TrainAcc=99.798 % | ValAcc=99.100 %\n",
      "   Epoch: 19 | Loss=0.010313067 | TrainAcc=99.782 % | ValAcc=99.060 %\n",
      "   Epoch: 20 | Loss=0.009137813 | TrainAcc=99.756 % | ValAcc=98.960 %\n",
      "   Epoch: 21 | Loss=0.006883903 | TrainAcc=99.855 % | ValAcc=99.100 %\n",
      "   Epoch: 22 | Loss=0.008016893 | TrainAcc=99.884 % | ValAcc=99.100 %\n",
      "   Epoch: 23 | Loss=0.006898535 | TrainAcc=99.762 % | ValAcc=99.080 %\n",
      "   Epoch: 24 | Loss=0.007394552 | TrainAcc=99.856 % | ValAcc=99.080 %\n",
      "   Epoch: 25 | Loss=0.007185219 | TrainAcc=99.749 % | ValAcc=98.960 %\n",
      "   Epoch: 26 | Loss=0.006179710 | TrainAcc=99.873 % | ValAcc=99.120 %\n",
      "   Epoch: 27 | Loss=0.005712089 | TrainAcc=99.873 % | ValAcc=99.180 %\n",
      "   Epoch: 28 | Loss=0.006438843 | TrainAcc=99.920 % | ValAcc=99.240 %\n",
      "   Epoch: 29 | Loss=0.006216850 | TrainAcc=99.784 % | ValAcc=99.020 %\n",
      "   Epoch: 30 | Loss=0.005091197 | TrainAcc=99.609 % | ValAcc=98.840 %\n",
      "   Epoch: 31 | Loss=0.006682526 | TrainAcc=99.824 % | ValAcc=99.060 %\n",
      "   Epoch: 32 | Loss=0.004923757 | TrainAcc=99.345 % | ValAcc=98.600 %\n",
      "   Epoch: 33 | Loss=0.005443350 | TrainAcc=99.645 % | ValAcc=98.800 %\n",
      "   Epoch: 34 | Loss=0.005969307 | TrainAcc=99.933 % | ValAcc=99.100 %\n",
      "   Epoch: 35 | Loss=0.003896548 | TrainAcc=99.862 % | ValAcc=99.080 %\n",
      "   Epoch: 36 | Loss=0.004548977 | TrainAcc=99.887 % | ValAcc=99.080 %\n",
      "   Epoch: 37 | Loss=0.005475617 | TrainAcc=99.927 % | ValAcc=99.120 %\n",
      "   Epoch: 38 | Loss=0.004850613 | TrainAcc=99.796 % | ValAcc=99.020 %\n",
      "   Epoch: 39 | Loss=0.005263624 | TrainAcc=99.645 % | ValAcc=98.920 %\n",
      "   Epoch: 40 | Loss=0.005503693 | TrainAcc=99.911 % | ValAcc=99.100 %\n",
      "   Epoch: 41 | Loss=0.004473619 | TrainAcc=99.960 % | ValAcc=99.280 %\n",
      "   Epoch: 42 | Loss=0.003109437 | TrainAcc=99.829 % | ValAcc=98.880 %\n",
      "   Epoch: 43 | Loss=0.004165636 | TrainAcc=99.936 % | ValAcc=98.940 %\n",
      "   Epoch: 44 | Loss=0.005126901 | TrainAcc=99.927 % | ValAcc=99.100 %\n",
      "   Epoch: 45 | Loss=0.003757262 | TrainAcc=99.942 % | ValAcc=99.100 %\n",
      "   Epoch: 46 | Loss=0.005871023 | TrainAcc=99.916 % | ValAcc=99.280 %\n",
      "   Epoch: 47 | Loss=0.004543459 | TrainAcc=99.931 % | ValAcc=99.140 %\n",
      "   Epoch: 48 | Loss=0.002589572 | TrainAcc=99.811 % | ValAcc=99.180 %\n",
      "   Epoch: 49 | Loss=0.004607118 | TrainAcc=99.898 % | ValAcc=99.140 %\n",
      "   Epoch: 50 | Loss=0.003624668 | TrainAcc=99.756 % | ValAcc=98.960 %\n",
      "   Epoch: 51 | Loss=0.002096287 | TrainAcc=99.949 % | ValAcc=99.240 %\n",
      "   Epoch: 52 | Loss=0.006060471 | TrainAcc=99.913 % | ValAcc=99.080 %\n",
      "   Epoch: 53 | Loss=0.002603652 | TrainAcc=99.967 % | ValAcc=99.320 %\n",
      "   Epoch: 54 | Loss=0.004032092 | TrainAcc=99.718 % | ValAcc=98.940 %\n",
      "   Epoch: 55 | Loss=0.005231744 | TrainAcc=99.949 % | ValAcc=99.220 %\n",
      "   Epoch: 56 | Loss=0.003214992 | TrainAcc=99.971 % | ValAcc=99.320 %\n",
      "   Epoch: 57 | Loss=0.002487833 | TrainAcc=99.944 % | ValAcc=99.280 %\n",
      "   Epoch: 58 | Loss=0.003247790 | TrainAcc=99.916 % | ValAcc=99.180 %\n",
      "   Epoch: 59 | Loss=0.003783373 | TrainAcc=99.949 % | ValAcc=99.260 %\n",
      "   Epoch: 60 | Loss=0.004298883 | TrainAcc=99.982 % | ValAcc=99.180 %\n",
      "   Epoch: 61 | Loss=0.001291642 | TrainAcc=99.795 % | ValAcc=99.060 %\n",
      "   Epoch: 62 | Loss=0.005252897 | TrainAcc=99.953 % | ValAcc=99.100 %\n",
      "   Epoch: 63 | Loss=0.003517381 | TrainAcc=99.947 % | ValAcc=99.080 %\n",
      "   Epoch: 64 | Loss=0.001931658 | TrainAcc=99.896 % | ValAcc=99.020 %\n",
      "   Epoch: 65 | Loss=0.005800207 | TrainAcc=99.867 % | ValAcc=98.740 %\n",
      "   Epoch: 66 | Loss=0.002431609 | TrainAcc=99.933 % | ValAcc=99.020 %\n",
      "   Epoch: 67 | Loss=0.003836611 | TrainAcc=99.931 % | ValAcc=99.240 %\n",
      "   Epoch: 68 | Loss=0.003983492 | TrainAcc=99.924 % | ValAcc=99.260 %\n",
      "   Epoch: 69 | Loss=0.001929705 | TrainAcc=99.944 % | ValAcc=99.160 %\n",
      "   Epoch: 70 | Loss=0.003326075 | TrainAcc=99.989 % | ValAcc=99.160 %\n",
      "   Epoch: 71 | Loss=0.001090932 | TrainAcc=99.895 % | ValAcc=98.980 %\n",
      "   Epoch: 72 | Loss=0.004659232 | TrainAcc=99.913 % | ValAcc=99.040 %\n",
      "   Epoch: 73 | Loss=0.002539748 | TrainAcc=99.942 % | ValAcc=99.180 %\n",
      "   Epoch: 74 | Loss=0.003375766 | TrainAcc=99.936 % | ValAcc=99.020 %\n",
      "   Epoch: 75 | Loss=0.004620667 | TrainAcc=99.956 % | ValAcc=99.200 %\n",
      "   Epoch: 76 | Loss=0.002987844 | TrainAcc=99.985 % | ValAcc=99.220 %\n",
      "   Epoch: 77 | Loss=0.003254300 | TrainAcc=99.915 % | ValAcc=99.140 %\n",
      "   Epoch: 78 | Loss=0.002797339 | TrainAcc=99.980 % | ValAcc=99.160 %\n",
      "   Epoch: 79 | Loss=0.002236209 | TrainAcc=99.929 % | ValAcc=99.020 %\n",
      "   Epoch: 80 | Loss=0.004490857 | TrainAcc=99.833 % | ValAcc=98.940 %\n",
      "   Epoch: 81 | Loss=0.001345857 | TrainAcc=99.991 % | ValAcc=99.240 %\n",
      "   Epoch: 82 | Loss=0.000267231 | TrainAcc=99.998 % | ValAcc=99.260 %\n",
      "   Epoch: 83 | Loss=0.006268715 | TrainAcc=99.893 % | ValAcc=99.060 %\n",
      "   Epoch: 84 | Loss=0.003011923 | TrainAcc=99.967 % | ValAcc=99.060 %\n",
      "   Epoch: 85 | Loss=0.001397270 | TrainAcc=99.971 % | ValAcc=99.280 %\n",
      "   Epoch: 86 | Loss=0.002735677 | TrainAcc=99.933 % | ValAcc=99.080 %\n",
      "   Epoch: 87 | Loss=0.004854355 | TrainAcc=99.965 % | ValAcc=99.080 %\n",
      "   Epoch: 88 | Loss=0.002383849 | TrainAcc=99.985 % | ValAcc=99.160 %\n",
      "   Epoch: 89 | Loss=0.000544503 | TrainAcc=99.985 % | ValAcc=99.180 %\n",
      "   Epoch: 90 | Loss=0.005933389 | TrainAcc=99.918 % | ValAcc=98.980 %\n",
      "   Epoch: 91 | Loss=0.001724779 | TrainAcc=99.945 % | ValAcc=98.980 %\n",
      "   Epoch: 92 | Loss=0.003749183 | TrainAcc=99.918 % | ValAcc=98.920 %\n",
      "   Epoch: 93 | Loss=0.002835511 | TrainAcc=99.893 % | ValAcc=99.080 %\n",
      "   Epoch: 94 | Loss=0.004027278 | TrainAcc=99.971 % | ValAcc=99.080 %\n",
      "   Epoch: 95 | Loss=0.002428934 | TrainAcc=99.962 % | ValAcc=99.120 %\n",
      "   Epoch: 96 | Loss=0.002612717 | TrainAcc=99.980 % | ValAcc=99.160 %\n",
      "   Epoch: 97 | Loss=0.003043559 | TrainAcc=99.973 % | ValAcc=99.180 %\n",
      "   Epoch: 98 | Loss=0.003138739 | TrainAcc=99.940 % | ValAcc=99.080 %\n",
      "   Epoch: 99 | Loss=0.001992881 | TrainAcc=99.949 % | ValAcc=99.160 %\n",
      "   Epoch: 100 | Loss=0.004590027 | TrainAcc=99.873 % | ValAcc=98.980 %\n",
      "   Training Finished in 3280.2 seconds.\n",
      "   Final accuracies:\n",
      "   ~ TrainAcc: 99.873 %\n",
      "   ~ ValAcc: 98.980 %\n",
      "   ~ TestAcc: 98.820 %\n",
      "   Saving model in file: Models/lenet5-model_adam_relu_lr0001_bs50\n",
      "*Model [ lenet5-model_adam_relu_lr0001_bs128 ] {l_r: 0.0010; n_iter: 100; batch: 128}\n",
      "   Start Training!\n",
      "   Epoch: 01 | Loss=1.002236798 | TrainAcc=91.249 % | ValAcc=92.020 %\n",
      "   Epoch: 02 | Loss=0.175105191 | TrainAcc=96.762 % | ValAcc=96.880 %\n",
      "   Epoch: 03 | Loss=0.097114358 | TrainAcc=97.424 % | ValAcc=97.580 %\n",
      "   Epoch: 04 | Loss=0.073504702 | TrainAcc=98.193 % | ValAcc=98.060 %\n",
      "   Epoch: 05 | Loss=0.064052049 | TrainAcc=98.487 % | ValAcc=98.260 %\n",
      "   Epoch: 06 | Loss=0.052486020 | TrainAcc=98.707 % | ValAcc=98.360 %\n",
      "   Epoch: 07 | Loss=0.045462362 | TrainAcc=98.645 % | ValAcc=98.320 %\n",
      "   Epoch: 08 | Loss=0.042176366 | TrainAcc=98.784 % | ValAcc=98.360 %\n",
      "   Epoch: 09 | Loss=0.035755820 | TrainAcc=99.085 % | ValAcc=98.740 %\n",
      "   Epoch: 10 | Loss=0.031740524 | TrainAcc=99.091 % | ValAcc=98.640 %\n",
      "   Epoch: 11 | Loss=0.028404766 | TrainAcc=98.945 % | ValAcc=98.520 %\n",
      "   Epoch: 12 | Loss=0.026590637 | TrainAcc=99.355 % | ValAcc=98.720 %\n",
      "   Epoch: 13 | Loss=0.022017988 | TrainAcc=99.476 % | ValAcc=98.820 %\n",
      "   Epoch: 14 | Loss=0.022185760 | TrainAcc=99.495 % | ValAcc=99.040 %\n",
      "   Epoch: 15 | Loss=0.019726115 | TrainAcc=99.505 % | ValAcc=98.780 %\n",
      "   Epoch: 16 | Loss=0.015350560 | TrainAcc=99.669 % | ValAcc=98.920 %\n",
      "   Epoch: 17 | Loss=0.016147256 | TrainAcc=99.475 % | ValAcc=98.800 %\n",
      "   Epoch: 18 | Loss=0.013094706 | TrainAcc=99.795 % | ValAcc=98.960 %\n",
      "   Epoch: 19 | Loss=0.012707344 | TrainAcc=99.453 % | ValAcc=98.820 %\n",
      "   Epoch: 20 | Loss=0.012458899 | TrainAcc=99.773 % | ValAcc=98.860 %\n",
      "   Epoch: 21 | Loss=0.010439487 | TrainAcc=99.602 % | ValAcc=98.860 %\n",
      "   Epoch: 22 | Loss=0.010920267 | TrainAcc=99.842 % | ValAcc=99.000 %\n",
      "   Epoch: 23 | Loss=0.009402394 | TrainAcc=99.776 % | ValAcc=98.860 %\n",
      "   Epoch: 24 | Loss=0.008562180 | TrainAcc=99.867 % | ValAcc=98.980 %\n",
      "   Epoch: 25 | Loss=0.007158565 | TrainAcc=99.738 % | ValAcc=98.840 %\n",
      "   Epoch: 26 | Loss=0.006940532 | TrainAcc=99.853 % | ValAcc=99.000 %\n",
      "   Epoch: 27 | Loss=0.005100343 | TrainAcc=99.825 % | ValAcc=98.840 %\n",
      "   Epoch: 28 | Loss=0.007680293 | TrainAcc=99.869 % | ValAcc=99.140 %\n",
      "   Epoch: 29 | Loss=0.006324617 | TrainAcc=99.787 % | ValAcc=98.900 %\n",
      "   Epoch: 30 | Loss=0.006417040 | TrainAcc=99.773 % | ValAcc=98.860 %\n",
      "   Epoch: 31 | Loss=0.009357255 | TrainAcc=99.853 % | ValAcc=98.900 %\n",
      "   Epoch: 32 | Loss=0.005197679 | TrainAcc=99.895 % | ValAcc=98.980 %\n",
      "   Epoch: 33 | Loss=0.004114275 | TrainAcc=99.949 % | ValAcc=99.040 %\n",
      "   Epoch: 34 | Loss=0.005493058 | TrainAcc=99.815 % | ValAcc=98.880 %\n",
      "   Epoch: 35 | Loss=0.003721849 | TrainAcc=99.613 % | ValAcc=98.500 %\n",
      "   Epoch: 36 | Loss=0.004956170 | TrainAcc=99.700 % | ValAcc=98.620 %\n",
      "   Epoch: 37 | Loss=0.007215095 | TrainAcc=99.891 % | ValAcc=98.860 %\n",
      "   Epoch: 38 | Loss=0.003994456 | TrainAcc=99.436 % | ValAcc=98.640 %\n",
      "   Epoch: 39 | Loss=0.005395620 | TrainAcc=99.940 % | ValAcc=99.000 %\n",
      "   Epoch: 40 | Loss=0.004410369 | TrainAcc=99.907 % | ValAcc=98.960 %\n",
      "   Epoch: 41 | Loss=0.003368580 | TrainAcc=99.855 % | ValAcc=98.840 %\n",
      "   Epoch: 42 | Loss=0.006076222 | TrainAcc=99.907 % | ValAcc=99.040 %\n",
      "   Epoch: 43 | Loss=0.002378694 | TrainAcc=99.960 % | ValAcc=99.160 %\n",
      "   Epoch: 44 | Loss=0.002030471 | TrainAcc=99.936 % | ValAcc=98.880 %\n",
      "   Epoch: 45 | Loss=0.008238401 | TrainAcc=99.776 % | ValAcc=98.820 %\n",
      "   Epoch: 46 | Loss=0.002818661 | TrainAcc=99.965 % | ValAcc=99.000 %\n",
      "   Epoch: 47 | Loss=0.002121241 | TrainAcc=99.975 % | ValAcc=99.140 %\n",
      "   Epoch: 48 | Loss=0.003487663 | TrainAcc=99.900 % | ValAcc=98.920 %\n",
      "   Epoch: 49 | Loss=0.005742704 | TrainAcc=99.875 % | ValAcc=98.760 %\n",
      "   Epoch: 50 | Loss=0.003228344 | TrainAcc=99.833 % | ValAcc=98.960 %\n",
      "   Epoch: 51 | Loss=0.004218740 | TrainAcc=99.980 % | ValAcc=99.040 %\n",
      "   Epoch: 52 | Loss=0.002472707 | TrainAcc=99.980 % | ValAcc=99.080 %\n",
      "   Epoch: 53 | Loss=0.000636644 | TrainAcc=99.996 % | ValAcc=99.140 %\n",
      "   Epoch: 54 | Loss=0.001012747 | TrainAcc=99.855 % | ValAcc=98.800 %\n",
      "   Epoch: 55 | Loss=0.007610590 | TrainAcc=99.676 % | ValAcc=98.380 %\n",
      "   Epoch: 56 | Loss=0.003878254 | TrainAcc=99.858 % | ValAcc=99.020 %\n",
      "   Epoch: 57 | Loss=0.005084838 | TrainAcc=99.969 % | ValAcc=99.060 %\n",
      "   Epoch: 58 | Loss=0.000523700 | TrainAcc=99.980 % | ValAcc=99.080 %\n",
      "   Epoch: 59 | Loss=0.000786924 | TrainAcc=99.984 % | ValAcc=99.160 %\n",
      "   Epoch: 60 | Loss=0.003253044 | TrainAcc=99.640 % | ValAcc=98.780 %\n",
      "   Epoch: 61 | Loss=0.006802880 | TrainAcc=99.931 % | ValAcc=98.980 %\n",
      "   Epoch: 62 | Loss=0.001660514 | TrainAcc=99.995 % | ValAcc=99.080 %\n",
      "   Epoch: 63 | Loss=0.000323113 | TrainAcc=99.984 % | ValAcc=99.020 %\n",
      "   Epoch: 64 | Loss=0.003235942 | TrainAcc=99.893 % | ValAcc=99.040 %\n",
      "   Epoch: 65 | Loss=0.003523793 | TrainAcc=99.945 % | ValAcc=98.980 %\n",
      "   Epoch: 66 | Loss=0.005059128 | TrainAcc=99.931 % | ValAcc=98.900 %\n",
      "   Epoch: 67 | Loss=0.003044531 | TrainAcc=99.907 % | ValAcc=99.020 %\n",
      "   Epoch: 68 | Loss=0.003061482 | TrainAcc=99.769 % | ValAcc=98.660 %\n",
      "   Epoch: 69 | Loss=0.002238971 | TrainAcc=99.996 % | ValAcc=98.960 %\n",
      "   Epoch: 70 | Loss=0.000122698 | TrainAcc=100.000 % | ValAcc=98.980 %\n",
      "   100% Training accuracy -> Stopping\n",
      "   Training Finished in 1868.4 seconds.\n",
      "   Final accuracies:\n",
      "   ~ TrainAcc: 100.000 %\n",
      "   ~ ValAcc: 98.980 %\n",
      "   ~ TestAcc: 98.970 %\n",
      "   Saving model in file: Models/lenet5-model_adam_relu_lr0001_bs128\n",
      "*Model [ lenet5-model_adam_relu_lr00001_bs50 ] {l_r: 0.0001; n_iter: 100; batch: 50}\n",
      "   Start Training!\n",
      "   Epoch: 01 | Loss=1.653044257 | TrainAcc=61.880 % | ValAcc=63.180 %\n",
      "   Epoch: 02 | Loss=0.694801312 | TrainAcc=84.556 % | ValAcc=85.660 %\n",
      "   Epoch: 03 | Loss=0.376531262 | TrainAcc=90.951 % | ValAcc=91.900 %\n",
      "   Epoch: 04 | Loss=0.250102368 | TrainAcc=93.451 % | ValAcc=94.320 %\n",
      "   Epoch: 05 | Loss=0.192515003 | TrainAcc=94.458 % | ValAcc=95.020 %\n",
      "   Epoch: 06 | Loss=0.160383661 | TrainAcc=95.418 % | ValAcc=96.040 %\n",
      "   Epoch: 07 | Loss=0.142976664 | TrainAcc=95.976 % | ValAcc=96.280 %\n",
      "   Epoch: 08 | Loss=0.124162167 | TrainAcc=96.556 % | ValAcc=96.820 %\n",
      "   Epoch: 09 | Loss=0.115263588 | TrainAcc=96.636 % | ValAcc=96.760 %\n",
      "   Epoch: 10 | Loss=0.104776766 | TrainAcc=96.785 % | ValAcc=97.120 %\n",
      "   Epoch: 11 | Loss=0.099918639 | TrainAcc=97.131 % | ValAcc=97.240 %\n",
      "   Epoch: 12 | Loss=0.089135405 | TrainAcc=97.404 % | ValAcc=97.400 %\n",
      "   Epoch: 13 | Loss=0.088641560 | TrainAcc=97.513 % | ValAcc=97.520 %\n",
      "   Epoch: 14 | Loss=0.082265236 | TrainAcc=97.465 % | ValAcc=97.480 %\n",
      "   Epoch: 15 | Loss=0.080980202 | TrainAcc=97.760 % | ValAcc=97.720 %\n",
      "   Epoch: 16 | Loss=0.073589752 | TrainAcc=97.695 % | ValAcc=97.700 %\n",
      "   Epoch: 17 | Loss=0.070708526 | TrainAcc=97.749 % | ValAcc=97.640 %\n",
      "   Epoch: 18 | Loss=0.070103623 | TrainAcc=97.884 % | ValAcc=97.620 %\n",
      "   Epoch: 19 | Loss=0.065713172 | TrainAcc=97.844 % | ValAcc=97.600 %\n",
      "   Epoch: 20 | Loss=0.062628484 | TrainAcc=98.113 % | ValAcc=98.040 %\n",
      "   Epoch: 21 | Loss=0.060265606 | TrainAcc=98.320 % | ValAcc=98.000 %\n",
      "   Epoch: 22 | Loss=0.057173781 | TrainAcc=98.433 % | ValAcc=98.160 %\n",
      "   Epoch: 23 | Loss=0.054629861 | TrainAcc=98.413 % | ValAcc=98.360 %\n",
      "   Epoch: 24 | Loss=0.054466080 | TrainAcc=98.345 % | ValAcc=98.200 %\n",
      "   Epoch: 25 | Loss=0.052718307 | TrainAcc=98.575 % | ValAcc=98.140 %\n",
      "   Epoch: 26 | Loss=0.048289416 | TrainAcc=98.516 % | ValAcc=98.220 %\n",
      "   Epoch: 27 | Loss=0.048102016 | TrainAcc=98.520 % | ValAcc=98.240 %\n",
      "   Epoch: 28 | Loss=0.045260448 | TrainAcc=98.520 % | ValAcc=98.140 %\n",
      "   Epoch: 29 | Loss=0.045914190 | TrainAcc=98.813 % | ValAcc=98.340 %\n",
      "   Epoch: 30 | Loss=0.043232146 | TrainAcc=98.693 % | ValAcc=98.440 %\n",
      "   Epoch: 31 | Loss=0.040299431 | TrainAcc=98.902 % | ValAcc=98.560 %\n",
      "   Epoch: 32 | Loss=0.040621529 | TrainAcc=98.704 % | ValAcc=98.140 %\n",
      "   Epoch: 33 | Loss=0.038627299 | TrainAcc=98.927 % | ValAcc=98.400 %\n",
      "   Epoch: 34 | Loss=0.037620246 | TrainAcc=98.671 % | ValAcc=98.440 %\n",
      "   Epoch: 35 | Loss=0.036426143 | TrainAcc=98.896 % | ValAcc=98.440 %\n",
      "   Epoch: 36 | Loss=0.034827471 | TrainAcc=98.971 % | ValAcc=98.560 %\n",
      "   Epoch: 37 | Loss=0.036088676 | TrainAcc=99.053 % | ValAcc=98.640 %\n",
      "   Epoch: 38 | Loss=0.032539033 | TrainAcc=99.082 % | ValAcc=98.700 %\n",
      "   Epoch: 39 | Loss=0.032122850 | TrainAcc=99.024 % | ValAcc=98.620 %\n",
      "   Epoch: 40 | Loss=0.030219525 | TrainAcc=99.084 % | ValAcc=98.660 %\n",
      "   Epoch: 41 | Loss=0.028135105 | TrainAcc=99.275 % | ValAcc=98.720 %\n",
      "   Epoch: 42 | Loss=0.031449848 | TrainAcc=99.224 % | ValAcc=98.800 %\n",
      "   Epoch: 43 | Loss=0.026506793 | TrainAcc=98.955 % | ValAcc=98.540 %\n",
      "   Epoch: 44 | Loss=0.027793184 | TrainAcc=99.233 % | ValAcc=98.680 %\n",
      "   Epoch: 45 | Loss=0.027660849 | TrainAcc=99.207 % | ValAcc=98.540 %\n",
      "   Epoch: 46 | Loss=0.025486373 | TrainAcc=99.447 % | ValAcc=98.700 %\n",
      "   Epoch: 47 | Loss=0.025951856 | TrainAcc=99.296 % | ValAcc=98.660 %\n",
      "   Epoch: 48 | Loss=0.024498205 | TrainAcc=99.138 % | ValAcc=98.700 %\n",
      "   Epoch: 49 | Loss=0.023631162 | TrainAcc=99.062 % | ValAcc=98.460 %\n",
      "   Epoch: 50 | Loss=0.021649753 | TrainAcc=99.022 % | ValAcc=98.220 %\n",
      "   Epoch: 51 | Loss=0.022120994 | TrainAcc=99.462 % | ValAcc=98.740 %\n",
      "   Epoch: 52 | Loss=0.021921927 | TrainAcc=99.345 % | ValAcc=98.760 %\n",
      "   Epoch: 53 | Loss=0.021021124 | TrainAcc=99.425 % | ValAcc=98.720 %\n",
      "   Epoch: 54 | Loss=0.019883006 | TrainAcc=99.564 % | ValAcc=98.920 %\n",
      "   Epoch: 55 | Loss=0.018838957 | TrainAcc=99.520 % | ValAcc=98.880 %\n",
      "   Epoch: 56 | Loss=0.017875378 | TrainAcc=99.509 % | ValAcc=98.780 %\n",
      "   Epoch: 57 | Loss=0.018208221 | TrainAcc=99.591 % | ValAcc=98.780 %\n",
      "   Epoch: 58 | Loss=0.018245151 | TrainAcc=99.593 % | ValAcc=98.760 %\n",
      "   Epoch: 59 | Loss=0.017215948 | TrainAcc=99.571 % | ValAcc=98.760 %\n",
      "   Epoch: 60 | Loss=0.015720472 | TrainAcc=99.631 % | ValAcc=98.820 %\n",
      "   Epoch: 61 | Loss=0.017319894 | TrainAcc=99.602 % | ValAcc=98.740 %\n",
      "   Epoch: 62 | Loss=0.014363396 | TrainAcc=99.640 % | ValAcc=98.700 %\n",
      "   Epoch: 63 | Loss=0.015350660 | TrainAcc=99.527 % | ValAcc=98.600 %\n",
      "   Epoch: 64 | Loss=0.014143817 | TrainAcc=99.620 % | ValAcc=98.780 %\n",
      "   Epoch: 65 | Loss=0.014015471 | TrainAcc=99.529 % | ValAcc=98.760 %\n",
      "   Epoch: 66 | Loss=0.012598137 | TrainAcc=99.571 % | ValAcc=98.780 %\n",
      "   Epoch: 67 | Loss=0.013760652 | TrainAcc=99.644 % | ValAcc=98.860 %\n",
      "   Epoch: 68 | Loss=0.012145134 | TrainAcc=99.649 % | ValAcc=98.780 %\n",
      "   Epoch: 69 | Loss=0.012439666 | TrainAcc=99.609 % | ValAcc=98.760 %\n",
      "   Epoch: 70 | Loss=0.011371418 | TrainAcc=99.700 % | ValAcc=98.920 %\n",
      "   Epoch: 71 | Loss=0.011684269 | TrainAcc=99.762 % | ValAcc=98.960 %\n",
      "   Epoch: 72 | Loss=0.011503224 | TrainAcc=99.738 % | ValAcc=98.640 %\n",
      "   Epoch: 73 | Loss=0.010204002 | TrainAcc=99.782 % | ValAcc=98.760 %\n",
      "   Epoch: 74 | Loss=0.010355979 | TrainAcc=99.849 % | ValAcc=98.840 %\n",
      "   Epoch: 75 | Loss=0.009948653 | TrainAcc=99.715 % | ValAcc=98.860 %\n",
      "   Epoch: 76 | Loss=0.009427563 | TrainAcc=99.838 % | ValAcc=98.900 %\n",
      "   Epoch: 77 | Loss=0.008447451 | TrainAcc=99.793 % | ValAcc=98.920 %\n",
      "   Epoch: 78 | Loss=0.010013980 | TrainAcc=99.818 % | ValAcc=98.780 %\n",
      "   Epoch: 79 | Loss=0.008427799 | TrainAcc=99.796 % | ValAcc=98.820 %\n",
      "   Epoch: 80 | Loss=0.009098957 | TrainAcc=99.725 % | ValAcc=98.800 %\n",
      "   Epoch: 81 | Loss=0.007252152 | TrainAcc=99.771 % | ValAcc=98.880 %\n",
      "   Epoch: 82 | Loss=0.008566782 | TrainAcc=99.787 % | ValAcc=98.820 %\n",
      "   Epoch: 83 | Loss=0.006861291 | TrainAcc=99.789 % | ValAcc=98.680 %\n",
      "   Epoch: 84 | Loss=0.007148428 | TrainAcc=99.769 % | ValAcc=98.660 %\n",
      "   Epoch: 85 | Loss=0.007568949 | TrainAcc=99.882 % | ValAcc=98.860 %\n",
      "   Epoch: 86 | Loss=0.006765739 | TrainAcc=99.902 % | ValAcc=98.980 %\n",
      "   Epoch: 87 | Loss=0.006327723 | TrainAcc=99.571 % | ValAcc=98.800 %\n",
      "   Epoch: 88 | Loss=0.006656284 | TrainAcc=99.807 % | ValAcc=98.880 %\n",
      "   Epoch: 89 | Loss=0.005832313 | TrainAcc=99.802 % | ValAcc=98.760 %\n",
      "   Epoch: 90 | Loss=0.006304638 | TrainAcc=99.776 % | ValAcc=98.800 %\n",
      "   Epoch: 91 | Loss=0.005213225 | TrainAcc=99.833 % | ValAcc=98.900 %\n",
      "   Epoch: 92 | Loss=0.006485936 | TrainAcc=99.800 % | ValAcc=98.740 %\n",
      "   Epoch: 93 | Loss=0.005248056 | TrainAcc=99.796 % | ValAcc=98.900 %\n",
      "   Epoch: 94 | Loss=0.005520959 | TrainAcc=99.905 % | ValAcc=98.980 %\n",
      "   Epoch: 95 | Loss=0.005380047 | TrainAcc=99.904 % | ValAcc=98.880 %\n",
      "   Epoch: 96 | Loss=0.004708211 | TrainAcc=99.865 % | ValAcc=98.900 %\n",
      "   Epoch: 97 | Loss=0.005393801 | TrainAcc=99.931 % | ValAcc=98.960 %\n",
      "   Epoch: 98 | Loss=0.004514299 | TrainAcc=99.907 % | ValAcc=98.820 %\n",
      "   Epoch: 99 | Loss=0.004401247 | TrainAcc=99.924 % | ValAcc=98.920 %\n",
      "   Epoch: 100 | Loss=0.005224040 | TrainAcc=99.933 % | ValAcc=99.020 %\n",
      "   Training Finished in 3250.5 seconds.\n",
      "   Final accuracies:\n",
      "   ~ TrainAcc: 99.933 %\n",
      "   ~ ValAcc: 99.020 %\n",
      "   ~ TestAcc: 98.750 %\n",
      "   Saving model in file: Models/lenet5-model_adam_relu_lr00001_bs50\n",
      "*Model [ lenet5-model_adam_relu_lr00001_bs128 ] {l_r: 0.0001; n_iter: 100; batch: 128}\n",
      "   Start Training!\n",
      "   Epoch: 01 | Loss=2.119865159 | TrainAcc=42.864 % | ValAcc=43.780 %\n",
      "   Epoch: 02 | Loss=1.177369959 | TrainAcc=69.644 % | ValAcc=70.920 %\n",
      "   Epoch: 03 | Loss=0.740548670 | TrainAcc=79.242 % | ValAcc=80.340 %\n",
      "   Epoch: 04 | Loss=0.555677451 | TrainAcc=84.238 % | ValAcc=85.680 %\n",
      "   Epoch: 05 | Loss=0.445546990 | TrainAcc=87.493 % | ValAcc=88.600 %\n",
      "   Epoch: 06 | Loss=0.366048601 | TrainAcc=89.771 % | ValAcc=90.880 %\n",
      "   Epoch: 07 | Loss=0.301066094 | TrainAcc=91.516 % | ValAcc=92.520 %\n",
      "   Epoch: 08 | Loss=0.258525223 | TrainAcc=92.642 % | ValAcc=93.280 %\n",
      "   Epoch: 09 | Loss=0.228178994 | TrainAcc=93.324 % | ValAcc=94.140 %\n",
      "   Epoch: 10 | Loss=0.202880211 | TrainAcc=93.811 % | ValAcc=94.600 %\n",
      "   Epoch: 11 | Loss=0.188782848 | TrainAcc=94.611 % | ValAcc=95.280 %\n",
      "   Epoch: 12 | Loss=0.172132074 | TrainAcc=94.987 % | ValAcc=95.560 %\n",
      "   Epoch: 13 | Loss=0.161448823 | TrainAcc=95.344 % | ValAcc=95.860 %\n",
      "   Epoch: 14 | Loss=0.148115502 | TrainAcc=95.609 % | ValAcc=96.180 %\n",
      "   Epoch: 15 | Loss=0.143532305 | TrainAcc=95.893 % | ValAcc=96.300 %\n",
      "   Epoch: 16 | Loss=0.136909417 | TrainAcc=96.035 % | ValAcc=96.580 %\n",
      "   Epoch: 17 | Loss=0.126735421 | TrainAcc=96.167 % | ValAcc=96.640 %\n",
      "   Epoch: 18 | Loss=0.121645490 | TrainAcc=96.547 % | ValAcc=96.880 %\n",
      "   Epoch: 19 | Loss=0.118714501 | TrainAcc=96.524 % | ValAcc=96.880 %\n",
      "   Epoch: 20 | Loss=0.112385490 | TrainAcc=96.598 % | ValAcc=96.900 %\n",
      "   Epoch: 21 | Loss=0.107862289 | TrainAcc=96.800 % | ValAcc=96.920 %\n",
      "   Epoch: 22 | Loss=0.104625574 | TrainAcc=96.980 % | ValAcc=97.000 %\n",
      "   Epoch: 23 | Loss=0.095747969 | TrainAcc=97.098 % | ValAcc=97.100 %\n",
      "   Epoch: 24 | Loss=0.098790052 | TrainAcc=96.965 % | ValAcc=97.180 %\n",
      "   Epoch: 25 | Loss=0.094677257 | TrainAcc=97.284 % | ValAcc=97.280 %\n",
      "   Epoch: 26 | Loss=0.092580360 | TrainAcc=97.404 % | ValAcc=97.360 %\n",
      "   Epoch: 27 | Loss=0.086211580 | TrainAcc=97.431 % | ValAcc=97.260 %\n",
      "   Epoch: 28 | Loss=0.085795628 | TrainAcc=97.475 % | ValAcc=97.480 %\n",
      "   Epoch: 29 | Loss=0.082100380 | TrainAcc=97.495 % | ValAcc=97.460 %\n",
      "   Epoch: 30 | Loss=0.079325410 | TrainAcc=97.551 % | ValAcc=97.680 %\n",
      "   Epoch: 31 | Loss=0.079744811 | TrainAcc=97.722 % | ValAcc=97.520 %\n",
      "   Epoch: 32 | Loss=0.074975699 | TrainAcc=97.749 % | ValAcc=97.500 %\n",
      "   Epoch: 33 | Loss=0.076304431 | TrainAcc=97.905 % | ValAcc=97.720 %\n",
      "   Epoch: 34 | Loss=0.071786366 | TrainAcc=97.778 % | ValAcc=97.640 %\n",
      "   Epoch: 35 | Loss=0.071164667 | TrainAcc=97.915 % | ValAcc=97.820 %\n",
      "   Epoch: 36 | Loss=0.068906329 | TrainAcc=98.000 % | ValAcc=97.920 %\n",
      "   Epoch: 37 | Loss=0.066967504 | TrainAcc=98.107 % | ValAcc=97.880 %\n",
      "   Epoch: 38 | Loss=0.066970239 | TrainAcc=98.049 % | ValAcc=98.020 %\n",
      "   Epoch: 39 | Loss=0.065297960 | TrainAcc=98.073 % | ValAcc=97.760 %\n",
      "   Epoch: 40 | Loss=0.061918050 | TrainAcc=98.047 % | ValAcc=97.860 %\n",
      "   Epoch: 41 | Loss=0.060881495 | TrainAcc=98.176 % | ValAcc=97.880 %\n",
      "   Epoch: 42 | Loss=0.058141665 | TrainAcc=98.196 % | ValAcc=97.980 %\n",
      "   Epoch: 43 | Loss=0.061175777 | TrainAcc=98.313 % | ValAcc=98.100 %\n",
      "   Epoch: 44 | Loss=0.056304927 | TrainAcc=98.369 % | ValAcc=98.020 %\n",
      "   Epoch: 45 | Loss=0.058273177 | TrainAcc=98.118 % | ValAcc=97.820 %\n",
      "   Epoch: 46 | Loss=0.054806114 | TrainAcc=98.264 % | ValAcc=98.120 %\n",
      "   Epoch: 47 | Loss=0.053744064 | TrainAcc=98.395 % | ValAcc=98.200 %\n",
      "   Epoch: 48 | Loss=0.054394091 | TrainAcc=98.431 % | ValAcc=98.220 %\n",
      "   Epoch: 49 | Loss=0.051815206 | TrainAcc=98.420 % | ValAcc=98.260 %\n",
      "   Epoch: 50 | Loss=0.050138636 | TrainAcc=98.507 % | ValAcc=98.300 %\n",
      "   Epoch: 51 | Loss=0.050775044 | TrainAcc=98.544 % | ValAcc=98.260 %\n",
      "   Epoch: 52 | Loss=0.048942290 | TrainAcc=98.549 % | ValAcc=98.340 %\n",
      "   Epoch: 53 | Loss=0.048570286 | TrainAcc=98.578 % | ValAcc=98.200 %\n",
      "   Epoch: 54 | Loss=0.046933646 | TrainAcc=98.660 % | ValAcc=98.340 %\n",
      "   Epoch: 55 | Loss=0.048275059 | TrainAcc=98.449 % | ValAcc=98.320 %\n",
      "   Epoch: 56 | Loss=0.046286487 | TrainAcc=98.584 % | ValAcc=98.460 %\n",
      "   Epoch: 57 | Loss=0.044650686 | TrainAcc=98.584 % | ValAcc=98.240 %\n",
      "   Epoch: 58 | Loss=0.043889275 | TrainAcc=98.622 % | ValAcc=98.400 %\n",
      "   Epoch: 59 | Loss=0.044345253 | TrainAcc=98.618 % | ValAcc=98.220 %\n",
      "   Epoch: 60 | Loss=0.041081192 | TrainAcc=98.767 % | ValAcc=98.460 %\n",
      "   Epoch: 61 | Loss=0.043266359 | TrainAcc=98.818 % | ValAcc=98.440 %\n",
      "   Epoch: 62 | Loss=0.041104993 | TrainAcc=98.565 % | ValAcc=98.100 %\n",
      "   Epoch: 63 | Loss=0.038774961 | TrainAcc=98.735 % | ValAcc=98.280 %\n",
      "   Epoch: 64 | Loss=0.039534543 | TrainAcc=98.776 % | ValAcc=98.480 %\n",
      "   Epoch: 65 | Loss=0.039370675 | TrainAcc=98.796 % | ValAcc=98.360 %\n",
      "   Epoch: 66 | Loss=0.037999256 | TrainAcc=98.865 % | ValAcc=98.520 %\n",
      "   Epoch: 67 | Loss=0.037137022 | TrainAcc=98.924 % | ValAcc=98.580 %\n",
      "   Epoch: 68 | Loss=0.037509132 | TrainAcc=98.882 % | ValAcc=98.540 %\n",
      "   Epoch: 69 | Loss=0.037059930 | TrainAcc=98.880 % | ValAcc=98.620 %\n",
      "   Epoch: 70 | Loss=0.036499026 | TrainAcc=98.960 % | ValAcc=98.640 %\n",
      "   Epoch: 71 | Loss=0.033778524 | TrainAcc=99.024 % | ValAcc=98.560 %\n",
      "   Epoch: 72 | Loss=0.036193774 | TrainAcc=98.987 % | ValAcc=98.640 %\n",
      "   Epoch: 73 | Loss=0.033613385 | TrainAcc=98.835 % | ValAcc=98.360 %\n",
      "   Epoch: 74 | Loss=0.034406991 | TrainAcc=99.033 % | ValAcc=98.640 %\n",
      "   Epoch: 75 | Loss=0.032862643 | TrainAcc=98.856 % | ValAcc=98.360 %\n",
      "   Epoch: 76 | Loss=0.033273037 | TrainAcc=98.976 % | ValAcc=98.500 %\n",
      "   Epoch: 77 | Loss=0.031886878 | TrainAcc=99.009 % | ValAcc=98.600 %\n",
      "   Epoch: 78 | Loss=0.032120317 | TrainAcc=99.058 % | ValAcc=98.420 %\n",
      "   Epoch: 79 | Loss=0.030135295 | TrainAcc=98.965 % | ValAcc=98.480 %\n",
      "   Epoch: 80 | Loss=0.030467471 | TrainAcc=99.104 % | ValAcc=98.580 %\n",
      "   Epoch: 81 | Loss=0.029909991 | TrainAcc=99.129 % | ValAcc=98.600 %\n",
      "   Epoch: 82 | Loss=0.030432300 | TrainAcc=99.078 % | ValAcc=98.520 %\n",
      "   Epoch: 83 | Loss=0.028039969 | TrainAcc=99.069 % | ValAcc=98.560 %\n",
      "   Epoch: 84 | Loss=0.028668668 | TrainAcc=99.136 % | ValAcc=98.820 %\n",
      "   Epoch: 85 | Loss=0.029780842 | TrainAcc=99.224 % | ValAcc=98.640 %\n",
      "   Epoch: 86 | Loss=0.026394663 | TrainAcc=99.158 % | ValAcc=98.540 %\n",
      "   Epoch: 87 | Loss=0.026846643 | TrainAcc=99.233 % | ValAcc=98.700 %\n",
      "   Epoch: 88 | Loss=0.027267599 | TrainAcc=99.193 % | ValAcc=98.740 %\n",
      "   Epoch: 89 | Loss=0.025434776 | TrainAcc=99.244 % | ValAcc=98.620 %\n",
      "   Epoch: 90 | Loss=0.026467450 | TrainAcc=99.289 % | ValAcc=98.760 %\n",
      "   Epoch: 91 | Loss=0.025113746 | TrainAcc=99.120 % | ValAcc=98.480 %\n",
      "   Epoch: 92 | Loss=0.024460365 | TrainAcc=99.300 % | ValAcc=98.780 %\n",
      "   Epoch: 93 | Loss=0.026168538 | TrainAcc=99.222 % | ValAcc=98.720 %\n",
      "   Epoch: 94 | Loss=0.023431352 | TrainAcc=99.315 % | ValAcc=98.700 %\n",
      "   Epoch: 95 | Loss=0.024043131 | TrainAcc=99.231 % | ValAcc=98.640 %\n",
      "   Epoch: 96 | Loss=0.022724619 | TrainAcc=99.269 % | ValAcc=98.760 %\n",
      "   Epoch: 97 | Loss=0.024427595 | TrainAcc=99.376 % | ValAcc=98.660 %\n",
      "   Epoch: 98 | Loss=0.022164499 | TrainAcc=99.347 % | ValAcc=98.660 %\n",
      "   Epoch: 99 | Loss=0.021782059 | TrainAcc=99.378 % | ValAcc=98.800 %\n",
      "   Epoch: 100 | Loss=0.022131189 | TrainAcc=99.142 % | ValAcc=98.640 %\n",
      "   Training Finished in 2686.6 seconds.\n",
      "   Final accuracies:\n",
      "   ~ TrainAcc: 99.142 %\n",
      "   ~ ValAcc: 98.640 %\n",
      "   ~ TestAcc: 98.450 %\n",
      "   Saving model in file: Models/lenet5-model_adam_relu_lr00001_bs128\n"
     ]
    }
   ],
   "source": [
    "# Trying parameters with Adam Optimizer - AFTER CHANGING THE OPTIMIZER! \n",
    "# (Weight init & safe softmax)\n",
    "setOfParams = [ [0.001, 100, 50, 'lenet5-model_adam_relu_lr0001_bs50'],\n",
    "                [0.001, 100, 128, 'lenet5-model_adam_relu_lr0001_bs128'],\n",
    "                [0.0001, 100, 50, 'lenet5-model_adam_relu_lr00001_bs50'],\n",
    "                [0.0001, 100, 128, 'lenet5-model_adam_relu_lr00001_bs128'],\n",
    "    \n",
    "]\n",
    "for p in setOfParams:\n",
    "    CNNet (modelName=p[3], \n",
    "           learning_rate = p[0], \n",
    "           training_epochs = p[1],\n",
    "           batch_size = p[2],\n",
    "           activationFunc=tf.nn.relu,\n",
    "           optimFunc=tf.train.AdamOptimizer\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>COMMENT:</b><br/>\n",
    "Other measure that we took was to stop the training when reaching 100% accuracy. Nevertheless, even with 100% accuracy the loss keeps going down, which is why we implemented the model loading and retrained the best model for other 30 epochs:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Model [ lenet5-model_best ] {l_r: 0.0010; n_iter: 30; batch: 128}\n",
      " Start Training!\n",
      " Epoch: 01 | Loss=0.000036285 | TrainAcc=100.000 % | ValAcc=99.040 %\n",
      " Epoch: 02 | Loss=0.000020620 | TrainAcc=100.000 % | ValAcc=99.060 %\n",
      " Epoch: 03 | Loss=0.000013097 | TrainAcc=100.000 % | ValAcc=99.060 %\n",
      " Epoch: 04 | Loss=0.000011698 | TrainAcc=100.000 % | ValAcc=99.060 %\n",
      " Epoch: 05 | Loss=0.000008156 | TrainAcc=100.000 % | ValAcc=99.040 %\n",
      " Epoch: 06 | Loss=0.000006704 | TrainAcc=100.000 % | ValAcc=99.020 %\n",
      " Epoch: 07 | Loss=0.000005328 | TrainAcc=100.000 % | ValAcc=99.040 %\n",
      " Epoch: 08 | Loss=0.000004981 | TrainAcc=100.000 % | ValAcc=99.040 %\n",
      " Epoch: 09 | Loss=0.000003284 | TrainAcc=100.000 % | ValAcc=99.020 %\n",
      " Epoch: 10 | Loss=0.000003292 | TrainAcc=100.000 % | ValAcc=99.040 %\n",
      " Epoch: 11 | Loss=0.000002527 | TrainAcc=100.000 % | ValAcc=99.040 %\n",
      " Epoch: 12 | Loss=0.000001922 | TrainAcc=100.000 % | ValAcc=99.020 %\n",
      " Epoch: 13 | Loss=0.000001719 | TrainAcc=100.000 % | ValAcc=99.020 %\n",
      " Epoch: 14 | Loss=0.000001187 | TrainAcc=100.000 % | ValAcc=99.020 %\n",
      " Epoch: 15 | Loss=0.000001085 | TrainAcc=100.000 % | ValAcc=99.020 %\n",
      " Epoch: 16 | Loss=0.000000890 | TrainAcc=100.000 % | ValAcc=99.040 %\n",
      " Epoch: 17 | Loss=0.000000700 | TrainAcc=100.000 % | ValAcc=99.040 %\n",
      " Epoch: 18 | Loss=0.000000528 | TrainAcc=100.000 % | ValAcc=99.080 %\n",
      " Epoch: 19 | Loss=0.000000464 | TrainAcc=100.000 % | ValAcc=99.040 %\n",
      " Epoch: 20 | Loss=0.000000353 | TrainAcc=100.000 % | ValAcc=99.060 %\n",
      " Epoch: 21 | Loss=0.000000281 | TrainAcc=100.000 % | ValAcc=99.080 %\n",
      " Epoch: 22 | Loss=0.000000242 | TrainAcc=100.000 % | ValAcc=99.060 %\n",
      " Epoch: 23 | Loss=0.000000182 | TrainAcc=100.000 % | ValAcc=99.080 %\n",
      " Epoch: 24 | Loss=0.000000135 | TrainAcc=100.000 % | ValAcc=99.080 %\n",
      " Epoch: 25 | Loss=0.000000126 | TrainAcc=100.000 % | ValAcc=99.080 %\n",
      " Epoch: 26 | Loss=0.000000096 | TrainAcc=100.000 % | ValAcc=99.080 %\n",
      " Epoch: 27 | Loss=0.000000078 | TrainAcc=100.000 % | ValAcc=99.060 %\n",
      " Epoch: 28 | Loss=0.000000057 | TrainAcc=100.000 % | ValAcc=99.080 %\n",
      " Epoch: 29 | Loss=0.000000048 | TrainAcc=100.000 % | ValAcc=99.100 %\n",
      " Epoch: 30 | Loss=0.000000037 | TrainAcc=100.000 % | ValAcc=99.080 %\n",
      " Training Finished in 805.4 seconds.\n",
      " Final accuracies:\n",
      " ~ TrainAcc: 100.000 %\n",
      " ~ ValAcc: 99.080 %\n",
      " ~ TestAcc: 99.110 %\n",
      " Saving model in file: Models/lenet5-model_best\n"
     ]
    }
   ],
   "source": [
    "# Ending the training of the best optimizer\n",
    "CNNet (modelName='lenet5-model_best', \n",
    "       learning_rate = 0.001, \n",
    "       training_epochs = 30,\n",
    "       batch_size = 128,\n",
    "       activationFunc=tf.nn.relu,\n",
    "       optimFunc=tf.train.AdamOptimizer, \n",
    "       loadModel='lenet5-model_adam_relu_lr0001_bs128'\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.2.2 Dropout layer [Question 2.2.2]\n",
    "What about applying a dropout layer on the Fully conntected layer and then retraining the model with the best Optimizer and parameters(Learning rate and Batsh size) obtained in the previous section? (probability to keep units=0.75). For this stage we ensure that the keep prob is set to 1.0 to evaluate the performance of the network including all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Model [ lenet5-model ] {l_r: 0.0010; n_iter: 100; batch: 128}\n",
      "   Start Training!\n",
      "   Epoch: 01 | Loss=1.462579143 | TrainAcc=86.182 % | ValAcc=86.920 %\n",
      "   Epoch: 02 | Loss=0.320648048 | TrainAcc=95.773 % | ValAcc=95.900 %\n",
      "   Epoch: 03 | Loss=0.156452648 | TrainAcc=97.318 % | ValAcc=97.500 %\n",
      "   Epoch: 04 | Loss=0.107257504 | TrainAcc=97.940 % | ValAcc=97.900 %\n",
      "   Epoch: 05 | Loss=0.079941446 | TrainAcc=98.598 % | ValAcc=98.300 %\n",
      "   Epoch: 06 | Loss=0.067996642 | TrainAcc=98.916 % | ValAcc=98.600 %\n",
      "   Epoch: 07 | Loss=0.056781804 | TrainAcc=98.987 % | ValAcc=98.820 %\n",
      "   Epoch: 08 | Loss=0.049156536 | TrainAcc=99.084 % | ValAcc=98.780 %\n",
      "   Epoch: 09 | Loss=0.046372587 | TrainAcc=99.335 % | ValAcc=98.820 %\n",
      "   Epoch: 10 | Loss=0.039569505 | TrainAcc=99.242 % | ValAcc=98.860 %\n",
      "   Epoch: 11 | Loss=0.035637989 | TrainAcc=99.491 % | ValAcc=99.100 %\n",
      "   Epoch: 12 | Loss=0.033921816 | TrainAcc=99.558 % | ValAcc=99.000 %\n",
      "   Epoch: 13 | Loss=0.029400493 | TrainAcc=99.427 % | ValAcc=98.780 %\n",
      "   Epoch: 14 | Loss=0.029948988 | TrainAcc=99.547 % | ValAcc=98.920 %\n",
      "   Epoch: 15 | Loss=0.025876489 | TrainAcc=99.580 % | ValAcc=98.900 %\n",
      "   Epoch: 16 | Loss=0.024096347 | TrainAcc=99.653 % | ValAcc=99.000 %\n",
      "   Epoch: 17 | Loss=0.021928211 | TrainAcc=99.691 % | ValAcc=99.020 %\n",
      "   Epoch: 18 | Loss=0.020788123 | TrainAcc=99.789 % | ValAcc=98.960 %\n",
      "   Epoch: 19 | Loss=0.017248768 | TrainAcc=99.685 % | ValAcc=99.000 %\n",
      "   Epoch: 20 | Loss=0.019471339 | TrainAcc=99.747 % | ValAcc=98.920 %\n",
      "   Epoch: 21 | Loss=0.018876892 | TrainAcc=99.787 % | ValAcc=99.040 %\n",
      "   Epoch: 22 | Loss=0.016631310 | TrainAcc=99.796 % | ValAcc=98.960 %\n",
      "   Epoch: 23 | Loss=0.015412680 | TrainAcc=99.833 % | ValAcc=98.940 %\n",
      "   Epoch: 24 | Loss=0.014776390 | TrainAcc=99.838 % | ValAcc=99.220 %\n",
      "   Epoch: 25 | Loss=0.016413795 | TrainAcc=99.882 % | ValAcc=99.140 %\n",
      "   Epoch: 26 | Loss=0.013022174 | TrainAcc=99.878 % | ValAcc=99.080 %\n",
      "   Epoch: 27 | Loss=0.013108108 | TrainAcc=99.751 % | ValAcc=98.920 %\n",
      "   Epoch: 28 | Loss=0.012482495 | TrainAcc=99.845 % | ValAcc=99.020 %\n",
      "   Epoch: 29 | Loss=0.011373647 | TrainAcc=99.911 % | ValAcc=99.140 %\n",
      "   Epoch: 30 | Loss=0.013156030 | TrainAcc=99.884 % | ValAcc=99.220 %\n",
      "   Epoch: 31 | Loss=0.012250657 | TrainAcc=99.933 % | ValAcc=99.220 %\n",
      "   Epoch: 32 | Loss=0.011849047 | TrainAcc=99.945 % | ValAcc=99.180 %\n",
      "   Epoch: 33 | Loss=0.009569408 | TrainAcc=99.876 % | ValAcc=98.980 %\n",
      "   Epoch: 34 | Loss=0.010187886 | TrainAcc=99.876 % | ValAcc=99.020 %\n",
      "   Epoch: 35 | Loss=0.008628585 | TrainAcc=99.920 % | ValAcc=98.960 %\n",
      "   Epoch: 36 | Loss=0.011090174 | TrainAcc=99.880 % | ValAcc=99.140 %\n",
      "   Epoch: 37 | Loss=0.010603959 | TrainAcc=99.791 % | ValAcc=98.900 %\n",
      "   Epoch: 38 | Loss=0.010294543 | TrainAcc=99.949 % | ValAcc=99.140 %\n",
      "   Epoch: 39 | Loss=0.009244751 | TrainAcc=99.976 % | ValAcc=99.200 %\n",
      "   Epoch: 40 | Loss=0.007726011 | TrainAcc=99.904 % | ValAcc=99.000 %\n",
      "   Epoch: 41 | Loss=0.007734558 | TrainAcc=99.935 % | ValAcc=99.020 %\n",
      "   Epoch: 42 | Loss=0.009800617 | TrainAcc=99.918 % | ValAcc=99.060 %\n",
      "   Epoch: 43 | Loss=0.008972260 | TrainAcc=99.925 % | ValAcc=99.120 %\n",
      "   Epoch: 44 | Loss=0.007027677 | TrainAcc=99.931 % | ValAcc=99.140 %\n",
      "   Epoch: 45 | Loss=0.008789836 | TrainAcc=99.956 % | ValAcc=99.080 %\n",
      "   Epoch: 46 | Loss=0.008016213 | TrainAcc=99.962 % | ValAcc=99.220 %\n",
      "   Epoch: 47 | Loss=0.008566651 | TrainAcc=99.927 % | ValAcc=99.160 %\n",
      "   Epoch: 48 | Loss=0.006483152 | TrainAcc=99.849 % | ValAcc=99.000 %\n",
      "   Epoch: 49 | Loss=0.007160175 | TrainAcc=99.978 % | ValAcc=99.200 %\n",
      "   Epoch: 50 | Loss=0.006316839 | TrainAcc=99.949 % | ValAcc=99.120 %\n",
      "   Epoch: 51 | Loss=0.007614891 | TrainAcc=99.984 % | ValAcc=99.000 %\n",
      "   Epoch: 52 | Loss=0.007235258 | TrainAcc=99.880 % | ValAcc=98.980 %\n",
      "   Epoch: 53 | Loss=0.008472990 | TrainAcc=99.935 % | ValAcc=99.040 %\n",
      "   Epoch: 54 | Loss=0.005591699 | TrainAcc=99.947 % | ValAcc=99.100 %\n",
      "   Epoch: 55 | Loss=0.006767461 | TrainAcc=99.956 % | ValAcc=99.120 %\n",
      "   Epoch: 56 | Loss=0.008023979 | TrainAcc=99.949 % | ValAcc=99.100 %\n",
      "   Epoch: 57 | Loss=0.006355836 | TrainAcc=99.978 % | ValAcc=99.040 %\n",
      "   Epoch: 58 | Loss=0.007154427 | TrainAcc=99.982 % | ValAcc=99.220 %\n",
      "   Epoch: 59 | Loss=0.005448605 | TrainAcc=99.969 % | ValAcc=99.020 %\n",
      "   Epoch: 60 | Loss=0.005992711 | TrainAcc=99.951 % | ValAcc=99.000 %\n",
      "   Epoch: 61 | Loss=0.007338103 | TrainAcc=99.989 % | ValAcc=99.180 %\n",
      "   Epoch: 62 | Loss=0.006021912 | TrainAcc=99.982 % | ValAcc=99.240 %\n",
      "   Epoch: 63 | Loss=0.007646662 | TrainAcc=99.975 % | ValAcc=99.280 %\n",
      "   Epoch: 64 | Loss=0.005512220 | TrainAcc=99.984 % | ValAcc=99.180 %\n",
      "   Epoch: 65 | Loss=0.005569470 | TrainAcc=99.976 % | ValAcc=99.200 %\n",
      "   Epoch: 66 | Loss=0.005267732 | TrainAcc=99.982 % | ValAcc=99.160 %\n",
      "   Epoch: 67 | Loss=0.004858207 | TrainAcc=99.904 % | ValAcc=98.920 %\n",
      "   Epoch: 68 | Loss=0.008335329 | TrainAcc=99.913 % | ValAcc=99.040 %\n",
      "   Epoch: 69 | Loss=0.006250409 | TrainAcc=99.987 % | ValAcc=99.180 %\n",
      "   Epoch: 70 | Loss=0.007171843 | TrainAcc=99.929 % | ValAcc=99.060 %\n",
      "   Epoch: 71 | Loss=0.005226717 | TrainAcc=99.987 % | ValAcc=99.200 %\n",
      "   Epoch: 72 | Loss=0.005610764 | TrainAcc=99.995 % | ValAcc=99.260 %\n",
      "   Epoch: 73 | Loss=0.003621007 | TrainAcc=99.944 % | ValAcc=98.960 %\n",
      "   Epoch: 74 | Loss=0.005122103 | TrainAcc=99.991 % | ValAcc=99.140 %\n",
      "   Epoch: 75 | Loss=0.007763147 | TrainAcc=99.958 % | ValAcc=99.100 %\n",
      "   Epoch: 76 | Loss=0.004949913 | TrainAcc=99.975 % | ValAcc=99.120 %\n",
      "   Epoch: 77 | Loss=0.005895723 | TrainAcc=99.982 % | ValAcc=99.000 %\n",
      "   Epoch: 78 | Loss=0.005900653 | TrainAcc=99.982 % | ValAcc=99.140 %\n",
      "   Epoch: 79 | Loss=0.006594847 | TrainAcc=99.980 % | ValAcc=99.120 %\n",
      "   Epoch: 80 | Loss=0.004076439 | TrainAcc=99.993 % | ValAcc=99.220 %\n",
      "   Epoch: 81 | Loss=0.006026144 | TrainAcc=99.995 % | ValAcc=99.220 %\n",
      "   Epoch: 82 | Loss=0.004031588 | TrainAcc=99.993 % | ValAcc=99.140 %\n",
      "   Epoch: 83 | Loss=0.004725516 | TrainAcc=99.942 % | ValAcc=99.060 %\n",
      "   Epoch: 84 | Loss=0.004953352 | TrainAcc=99.980 % | ValAcc=99.060 %\n",
      "   Epoch: 85 | Loss=0.007481557 | TrainAcc=99.927 % | ValAcc=99.100 %\n",
      "   Epoch: 86 | Loss=0.004820829 | TrainAcc=99.982 % | ValAcc=99.120 %\n",
      "   Epoch: 87 | Loss=0.004262584 | TrainAcc=99.980 % | ValAcc=99.080 %\n",
      "   Epoch: 88 | Loss=0.004377514 | TrainAcc=99.989 % | ValAcc=99.120 %\n",
      "   Epoch: 89 | Loss=0.006101638 | TrainAcc=99.995 % | ValAcc=99.180 %\n",
      "   Epoch: 90 | Loss=0.004651474 | TrainAcc=99.982 % | ValAcc=99.080 %\n",
      "   Epoch: 91 | Loss=0.003524272 | TrainAcc=99.976 % | ValAcc=99.060 %\n",
      "   Epoch: 92 | Loss=0.003793414 | TrainAcc=99.978 % | ValAcc=98.960 %\n",
      "   Epoch: 93 | Loss=0.004249668 | TrainAcc=99.982 % | ValAcc=99.140 %\n",
      "   Epoch: 94 | Loss=0.005965028 | TrainAcc=99.920 % | ValAcc=99.120 %\n",
      "   Epoch: 95 | Loss=0.005710053 | TrainAcc=99.967 % | ValAcc=99.140 %\n",
      "   Epoch: 96 | Loss=0.003603934 | TrainAcc=99.958 % | ValAcc=99.080 %\n",
      "   Epoch: 97 | Loss=0.003396762 | TrainAcc=99.958 % | ValAcc=99.060 %\n",
      "   Epoch: 98 | Loss=0.004033627 | TrainAcc=99.989 % | ValAcc=99.100 %\n",
      "   Epoch: 99 | Loss=0.006118423 | TrainAcc=99.984 % | ValAcc=99.000 %\n",
      "   Epoch: 100 | Loss=0.003984706 | TrainAcc=99.991 % | ValAcc=99.200 %\n",
      "   Training Finished in 2681.9 seconds.\n",
      "   Final accuracies:\n",
      "   ~ TrainAcc: 99.991 %\n",
      "   ~ ValAcc: 99.200 %\n",
      "   ~ TestAcc: 99.040 %\n",
      "   Saving model in file: Models/lenet5-model\n"
     ]
    }
   ],
   "source": [
    "CNNet ('lenet5-model', \n",
    "           learning_rate = 0.001, \n",
    "           training_epochs = 100,\n",
    "           batch_size = 128,\n",
    "           activationFunc = tf.nn.relu,\n",
    "           optimFunc=tf.train.AdamOptimizer,\n",
    "           keep_p = 0.75\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class='alert alert-warning'>\n",
    "<b>COMMENT:</b><br/>\n",
    "We had everything implemented in the functions above, we only needed to call it.<br/><br/>\n",
    "\n",
    "We once again surpass the 99% frontier! In fact, since the validation acuracy is 99.2%, this suggests that further playing with the keep probability could lead to an even better model. However, this is our of the scope of this notebook\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
